[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Dr. Matt Williamson\n   4125 Environmental Research Building\n   mattwilliamson@boisestate.edu\n   MwilliamsonMatt\n   Schedule an appointment\n\n\n\n\n\n   Mondays and Wednesdays\n   August 21–December 13, 2023\n   1:30–2:45 PM\n   SMASH 116\n   Slack"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nSpatial data are ubiquitous and form the basis for many of our inquiries into social, ecological, and evolutionary processes. As such, developing the skills necessary for incorporating spatial data into reproducible statistical workflows is critical. In this course, we will introduce the core components of manipulating spatial data within the R statistical environment including managing vector and raster data, projections, extraction of data values, interpolation, and plotting. Students will also learn to prototype and benchmark different workflows to aid in applying the appropriate tools to their research questions."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course Objectives",
    "text": "Course Objectives\nStudents completing this course should be able to:\n\nArticulate the opportunities and challenges posed by geographic analysis.\nSelect the appropriate R packages and functions for manipulating different types of spatial data\nDesign statistical analyses that integrate geospatial and tabular data\n\nConstruct appropriate data visualizations for conveying geospatial data\nDevelop reproducible workflows for manipulating, visualizing, and analyzing spatial data."
  },
  {
    "objectID": "syllabus.html#expectations",
    "href": "syllabus.html#expectations",
    "title": "Syllabus",
    "section": "Expectations",
    "text": "Expectations\nBe nice. Be honest. Try hard.\nThe beauty of working with open source software is the community of users working on problems just like yours (and nothing like yours). Like any community, this one functions best when its members are kind, genuine, and make good-faith efforts to solve their problems along the way (more on this below).\nYou can (and should) expect me to:\n\nCreate a space where you can ask questions without fear of embarrassment or retribution\nProvide feedback on your work within 1 week of submission\nRespond to email and slack messages within 48 hours\nMake every attempt to answer your questions (when I can) or point you toward resources that may help\n\nIn turn, I expect you to:\n\nTreat all of us with respect and compassion\nMake an honest effort to work through the assignments\nDemonstrate that you have tried to solve your coding errors before asking me\nCommunicate with me when the course isn’t working for you"
  },
  {
    "objectID": "syllabus.html#prerequisite-knowledge-and-skills",
    "href": "syllabus.html#prerequisite-knowledge-and-skills",
    "title": "Syllabus",
    "section": "Prerequisite Knowledge and Skills",
    "text": "Prerequisite Knowledge and Skills\nYou can succeed in this class.\nSome familiarity with the R statistical environment is helpful, but not necessary. My goal is to foster an environment where we are all learning from each other and sharing the tips and tricks that help us along the way. Learning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. I find it helpful to remember the following:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later. Even experienced programmers find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n— Hadley Wickham\n\n\nIf you want to start learning a few of the basics, the Resources tab has some background information to get you started. Note that this is not an exhaustive list - the number of new R tutorials available on the internet seems to be growing exponentially.\n\nGetting Help With R problems\nI am happy to help you work through your R coding challenges, but there are a lot of you and only one of me. Moreover, I may not always know exactly how to fix your problem any better than you do. In order to make sure that I am not the primary obstacle to your ability to complete the class assignments, I’m asking that you use the following steps prior to emailing/Slacking me with your coding questions.\n\n\n\n\n\n\nTip\n\n\n\nWhen you send me a question, please let me know what you searched, why the solutions you found don’t work for you, and what output you are expecting**\n\n\nWe’ll spend a bit of time on asking better questions and getting better answers so don’t worry if you aren’t quite sure how this all works.\n\nGoogle it! Searching for help with R on Google can sometimes be tricky. Google is generally smart enough to figure out what you mean when you search for “r reproject polygons”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats reproject polygons”). Also, since most of your R work will deal with the RSpatial packages, it’s often easier to just search for the package name and operation rather than the letter “r” (e.g. “sf reproject polygons”). I often paste the specific error message I get along with the spatial package I’m using to try and help Google find my solutions.\nAsk your colleagues We have an r_spatial chatroom at Slack where anyone in this class can ask questions and anyone can answer. Ask questions about code or class materials. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too. As a bonus, Slack allows you to format code to make it easy for all of us to copy and paste your code and distinguish it from the rest of your question.\nUse the forums Two of the most important sources for help with R-coding are StackOverflow (a Q&A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)). If you aren’t able to find an answer to your question from the thousands of existing questions, you can post your own. You’ll need to create a reproducible example so others can figure out what you’re trying to do and what error you’re receiving, but you’d be amazed how helpful the community can be.\nAsk me! Sign up for a time to meet with me during student hours at https://calendly.com/mattwilliamson/. I’ll want to know what searches you’ve tried (so I don’t chase down answers that you’ve already seen) and what approaches you’ve tried and why they haven’t worked. Remember, I’m here to help (but not write your code for you)."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nR and RStudio\nR is free, but it can sometimes be a pain to install and configure especially when dealing with spatial packages (we’ll talk more about why this is during class). To make life easier, I have set up an online RStudio server service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer and we should be able to avoid a number of the machine-specific issues that pop-up when 20 students have 20 different computers, operating systems (OS), etc. If you haven’t installed R on your local machine and would like some help getting that set up, there’ a useful set of instructions for installing R, RStudio, and all the tidyverse packages here.\n\n\nGit and Github Classroom\nAll assignments will be managed using Github classroom. This will allow each you to have your own repositories for each assignment and make it easier for me to comment on and help with your code. To use this, you should sign up for the GitHub Student Developers Pack as soon as possible and send me your github username. Once I have that, I can add you to the course and make sure that you have access to all of the necessary data and example code.\n\n\nReadings\nThe goal of this course is primarily to get you started with spatial workflows in R. That said, maps (and the spatial data that produce them) are extremely powerful and their use comes with risks and responsibilities. Although most of this course will focus on getting the code right, I’ll mix in a few readings each week to help tie the technical details of our code back to the broader contexts of spatial analysis or to illustrate new applications of the methods you are learning."
  },
  {
    "objectID": "syllabus.html#course-schedule",
    "href": "syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThis course is organized in 4 sections:\n\nGetting Started: What is spatial analysis and how do we do it in R?\nSpatial Data Operations in R: Prepping geospatial data for use in R\nStatistical Workflows for Spatial Data: Putting spatial data to work!\nVisualizing Spatial Data: Everyone loves a map…\n\nThe schedule page provides an overview of what to expect each week.\nThis syllabus reflects a plan for the semester. Deviations may become necessary as the course progresses."
  },
  {
    "objectID": "syllabus.html#assignments-and-grades",
    "href": "syllabus.html#assignments-and-grades",
    "title": "Syllabus",
    "section": "Assignments and Grades",
    "text": "Assignments and Grades\nI teach this course because I believe that a) we can learn a lot about social and ecological processes by studying where they happen, b) integrating spatial analysis directly into statistical workflows makes those analyses more robust and reproducible, and c) overcoming coding challenges can provide a profound sense of accomplishment. That said, I recognize that there are many reasons that you are taking this course and that my objectives may differ from yours. In order to make sure that you get what you need out of this class, we’ll be using a mix of approaches for determining your grade in this course.\nSelf-assessment (12.5 pts x 2):  During the first week of the course, I’m going to ask you to reflect on what you want out of this course (concepts, skills, practice, etc.). This assessment will help me do a better job of aligning the content of the course to your specific needs. Grading for the self-assessment is described on the assignments page.\nExercises (5pts x 10): There are ten homework assignments. These exercises are designed to reinforce the material we cover in lecture, give you practice designing and implementing your own workflows, and build habits that promote reproducibility in science. They also allow me get a sense for your engagement in the course. Exercises are due at 11:59PM on their due date (generally Thursdays). I will post the “key” within 3 days of the due date and will not accept submissions after the key is posted. If you turn the assignment in on-time with the required number of commits, you’ll recieve full credit.\nAssignment Revisions (25pts x 3): We will have three “assignment revisions” due during the course. These provide an opportunity for me to check in and see how things are going. You’ll be able to update your responses to the homeworks based on the keys and reflect on what you’ve learned throughout the course of the assignments. You’ll also be able to provide additional feedback on how the course is going for you. Rather than assign arbitrary points to each assignment, I’m going to grade your assignment revisions using the following ‘levels’ (inspired by Sarah K. Johnson’s description of her graduate data analysis course at Tufts):\n\nPlease Resubmit: This indicates that either your code does not run as written (i.e., your Rmarkdown document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before you get to the next assessment. Failure to resubmit will result in no credit for the assessment.\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. Your are welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\nFinal Project (50pts):  The final project asks you to conduct an entire spatial analysis from layout to results. Grades on the final project are based on your objectives and your self-assessment of whether or not you achieved those objectives. Your first draft of the final project will be due December 5. I’ll make comments based on the same categories for the homework revision and you’ll have time to revise your submission prior to the final deadline of December 14.\nYou can find descriptions for all the assignments on the assignments page.\n\nGrades\nWe’ll use a form of contract grading to determine your grades in the course. Contract grading allows us to have a conversation about what you want out of the course, what you expect to put into it, and what I think you need to be successful in deploying the skills we learn here. Based on your goals for course, we’ll sign a contract that instantiates your objectives into the grade you’ll receive for the course. Complete the assignments and meet your objectives and you’ll get the grade you chose.\nThe expectations for the grades are:\n\nA You complete all of the self-assessments and at least 8 of the exercises. All of the assignment revisions achieve the “Good to Go” level. Your final project achieves the “Good to Go” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nB You complete all of the self-assessments and at least 8 of the exercises. At least one of the assignment revisions achieves the “Good to Go” level with the remainder achieving “Resubmit if you like”. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nC You complete all of the self-assessments and at least 6 of the exercises. All of your assignment revisions achieve the “Resubmit if you like” level. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\nD You complete all of the self-assessments and at least 4 of the exercises. At least one of your assignment revisions achieves the “Resubmit if you like” level. Your final project achieves the “Resubmit if you like” level. My assessment of the various levels will be based on your objectives for the course and your ability to follow instructions.\n\n\n\nAttendance and incomplete assignments\nAttendance is an important part of this course. You are allowed to miss 2 classes without providing any justification (stuff happens). Beyond that, each additional absence will result in a 0.5 grading reduction (i.e., an A becomes and A-). Similarly, completing the assignments to a satisfactory level is vital to ensure you have a firm grip on the code and concepts. Hence, each assignment that fails to achieve a “Resubmit If You Like” will result in 0.5 grading reduction.\n\n\nLate work\nI would highly recommend staying caught up as much as possible, but if you need to turn something (other than the exercises and final project) in late, that’s fine—there’s no penalty."
  },
  {
    "objectID": "syllabus.html#student-wellbeing",
    "href": "syllabus.html#student-wellbeing",
    "title": "Syllabus",
    "section": "Student Wellbeing",
    "text": "Student Wellbeing\nIf you are struggling for any reason (COVID, relationship, family, or life’s stresses) and believe these may impact your performance in the course, I encourage you to contact the Dean of Students at (208) 426-1527 or emaildeanofstundents@boisestate.edu for support. If you notice a significant change in your mood, sleep, feelings of hopelessness or a lack of self worth, consider connecting immediately with Counseling Services (1529 Belmont Street, Norco Building) at (208) 426-1459 or email healthservices@boisestate.edu.\n\nLearning during a pandemic\nIf you tell me you’re having trouble, I will not judge you or think less of you. I hope you’ll extend me the same grace.\nYou never owe me personal information about your health (mental or physical). You are always welcome to talk to me about things that you’re going through, though. If I can’t help you, I usually know somebody who can.\nIf you need extra help, or if you need more time with something, or if you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! I will work with you. I promise."
  },
  {
    "objectID": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "href": "syllabus.html#this-course-was-designed-with-you-in-mind",
    "title": "Syllabus",
    "section": "This course was designed with you in mind",
    "text": "This course was designed with you in mind\nI developed this course to provide a welcoming environment and effective, equitable learning experience for all students. If you encounter barriers in this course, please bring them to my attention so that I may work to address them.\n\nThis class’s community is inclusive.\nStudents in this class represent a rich variety of backgrounds and perspectives. The Human-Environment Systems group is committed to providing an atmosphere for learning that respects diversity and creates inclusive environments in our courses. While working together to build this community, we ask all members to: * share their unique experiences, values, and beliefs, if comfortable doing so.\n\nlisten deeply to one another.\nhonor the uniqueness of their peers.\nappreciate the opportunity we have to learn from each other in this community.\nuse this opportunity together to discuss ways in which we can create an inclusive environment in this course and across the campus community.\nrecognize opportunities to invite a community member to exhibit more inclusive, equitable speech or behavior—and then also invite them into further conversation. We also expect community members to respond with gratitude and to take a moment of reflection when they receive such an invitation, rather than react immediately from defensiveness.\nkeep confidential any discussions that the community has of a personal (or professional) nature, unless the speaker has given explicit permission to share what they have said.\nrespect the right of students to be addressed and referred to by the names and pronouns that correspond to their gender identities, including the use of non-binary pronouns.\n\n\n\nWe use each other’s preferred names and pronouns.\nI will ask you to let me know your preferred or adopted name and gender pronoun(s), and I will make those changes to my own records and address you that way in all cases.\nTo change to a preferred name so that it displays on all BSU sites, including Canvas and our course roster, contact the Registrar’s Office at (208) 426-4249. Note that only a legal name change can alter your name on BSU official and legal documents (e.g., your transcript).\n\n\nThis course is accessible to students with disabilities.\nI recognize that navigating your education and life can often be more difficult if you have disabilities. I want you to achieve at your highest capacity in this class. If you have a disability, I need to know if you encounter inequitable opportunities in my course related to:\n\naccessing and understanding course materials engaging with course materials and other students in the course\ndemonstrating your skills and knowledge on assignments and exams.\n\nIf you have a documented disability, you may be eligible for accommodations in all of your courses. To learn more, make an appointment with the university’s Educational Access Center.\n\n\nFor students responsible for children\nI recognize the unique challenges that can arise for students who are also parents or guardians of children. Any student needing to temporarily bring children or another dependent to class is welcome to do so to stay engaged with the class."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAcademic integrity is the principle that asks students to engage with their academic work to the fullest and to behave honestly, transparently, and ethically in every assignment and every interaction with a peer, professor, or research participant. When a strong culture of academic integrity is fostered by students and faculty in an academic program, students learn more, build positive relationships and collaborations, and can feel more confident in the value of their degrees.\nIn order to cultivate fairness and credibility, everyone must participate in upholding academic integrity. Students in this class are responsible for asking for help or clarification when it’s needed, speaking up when they see unethical behavior taking place, and understanding and adhering to the Student Code of Conduct, including the section on academic misconduct. Boise State and I take academic misconduct very seriously. It’s important to know that when a student engages in academic misconduct, I will report the incident to the Office of the Dean of Students. I also have the right to assign sanctions, which could include requirements to revise or redo work, complete educational assignments to learn about academic integrity, and grade penalties ranging from lower credit on an assignment to failing this class1. Students should learn more by reviewing the Student Code of Conduct."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo seriously, just don’t cheat or plagiarize!↩︎"
  },
  {
    "objectID": "slides/20-slides.html#statistical-interpolation-1",
    "href": "slides/20-slides.html#statistical-interpolation-1",
    "title": "Areal Data and Proximity",
    "section": "Statistical Interpolation",
    "text": "Statistical Interpolation\n\n\n\nPrevious methods predict \\(z\\) as a (weighted) function of distance\nTreat the observations as perfect (no error)\nIf we imagine that \\(z\\) is the outcome of some spatial process such that:"
  },
  {
    "objectID": "slides/20-slides.html#trend-surface-modeling",
    "href": "slides/20-slides.html#trend-surface-modeling",
    "title": "Areal Data and Proximity",
    "section": "Trend Surface Modeling",
    "text": "Trend Surface Modeling\n\nBasically a regression on the coordinates of your data points\nCoefficients apply to the coordinates and their interaction\nRelies on different functional forms"
  },
  {
    "objectID": "slides/20-slides.html#th-order-trend-surface",
    "href": "slides/20-slides.html#th-order-trend-surface",
    "title": "Areal Data and Proximity",
    "section": "0th Order Trend Surface",
    "text": "0th Order Trend Surface\n\n\n\n\n\n\n\n\n\nSimplest form of trend surface\n\\(Z=a\\) where \\(a\\) is the mean value of air quality\nResult is a simple horizontal surface where all values are the same."
  },
  {
    "objectID": "slides/20-slides.html#th-order-trend-surface-1",
    "href": "slides/20-slides.html#th-order-trend-surface-1",
    "title": "Areal Data and Proximity",
    "section": "0th order trend surface",
    "text": "0th order trend surface\n\n#set up interpolation grid\n# Create an empty grid where n is the total number of cells\ngrd &lt;- as.data.frame(spsample(as(id.cty, \"Spatial\"), \"regular\", n=20000))\nnames(grd)       &lt;- c(\"X\", \"Y\")\ncoordinates(grd) &lt;- c(\"X\", \"Y\")\ngridded(grd)     &lt;- TRUE  # Create SpatialPixel object\nfullgrid(grd)    &lt;- TRUE  # Create SpatialGrid object\nproj4string(grd) &lt;- proj4string(as(aq.sum, \"Spatial\"))\n# Define the polynomial equation\nf.0  &lt;- as.formula(meanpm25 ~ 1)\n\n# Run the regression model\nlm.0 &lt;- lm( f.0 , data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ndat.0th &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.0, newdata=grd)))\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.0th)\nr.m &lt;- mask(r, st_as_sf(id.cty))"
  },
  {
    "objectID": "slides/20-slides.html#st-order-trend-surface",
    "href": "slides/20-slides.html#st-order-trend-surface",
    "title": "Areal Data and Proximity",
    "section": "1st Order Trend Surface",
    "text": "1st Order Trend Surface\n\n\n\nCreates a slanted surface\n\\(Z = a + bX + cY\\)\nX and Y are the coordinate pairs"
  },
  {
    "objectID": "slides/20-slides.html#st-order-trend-surface-1",
    "href": "slides/20-slides.html#st-order-trend-surface-1",
    "title": "Areal Data and Proximity",
    "section": "1st Order Trend Surface",
    "text": "1st Order Trend Surface\n\n# Define the polynomial equation\nf.1  &lt;- as.formula(meanpm25 ~ X + Y)\n\naq.sum$X &lt;- st_coordinates(aq.sum)[,1]\naq.sum$Y &lt;- st_coordinates(aq.sum)[,2]\n\n# Run the regression model\nlm.1 &lt;- lm( f.1 , data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ndat.1st &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.1, newdata=grd)))\n\n# Convert to raster object to take advantage of rasterVis' imaging\n# environment\nr   &lt;- rast(dat.1st)\nr.m &lt;- mask(r, st_as_sf(id.cty))"
  },
  {
    "objectID": "slides/20-slides.html#nd-order-trend-surfaces",
    "href": "slides/20-slides.html#nd-order-trend-surfaces",
    "title": "Areal Data and Proximity",
    "section": "2nd Order Trend Surfaces",
    "text": "2nd Order Trend Surfaces\n\n\n\nProduces a parabolic surface\n\\(Z = a + bX + cY + dX^2 + eY^2 + fXY\\)\nHighlights the interaction of both directions"
  },
  {
    "objectID": "slides/20-slides.html#nd-order-trend-surfaces-1",
    "href": "slides/20-slides.html#nd-order-trend-surfaces-1",
    "title": "Areal Data and Proximity",
    "section": "2nd Order Trend Surfaces",
    "text": "2nd Order Trend Surfaces\n\n# Define the 1st order polynomial equation\nf.2 &lt;- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n \n# Run the regression model\nlm.2 &lt;- lm( f.2, data=aq.sum)\n\n# Use the regression model output to interpolate the surface\ndat.2nd &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.2, newdata=grd))) \n\nr   &lt;- rast(dat.2nd)\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\ntm_shape(r.m) + tm_raster(n=10, palette=\"RdBu\", title=\"Predicted air quality\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "slides/20-slides.html#kriging",
    "href": "slides/20-slides.html#kriging",
    "title": "Areal Data and Proximity",
    "section": "Kriging",
    "text": "Kriging\n\n\nPrevious methods predict \\(z\\) as a (weighted) function of distance\nTreat the observations as perfect (no error)\nIf we imagine that \\(z\\) is the outcome of some spatial process such that:\n\n\\[\n\\begin{equation}\nz(\\mathbf{x}) = \\mu(\\mathbf{x}) + \\epsilon(\\mathbf{x})\n\\end{equation}\n\\]\nthen any observed value of \\(z\\) is some function of the process (\\(\\mu(\\mathbf{x})\\)) and some error (\\(\\epsilon(\\mathbf{x})\\))\n\nKriging exploits autocorrelation in \\(\\epsilon(\\mathbf{x})\\) to identify the trend and interpolate accordingly"
  },
  {
    "objectID": "slides/20-slides.html#autocorrelation",
    "href": "slides/20-slides.html#autocorrelation",
    "title": "Areal Data and Proximity",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nCorrelation the tendency for two variables to be related\nAutocorrelation the tendency for observations that are closer (in space or time) to be correlated\nPositive autocorrelation neighboring observations have \\(\\epsilon\\) with the same sign\nNegative autocorrelation neighboring observations have \\(\\epsilon\\) with a different sign (rare in geography)"
  },
  {
    "objectID": "slides/20-slides.html#ordinary-kriging",
    "href": "slides/20-slides.html#ordinary-kriging",
    "title": "Areal Data and Proximity",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging\n\nAssumes that the deterministic part of the process (\\(\\mu(\\mathbf{x})\\)) is an unknown constant (\\(\\mu\\))\n\n\\[\n\\begin{equation}\nz(\\mathbf{x}) = \\mu + \\epsilon(\\mathbf{x})\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/20-slides.html#steps-for-ordinary-kriging",
    "href": "slides/20-slides.html#steps-for-ordinary-kriging",
    "title": "Areal Data and Proximity",
    "section": "Steps for Ordinary Kriging",
    "text": "Steps for Ordinary Kriging\n\nRemoving any spatial trend in the data (if present).\nComputing the experimental variogram, \\(\\gamma\\), which is a measure of spatial autocorrelation.\nDefining an experimental variogram model that best characterizes the spatial autocorrelation in the data.\nInterpolating the surface using the experimental variogram.\nAdding the kriged interpolated surface to the trend interpolated surface to produce the final output."
  },
  {
    "objectID": "slides/20-slides.html#removing-spatial-trend",
    "href": "slides/20-slides.html#removing-spatial-trend",
    "title": "Areal Data and Proximity",
    "section": "Removing Spatial Trend",
    "text": "Removing Spatial Trend\n\nMean and variance need to be constant across study area\nTrend surfaces indicate that is not the case\nNeed to remove that trend\n\n\nf.2 &lt;- as.formula(meanpm25 ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y))\n \n# Run the regression model\nlm.2 &lt;- lm( f.2, data=aq.sum)\n\n# Copy the residuals to the point object\naq.sum$res &lt;- lm.2$residuals"
  },
  {
    "objectID": "slides/20-slides.html#removing-the-trend",
    "href": "slides/20-slides.html#removing-the-trend",
    "title": "Areal Data and Proximity",
    "section": "Removing the trend",
    "text": "Removing the trend"
  },
  {
    "objectID": "slides/20-slides.html#calculate-the-experimental-variogram",
    "href": "slides/20-slides.html#calculate-the-experimental-variogram",
    "title": "Areal Data and Proximity",
    "section": "Calculate the experimental variogram",
    "text": "Calculate the experimental variogram\n\nnugget - the proportion of semivariance that occurs at small distances\nsill - the maximum semivariance between pairs of observations\nrange - the distance at which the sill occurs\nexperimental vs. fitted variograms"
  },
  {
    "objectID": "slides/20-slides.html#a-note-about-semivariograms",
    "href": "slides/20-slides.html#a-note-about-semivariograms",
    "title": "Areal Data and Proximity",
    "section": "A Note about Semivariograms",
    "text": "A Note about Semivariograms"
  },
  {
    "objectID": "slides/20-slides.html#fitted-semivariograms",
    "href": "slides/20-slides.html#fitted-semivariograms",
    "title": "Areal Data and Proximity",
    "section": "Fitted Semivariograms",
    "text": "Fitted Semivariograms\n\nRely on functional forms to model semivariance"
  },
  {
    "objectID": "slides/20-slides.html#calculate-the-experimental-variogram-1",
    "href": "slides/20-slides.html#calculate-the-experimental-variogram-1",
    "title": "Areal Data and Proximity",
    "section": "Calculate the experimental variogram",
    "text": "Calculate the experimental variogram\n\nvar.cld  &lt;- gstat::variogram(res ~ 1, aq.sum, cloud = TRUE)\nvar.df  &lt;- as.data.frame(var.cld)\nindex1  &lt;- which(with(var.df, left==21 & right==2))\n\nOP &lt;- par( mar=c(4,6,1,1))\nplot(var.cld$dist/1000 , var.cld$gamma, col=\"grey\", \n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( frac((res[2] - res[1])^2 , 2)) )\n\npar(OP)"
  },
  {
    "objectID": "slides/20-slides.html#simplifying-the-cloud-plot",
    "href": "slides/20-slides.html#simplifying-the-cloud-plot",
    "title": "Areal Data and Proximity",
    "section": "Simplifying the cloud plot",
    "text": "Simplifying the cloud plot\n\n# Compute the sample experimental variogram\nvar.smpl &lt;- gstat::variogram(f.2, aq.sum, cloud = FALSE)\n\nbins.ct &lt;- c(0, var.smpl$dist , max(var.cld$dist) )\nbins &lt;- vector()\nfor (i in 1: (length(bins.ct) - 1) ){\n  bins[i] &lt;- mean(bins.ct[ seq(i,i+1, length.out=2)] ) \n}\nbins[length(bins)] &lt;- max(var.cld$dist)\nvar.bins &lt;- findInterval(var.cld$dist, bins)\n\n# Point data cloud with bin boundaries\nOP &lt;- par( mar = c(5,6,1,1))\nplot(var.cld$gamma ~ eval(var.cld$dist/1000), col=rgb(0,0,0,0.2), pch=16, cex=0.7,\n     xlab = \"Distance between point pairs (km)\",\n     ylab = expression( gamma ) )\npoints( var.smpl$dist/1000, var.smpl$gamma, pch=21, col=\"black\", bg=\"red\", cex=1.3)\nabline(v=bins/1000, col=\"red\", lty=2)\n\n\n\npar(OP)"
  },
  {
    "objectID": "slides/20-slides.html#looking-at-the-sample-variogram",
    "href": "slides/20-slides.html#looking-at-the-sample-variogram",
    "title": "Areal Data and Proximity",
    "section": "Looking at the sample Variogram",
    "text": "Looking at the sample Variogram"
  },
  {
    "objectID": "slides/20-slides.html#estimating-the-sample-variogram",
    "href": "slides/20-slides.html#estimating-the-sample-variogram",
    "title": "Areal Data and Proximity",
    "section": "Estimating the sample variogram",
    "text": "Estimating the sample variogram\n\nvar.smpl &lt;- gstat::variogram(f.2, aq.sum, cloud = FALSE, cutoff = 1000000)\n\n\n# Compute the variogram model by passing the nugget, sill and range values\n# to fit.variogram() via the vgm() function.\ndat.fit  &lt;- gstat::fit.variogram(var.smpl, gstat::vgm(nugget = 12, range= 60000, model=\"Gau\", cutoff=1000000))"
  },
  {
    "objectID": "slides/20-slides.html#ordinary-kriging-1",
    "href": "slides/20-slides.html#ordinary-kriging-1",
    "title": "Areal Data and Proximity",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging\n\n\n[using ordinary kriging]"
  },
  {
    "objectID": "slides/20-slides.html#ordinary-kriging-2",
    "href": "slides/20-slides.html#ordinary-kriging-2",
    "title": "Areal Data and Proximity",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging\n\ndat.krg &lt;- gstat::krige( res~1, as(aq.sum, \"Spatial\"), grd, dat.fit)"
  },
  {
    "objectID": "slides/20-slides.html#combining-with-the-trend-data",
    "href": "slides/20-slides.html#combining-with-the-trend-data",
    "title": "Areal Data and Proximity",
    "section": "Combining with the trend data",
    "text": "Combining with the trend data\n\n\n[using universal kriging]"
  },
  {
    "objectID": "slides/20-slides.html#combining-with-the-trend-data-1",
    "href": "slides/20-slides.html#combining-with-the-trend-data-1",
    "title": "Areal Data and Proximity",
    "section": "Combining with the trend data",
    "text": "Combining with the trend data\n\ndat.krg &lt;- gstat::krige( f.2, as(aq.sum, \"Spatial\"), grd, dat.fit)\n\nr &lt;- rast(dat.krg)$var1.pred\nr.m &lt;- mask(r, st_as_sf(id.cty))\n\n# Plot the raster and the sampled points\ntm_shape(r.m) + tm_raster(n=10, palette=\"RdBu\", title=\"Predicted air quality\") +tm_shape(aq.sum) + tm_dots(size=0.2) +\n  tm_legend(legend.outside=TRUE)"
  },
  {
    "objectID": "slides/20-slides.html#visualizing-uncertainty",
    "href": "slides/20-slides.html#visualizing-uncertainty",
    "title": "Areal Data and Proximity",
    "section": "Visualizing Uncertainty",
    "text": "Visualizing Uncertainty"
  },
  {
    "objectID": "slides/20-slides.html#universal-kriging",
    "href": "slides/20-slides.html#universal-kriging",
    "title": "Areal Data and Proximity",
    "section": "Universal Kriging",
    "text": "Universal Kriging\n\nAssumes that the deterministic part of the process (\\(\\mu(\\mathbf{x})\\)) is now a function of the location \\(\\mathbf{x}\\)\nCould be the location or some other attribute\nNow y is a function of some aspect of x\n\n\nvu &lt;- variogram(log(zinc)~elev, ~x+y, data=meuse)\nmu &lt;- fit.variogram(vu, vgm(1, \"Sph\", 300, 1))\ngUK &lt;- gstat(NULL, \"log.zinc\", log(zinc)~elev, meuse, locations=~x+y, model=mu)\nnames(r) &lt;- \"elev\"\nUK &lt;- interpolate(r, gUK, debug.level=0)"
  },
  {
    "objectID": "slides/20-slides.html#universal-kriging-1",
    "href": "slides/20-slides.html#universal-kriging-1",
    "title": "Areal Data and Proximity",
    "section": "Universal Kriging",
    "text": "Universal Kriging"
  },
  {
    "objectID": "slides/20-slides.html#universal-kriging-2",
    "href": "slides/20-slides.html#universal-kriging-2",
    "title": "Areal Data and Proximity",
    "section": "Universal Kriging",
    "text": "Universal Kriging\n\nvu &lt;- variogram(log(zinc)~x + x^2 + y + y^2, ~x+y, data=meuse)\nmu &lt;- fit.variogram(vu, vgm(1, \"Sph\", 300, 1))\ngUK &lt;- gstat(NULL, \"log.zinc\", log(zinc)~x + x^2 + y + y^2, meuse, locations=~x+y, model=mu)\nnames(r) &lt;- \"elev\"\nUK &lt;- interpolate(r, gUK, debug.level=0)"
  },
  {
    "objectID": "slides/20-slides.html#universal-kriging-3",
    "href": "slides/20-slides.html#universal-kriging-3",
    "title": "Areal Data and Proximity",
    "section": "Universal Kriging",
    "text": "Universal Kriging"
  },
  {
    "objectID": "slides/20-slides.html#co-kriging",
    "href": "slides/20-slides.html#co-kriging",
    "title": "Areal Data and Proximity",
    "section": "Co-Kriging",
    "text": "Co-Kriging\n\nrelies on autocorrelation in \\(\\epsilon_1(\\mathbf{x})\\) for \\(z_1\\) AND cross correlation with other variables (\\(z_{2...j}\\))\nExtending the ordinary kriging model gives:\n\n\\[\n\\begin{equation}\nz_1(\\mathbf{x}) = \\mu_1 + \\epsilon_1(\\mathbf{x})\\\\\nz_2(\\mathbf{x}) = \\mu_2 + \\epsilon_2(\\mathbf{x})\n\\end{equation}\n\\] * Note that there is autocorrelation within both \\(z_1\\) and \\(z_2\\) (because of the \\(\\epsilon\\)) and cross-correlation (because of the location, \\(\\mathbf{x}\\))\n\nNot required that all variables are measured at exactly the same points"
  },
  {
    "objectID": "slides/20-slides.html#co-kriging-1",
    "href": "slides/20-slides.html#co-kriging-1",
    "title": "Areal Data and Proximity",
    "section": "Co-Kriging",
    "text": "Co-Kriging\n\nProcess is just a linked series of gstat calls\n\n\ngCoK &lt;- gstat(NULL, 'log.zinc', log(zinc)~1, meuse, locations=~x+y)\ngCoK &lt;- gstat(gCoK, 'elev', elev~1, meuse, locations=~x+y)\ngCoK &lt;- gstat(gCoK, 'cadmium', cadmium~1, meuse, locations=~x+y)\ncoV &lt;- variogram(gCoK)\ncoV.fit &lt;- fit.lmc(coV, gCoK, vgm(model='Sph', range=1000))\n\ncoK &lt;- interpolate(r, coV.fit, debug.level=0)"
  },
  {
    "objectID": "slides/20-slides.html#co-kriging-2",
    "href": "slides/20-slides.html#co-kriging-2",
    "title": "Areal Data and Proximity",
    "section": "Co-Kriging",
    "text": "Co-Kriging"
  },
  {
    "objectID": "slides/20-slides.html#co-kriging-3",
    "href": "slides/20-slides.html#co-kriging-3",
    "title": "Areal Data and Proximity",
    "section": "Co-Kriging",
    "text": "Co-Kriging"
  },
  {
    "objectID": "slides/20-slides.html#a-note-about-semivariograms-1",
    "href": "slides/20-slides.html#a-note-about-semivariograms-1",
    "title": "Areal Data and Proximity",
    "section": "A Note about Semivariograms",
    "text": "A Note about Semivariograms"
  },
  {
    "objectID": "slides/18-slides.html#what-is-a-point-pattern",
    "href": "slides/18-slides.html#what-is-a-point-pattern",
    "title": "Point Pattern Analysis",
    "section": "What is a point pattern?",
    "text": "What is a point pattern?\n\n\n\n\nPoint pattern: A set of events within a study region (i.e., a window) generated by a random process\nSet: A collection of mathematical events\nEvents: The existence of a point object of the type we are interested in at a particular location in the study region\nA marked point pattern refers to a point pattern where the events have additional descriptors\n\n\n\n\nSome notation:\n\n\\(S\\): refers to the entire set\n\\(\\mathbf{s_i}\\) denotes the vector of data describing point \\(s_i\\) in set \\(S\\)\n\\(\\#(S \\in A )\\) refers to the number of points in \\(S\\) within study area \\(A\\)"
  },
  {
    "objectID": "slides/18-slides.html#requirements-for-a-set-to-be-considered-a-point-pattern",
    "href": "slides/18-slides.html#requirements-for-a-set-to-be-considered-a-point-pattern",
    "title": "Point Pattern Analysis",
    "section": "Requirements for a set to be considered a point pattern",
    "text": "Requirements for a set to be considered a point pattern\n\nThe pattern must be mapped on a plane to preserve distance\nThe study area, \\(A\\), should be objectively determined\nThere should be a \\(1:1\\) correspondence between objects in \\(A\\) and events in the pattern\nEvents must be proper i.e., refer to actual locations of the event\nFor some analyses the pattern should be a census of the relevant events"
  },
  {
    "objectID": "slides/18-slides.html#describing-point-patterns",
    "href": "slides/18-slides.html#describing-point-patterns",
    "title": "Point Pattern Analysis",
    "section": "Describing Point Patterns",
    "text": "Describing Point Patterns\n\n\n\n\nDensity-based metrics: the \\(\\#\\) of points within area, \\(a\\), in study area \\(A\\)\nDistance-based metrics: based on nearest neighbor distances or the distance matrix for all points\nFirst order effects reflect variation in intensity due to variation in the ‘attractiveness’ of locations\nSecond order effects reflect variation in intensity due to the presence of points themselves\n\n\n\n\n\n\nfrom Manuel Gimond"
  },
  {
    "objectID": "slides/18-slides.html#centrography",
    "href": "slides/18-slides.html#centrography",
    "title": "Point Pattern Analysis",
    "section": "Centrography",
    "text": "Centrography\n\n\n\n\nMean center: the point, \\(\\hat{\\mathbf{s}}\\), whose coordinates are the average of all events in the pattern\nStandard distance: a measure of the dispersion of points around the mean center\nStandard ellipse: dispersion in one dimension\n\n\n\n\n\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/18-slides.html#analyzing-point-patterns",
    "href": "slides/18-slides.html#analyzing-point-patterns",
    "title": "Point Pattern Analysis",
    "section": "Analyzing Point Patterns",
    "text": "Analyzing Point Patterns\n\nModeling random processes means we are interested in probability densities of the points (first-order;density)\nAlso interested in how the presence of some events affects the probability of other events (second-order;distance)\nFinally interested in how the attributes of an event affect location (marked)\nNeed to introduce a few new packages (spatstat and gstat)"
  },
  {
    "objectID": "slides/18-slides.html#density-based-methods",
    "href": "slides/18-slides.html#density-based-methods",
    "title": "Point Pattern Analysis",
    "section": "Density based methods",
    "text": "Density based methods\n\n\n\nThe overall intensity of a point pattern is a crude density estimate\n\n\\[\n\\begin{equation}\n\\hat{\\lambda} = \\frac{\\#(S \\in A )}{a}\n\\end{equation}\n\\] * Local density = quadrat counts"
  },
  {
    "objectID": "slides/18-slides.html#kernel-density-estimates-kde",
    "href": "slides/18-slides.html#kernel-density-estimates-kde",
    "title": "Point Pattern Analysis",
    "section": "Kernel Density Estimates (KDE)",
    "text": "Kernel Density Estimates (KDE)\n\\[\n\\begin{equation}\n\\hat{f}(x) = \\frac{1}{nh_xh_y} \\sum_{i=1}^n k\\bigg(\\frac{{x-x_i}}{h_x},\\frac{{y-y_i}}{h_y} \\bigg)\n\\end{equation}\n\\]\n\n\nAssume each location in \\(\\mathbf{s_i}\\) drawn from unknown distribution\nDistribution has probability density \\(f(\\mathbf{x})\\)\nEstimate \\(f(\\mathbf{x})\\) by averaging probability “bumps” around each location\nNeed different object types for most operations in R (as.ppp)"
  },
  {
    "objectID": "slides/18-slides.html#kernel-density-estimates-kde-1",
    "href": "slides/18-slides.html#kernel-density-estimates-kde-1",
    "title": "Point Pattern Analysis",
    "section": "Kernel Density Estimates (KDE)",
    "text": "Kernel Density Estimates (KDE)\n\n\n\\(h\\) is the bandwidth and \\(k\\) is the kernel\nWe can use stats::density to explore\nkernel: defines the shape, size, and weight assigned to observations in the window\nbandwidth often assigned based on distance from the window center\n\n\nx &lt;- rpoispp(lambda =50)\nK1 &lt;- density(x, bw=2)\nK2 &lt;- density(x, bw=10)\nK3 &lt;- density(x, bw=2, kernel=\"disc\")"
  },
  {
    "objectID": "slides/18-slides.html#choosing-bandwidths-and-kernels",
    "href": "slides/18-slides.html#choosing-bandwidths-and-kernels",
    "title": "Point Pattern Analysis",
    "section": "Choosing bandwidths and kernels",
    "text": "Choosing bandwidths and kernels\n\nSmall values for \\(h\\) give ‘spiky’ densities\nLarge values for \\(h\\) smooth much more\nSome kernels have optimal bandwidth detection\ntmap package provides additional functionality"
  },
  {
    "objectID": "slides/18-slides.html#second-order-analysis-1",
    "href": "slides/18-slides.html#second-order-analysis-1",
    "title": "Point Pattern Analysis",
    "section": "Second-Order Analysis",
    "text": "Second-Order Analysis\n\nKDEs assume independence of points (first order randomness)\nSecond-order methods allow dependence amongst points (second-order randomness)\nSeveral functions for assessing second order dependence (\\(K\\), \\(L\\), and \\(G\\))"
  },
  {
    "objectID": "slides/18-slides.html#distance-based-metrics",
    "href": "slides/18-slides.html#distance-based-metrics",
    "title": "Point Pattern Analysis",
    "section": "Distance based metrics",
    "text": "Distance based metrics\n\nProvide an estimate of the second order effects\nMean nearest-neighbor distance: \\[\\hat{d}_{min} = \\frac{\\sum_{i = 1}^{m} d_{min}(\\mathbf{s_i})}{n}\\]"
  },
  {
    "objectID": "slides/18-slides.html#nearest-neighbor-distance",
    "href": "slides/18-slides.html#nearest-neighbor-distance",
    "title": "Point Pattern Analysis",
    "section": "Nearest-neighbor distance",
    "text": "Nearest-neighbor distance\n\nANN &lt;- apply(nndist(x, k=1:50),2,FUN=mean)\nplot(ANN ~ eval(1:50), type=\"b\", main=NULL, las=1)"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function",
    "href": "slides/18-slides.html#ripleys-k-function",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\n\nNearest neighbor methods throw away a lot of information\nIf points have independent, fixed marginal densities, then they exhibit complete, spatial randomness (CSR)\nThe K function is an alternative, based on a series of circles with increasing radius\n\n\\[\n\\begin{equation}\nK(d) = \\lambda^{-1}E(N_d)\n\\end{equation}\n\\]\n\nWe can test for clustering by comparing to the expectation:\n\n\\[\n\\begin{equation}\nK_{CSR}(d) = \\pi d^2\n\\end{equation}\n\\]\n\nif \\(k(d) &gt; K_{CSR}(d)\\) then there is clustering at the scale defined by \\(d\\)"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function-1",
    "href": "slides/18-slides.html#ripleys-k-function-1",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nWhen working with a sample the distribution of \\(K\\) is unknown\nEstimate with\n\n\\[\n\\begin{equation}\n\\hat{K}(d) = \\hat{\\lambda}^{-1}\\sum_{i=1}^n\\sum_{j=1}^n\\frac{I(d_{ij} &lt;d)}{n(n-1)}\n\\end{equation}\n\\]\nwhere:\n\\[\n\\begin{equation}\n\\hat{\\lambda} = \\frac{n}{|A|}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function-2",
    "href": "slides/18-slides.html#ripleys-k-function-2",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nUsing the spatstat package"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function-3",
    "href": "slides/18-slides.html#ripleys-k-function-3",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nkf &lt;- Kest(bramblecanes, correction-\"border\")\nplot(kf)"
  },
  {
    "objectID": "slides/18-slides.html#ripleys-k-function-4",
    "href": "slides/18-slides.html#ripleys-k-function-4",
    "title": "Point Pattern Analysis",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\naccounting for variation in \\(d\\)\n\n\nkf.env &lt;- envelope(bramblecanes, correction=\"border\", envelope = FALSE, verbose = FALSE)\nplot(kf.env)"
  },
  {
    "objectID": "slides/18-slides.html#other-functions",
    "href": "slides/18-slides.html#other-functions",
    "title": "Point Pattern Analysis",
    "section": "Other functions",
    "text": "Other functions\n\n\n\n\\(L\\) function: square root transformation of \\(K\\)\n\\(G\\) function: the cummulative frequency distribution of the nearest neighbor distances\n\\(F\\) function: similar to \\(G\\) but based on randomly located points"
  },
  {
    "objectID": "slides/16-slides.html#objectives",
    "href": "slides/16-slides.html#objectives",
    "title": "Integrating Rasters and Vector Data",
    "section": "Objectives",
    "text": "Objectives\n\nUse dplyr with predicates and measures to subset and manipulate data\nUse extract to access raster data\nUse zonal to summarize access data\nJoin data into a single analyzable dataframe"
  },
  {
    "objectID": "slides/16-slides.html#thinking-about-the-data",
    "href": "slides/16-slides.html#thinking-about-the-data",
    "title": "Integrating Rasters and Vector Data",
    "section": "Thinking about the data",
    "text": "Thinking about the data\n\nDatasets - Forest Service Boundaries, CFLRP Boundaries, Wildfire Risk Raster, CEJST shapefile\nDependent Variable - CFLRP (T or F)\nIndependent Variables - Wildfire hazard, income, education, housing burdent"
  },
  {
    "objectID": "slides/16-slides.html#building-some-pseudocode",
    "href": "slides/16-slides.html#building-some-pseudocode",
    "title": "Integrating Rasters and Vector Data",
    "section": "Building some Pseudocode",
    "text": "Building some Pseudocode\n\n1. Load Libraries\n2. Load data\n3. Check validity and alignment\n4. Subset to relevant geographies\n5. Select relevant attributes\n6. Extract wildfire risk\n7. CFLRP T or F"
  },
  {
    "objectID": "slides/16-slides.html#load-libraries",
    "href": "slides/16-slides.html#load-libraries",
    "title": "Integrating Rasters and Vector Data",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)"
  },
  {
    "objectID": "slides/16-slides.html#load-the-data",
    "href": "slides/16-slides.html#load-the-data",
    "title": "Integrating Rasters and Vector Data",
    "section": "Load the data",
    "text": "Load the data\n\nDownloading USFS data using tempfiles and unzip\n\n\n### FS Boundaries\ntmp &lt;- tempfile()\nfs.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.AdministrativeForest.zip\"\ndownload.file(fs.url, tmp)\ntmp2 &lt;- tempfile()\nunzip(zipfile=tmp, exdir = tmp2 )\n\nfs.bdry &lt;- read_sf(tmp2)\n\n### CFLRP Data\ntmp &lt;- tempfile()\ncflrp.url &lt;- \"https://data.fs.usda.gov/geodata/edw/edw_resources/shp/S_USA.CFLR_HPRP_ProjectBoundary.zip\"\ndownload.file(cflrp.url, tmp)\ntmp2 &lt;- tempfile()\nunzip(zipfile=tmp, exdir = tmp2 )\ncflrp.bdry &lt;- read_sf(tmp2)"
  },
  {
    "objectID": "slides/16-slides.html#load-the-data-1",
    "href": "slides/16-slides.html#load-the-data-1",
    "title": "Integrating Rasters and Vector Data",
    "section": "Load the data",
    "text": "Load the data\n\nFrom our class folder\n\n\n### Wildfire Hazard Data\nwildfire.haz &lt;- rast(\"opt/data/2023/assignment01/wildfire_hazard_agg.tif\")\n\n## CEJST data\ncejst &lt;- read_sf(\"opt/data/2023/assignment01/cejst_pnw.shp\")"
  },
  {
    "objectID": "slides/16-slides.html#check-validity",
    "href": "slides/16-slides.html#check-validity",
    "title": "Integrating Rasters and Vector Data",
    "section": "Check Validity",
    "text": "Check Validity\n\nThe USFS datasets are new; let’s check the geometries\n\n\nall(st_is_valid(fs.bdry))\n\n[1] FALSE\n\nall(st_is_valid(cflrp.bdry))\n\n[1] FALSE\n\n\n\nMake them valid\n\n\nfs.bdry.valid &lt;- st_make_valid(fs.bdry)\nall(st_is_valid(fs.bdry.valid))\n\n[1] TRUE\n\ncflrp.bdry.valid &lt;- st_make_valid(cflrp.bdry)\nall(st_is_valid(cflrp.bdry.valid))\n\n[1] TRUE"
  },
  {
    "objectID": "slides/16-slides.html#set-projection",
    "href": "slides/16-slides.html#set-projection",
    "title": "Integrating Rasters and Vector Data",
    "section": "Set Projection",
    "text": "Set Projection\n\nWe know these are in different CRS\nProject to the CRS of the raster\nUsing %&gt;% to pipe data through the function\n\n\ncflrp.proj &lt;- cflrp.bdry.valid %&gt;% st_transform(., crs=crs(wildfire.haz))\ncejst.proj &lt;- cejst %&gt;% st_transform(., crs=crs(wildfire.haz))\nfs.proj &lt;- fs.bdry.valid %&gt;% st_transform(., crs=crs(wildfire.haz))"
  },
  {
    "objectID": "slides/16-slides.html#subset-geometries",
    "href": "slides/16-slides.html#subset-geometries",
    "title": "Integrating Rasters and Vector Data",
    "section": "Subset Geometries",
    "text": "Subset Geometries\n\nWe can use the [] notation to subset the one dataset based on the geometry of the other\nNeed USFS and CFLRP within the region\nThen need tracts that overlap USFS\n\n\nfs.subset &lt;- fs.proj[cejst.proj, ]\ncflrp.subset &lt;- cflrp.proj[cejst.proj, ]\ncejst.subset &lt;- cejst.proj[fs.subset, ]"
  },
  {
    "objectID": "slides/16-slides.html#subset-geometries-1",
    "href": "slides/16-slides.html#subset-geometries-1",
    "title": "Integrating Rasters and Vector Data",
    "section": "Subset geometries",
    "text": "Subset geometries\n\n\n\nsub.map &lt;- tm_shape(cejst.subset) +\n  tm_polygons(col=\"gray\") +\n  tm_shape(fs.subset) + \n  tm_polygons(col=\"darkgreen\") +\n  tm_shape(cflrp.subset) +\n  tm_polygons(col=\"goldenrod\")"
  },
  {
    "objectID": "slides/16-slides.html#select-relevant-columns",
    "href": "slides/16-slides.html#select-relevant-columns",
    "title": "Integrating Rasters and Vector Data",
    "section": "Select Relevant Columns",
    "text": "Select Relevant Columns\n\nUse the codebook to identify the right columns\nThen use select from dplyr\ngeometries are sticky!\n\n\ncejst.df &lt;- cejst.subset %&gt;% \n  select(GEOID10, LMI_PFS, LHE, HBF_PFS)\nhead(cejst.df)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -1598729 ymin: 2388182 xmax: -1475201 ymax: 3000813\nProjected CRS: unnamed\n# A tibble: 6 × 5\n  GEOID10     LMI_PFS   LHE HBF_PFS                                     geometry\n  &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;                           &lt;MULTIPOLYGON [m]&gt;\n1 16025970100    0.75     0    0.6  (((-1485848 2427049, -1485813 2426977, -148…\n2 16057005500    0.43     0    0.44 (((-1567845 2843218, -1567803 2843209, -156…\n3 16057005600    0.3      0    0.05 (((-1573408 2823058, -1573412 2823071, -157…\n4 16009950100    0.55     1    0.4  (((-1566013 2857573, -1565966 2857623, -156…\n5 16017950500    0.75     1    0.53 (((-1545064 2973813, -1545058 2973793, -154…\n6 16017950400    0.6      1    0.68 (((-1544958 2973552, -1544957 2973564, -154…"
  },
  {
    "objectID": "slides/16-slides.html#extract-wildfire-data",
    "href": "slides/16-slides.html#extract-wildfire-data",
    "title": "Integrating Rasters and Vector Data",
    "section": "Extract wildfire data",
    "text": "Extract wildfire data\n\nCan use zonal for one summary statistic\nOr extract for multiple\n\n\nwildfire.zones &lt;- terra::zonal(wildfire.haz, vect(cejst.df), fun=\"mean\", na.rm=TRUE)\n\nhead(wildfire.zones)\n\n     WHP_ID\n1 2997.7951\n2  182.8864\n3  386.9580\n4  173.1703\n5  193.4199\n6  210.4406"
  },
  {
    "objectID": "slides/14-slides.html#objectives",
    "href": "slides/14-slides.html#objectives",
    "title": "Raster Data: II",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset."
  },
  {
    "objectID": "slides/14-slides.html#why-use-moving-windows",
    "href": "slides/14-slides.html#why-use-moving-windows",
    "title": "Raster Data: II",
    "section": "Why use moving windows?",
    "text": "Why use moving windows?\n\nTo create new data that reflects “neighborhood” data\nTo smooth out values\nTo detect (and fill) holes or edges\nChange the thematic scale of your data (without changing resolution)"
  },
  {
    "objectID": "slides/14-slides.html#what-is-a-moving-window",
    "href": "slides/14-slides.html#what-is-a-moving-window",
    "title": "Raster Data: II",
    "section": "What is a moving window?",
    "text": "What is a moving window?"
  },
  {
    "objectID": "slides/14-slides.html#implementing-moving-windows-in-r",
    "href": "slides/14-slides.html#implementing-moving-windows-in-r",
    "title": "Raster Data: II",
    "section": "Implementing Moving Windows in R",
    "text": "Implementing Moving Windows in R\n\nUse the focal function in terra\n\nfocal(x, w=3, fun=\"sum\", ..., na.policy=\"all\", fillvalue=NA,          expand=FALSE, silent=TRUE, filename=\"\", overwrite=FALSE, wopt=list())"
  },
  {
    "objectID": "slides/14-slides.html#focal-for-continuous-rasters",
    "href": "slides/14-slides.html#focal-for-continuous-rasters",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(spData)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nsrtm3  &lt;-  focal(x = srtm, w = 3)\nsrtm9  &lt;-  focal(x = srtm, w = 9)\nsrtm21  &lt;-  focal(x = srtm, w = 21)"
  },
  {
    "objectID": "slides/14-slides.html#focal-for-continuous-rasters-1",
    "href": "slides/14-slides.html#focal-for-continuous-rasters-1",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters"
  },
  {
    "objectID": "slides/14-slides.html#focal-for-continuous-rasters-2",
    "href": "slides/14-slides.html#focal-for-continuous-rasters-2",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters\n\nsrtmsum  &lt;-  focal(x = srtm, w = 3, fun=\"sum\")\nsrtmmax  &lt;-  focal(x = srtm, w = 9, fun=\"mean\")\nsrtmmin  &lt;-  focal(x = srtm, w = 21, fun=\"min\")"
  },
  {
    "objectID": "slides/14-slides.html#focal-for-continuous-rasters-3",
    "href": "slides/14-slides.html#focal-for-continuous-rasters-3",
    "title": "Raster Data: II",
    "section": "focal for Continuous Rasters",
    "text": "focal for Continuous Rasters"
  },
  {
    "objectID": "slides/14-slides.html#focal-for-continous-rasters",
    "href": "slides/14-slides.html#focal-for-continous-rasters",
    "title": "Raster Data: II",
    "section": "focal for Continous Rasters",
    "text": "focal for Continous Rasters\n\ncan alter the size and shape of window by providing a weights matrix for w\nCan create different custom functions for fun (see the help file)\nna.policy for filling holes or avoiding them"
  },
  {
    "objectID": "slides/14-slides.html#reclassification-1",
    "href": "slides/14-slides.html#reclassification-1",
    "title": "Raster Data: II",
    "section": "Reclassification",
    "text": "Reclassification\n\nCreate new data based on the presence of a particular class(es) of interest\nCombine classes in a categorical map\nUseful as inputs for overlay analyses"
  },
  {
    "objectID": "slides/14-slides.html#reclassifying-rasters-in-r",
    "href": "slides/14-slides.html#reclassifying-rasters-in-r",
    "title": "Raster Data: II",
    "section": "Reclassifying rasters in R",
    "text": "Reclassifying rasters in R\n\nUsing [] and conditionals\n\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nsrtm.lowelev &lt;- srtm\nsrtm.lowelev[srtm.lowelev &gt; 2500] &lt;- 1\nplot(srtm.lowelev)"
  },
  {
    "objectID": "slides/14-slides.html#reclassifying-rasters-in-r-1",
    "href": "slides/14-slides.html#reclassifying-rasters-in-r-1",
    "title": "Raster Data: II",
    "section": "Reclassifying rasters in R",
    "text": "Reclassifying rasters in R\n\nUsing [] and conditionals\n\n\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\n\n\nsrtm.na &lt;- srtm\nsrtm.na[200:300, 200:300] &lt;- NA\nsrtm.na[is.na(srtm.na)] &lt;- 8000\nplot(srtm.na)"
  },
  {
    "objectID": "slides/14-slides.html#reclassifying-categorical-rasters",
    "href": "slides/14-slides.html#reclassifying-categorical-rasters",
    "title": "Raster Data: II",
    "section": "Reclassifying Categorical Rasters",
    "text": "Reclassifying Categorical Rasters\n\nNeed a classification matrix\nUse classify\n\n\nmintemp &lt;- rast(\"ftp://ftp.hafro.is/pub/data/rasters/Iceland_minbtemp.tif\")\ncm &lt;- matrix(c(\n  -2, 2, 0,\n  2, 4, 1,\n  4, 10, 2), ncol = 3, byrow = TRUE)\n\n# Create a raster with integers\ntemp_reclass &lt;- classify(mintemp, cm)\ntempcats &lt;- c(\"cold\", \"mild\", \"warm\")\nlevels(temp_reclass) &lt;- tempcats"
  },
  {
    "objectID": "slides/14-slides.html#reclassifying-categorical-rasters-1",
    "href": "slides/14-slides.html#reclassifying-categorical-rasters-1",
    "title": "Raster Data: II",
    "section": "Reclassifying Categorical Rasters",
    "text": "Reclassifying Categorical Rasters"
  },
  {
    "objectID": "slides/14-slides.html#raster-math",
    "href": "slides/14-slides.html#raster-math",
    "title": "Raster Data: II",
    "section": "Raster Math",
    "text": "Raster Math\n\nPerforms cell-wise calculations on 1 (or more) SpatRasters\nGenerally works the same as matrix operations\nAll layers must be aligned"
  },
  {
    "objectID": "slides/14-slides.html#raster-math-1",
    "href": "slides/14-slides.html#raster-math-1",
    "title": "Raster Data: II",
    "section": "Raster Math",
    "text": "Raster Math\n\nr &lt;- rast(ncol=5, nrow=5)\nvalues(r) &lt;- 1:ncell(r)\nr2 &lt;- r*2\nr3 &lt;- t(r)\nr4 &lt;- r + r2"
  },
  {
    "objectID": "slides/14-slides.html#cell-wise-operations",
    "href": "slides/14-slides.html#cell-wise-operations",
    "title": "Raster Data: II",
    "section": "Cell-wise operations",
    "text": "Cell-wise operations\n\nterra has a special set of apply functions\napp, lapp, tapp\napp applies a function to the values of each cell\nlapp applies a function using the layer as the value\ntapp applies the function to a subset of layers"
  },
  {
    "objectID": "slides/14-slides.html#context-specific-functions",
    "href": "slides/14-slides.html#context-specific-functions",
    "title": "Raster Data: II",
    "section": "Context-specific Functions",
    "text": "Context-specific Functions\n\ndistance and relatives are based on relationships between cells\nterrain allows calculation of slope, ruggedness, aspect using elevation rasters\nshade calculates hillshade based on terrain"
  },
  {
    "objectID": "slides/12-slides.html#objectives",
    "href": "slides/12-slides.html#objectives",
    "title": "Operations With Vector Data II",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nTranslate pseudocode commands into functional workflows\nArticulate the importance of key arguments to sf functions\nGenerate new attributes and geometries from existing data."
  },
  {
    "objectID": "slides/12-slides.html#example-questions",
    "href": "slides/12-slides.html#example-questions",
    "title": "Operations With Vector Data II",
    "section": "Example questions",
    "text": "Example questions\n\nWhat is the chronic heart disease risk of the 10 ID tracts that are furthest from hospitals?\nHow may \\(km^2\\) of ID are served by more than 1 hospital?\nWhat is the difference between the average risk of chronic heart disease in the tracts served by at least two hospitals compared to those that aren’t served by any?"
  },
  {
    "objectID": "slides/12-slides.html#key-assummptions",
    "href": "slides/12-slides.html#key-assummptions",
    "title": "Operations With Vector Data II",
    "section": "Key assummptions",
    "text": "Key assummptions\n\nAll hospital locations are contained in the landmarks dataset\nA hospital service area is defined as a 50km radius\nHospital service areas can cross state lines."
  },
  {
    "objectID": "slides/12-slides.html#what-do-we-need-to-know",
    "href": "slides/12-slides.html#what-do-we-need-to-know",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nWhere are the hospitals?\nHow far are the hospitals from ID tracts?\nWhich tracts are the furthest?\nWhat is the CHD risk?"
  },
  {
    "objectID": "slides/12-slides.html#pseudocode",
    "href": "slides/12-slides.html#pseudocode",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n1. Load the hospital and cdc datasets\n2. Align the data\n3. Filter cdc so it only has Idaho tracts\n4. Calculate distance from hospitals\n5. Find top 10 tracts based on distance\n6. Map chronic heart disease risk"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions",
    "href": "slides/12-slides.html#adding-functions",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nLoad the hospital and cdc datasets\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nhospital.sf &lt;- read_csv(\"data/opt/data/2023/vectorexample/hospitals_pnw.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"))\nst_crs(hospital.sf)\n\nCoordinate Reference System: NA\n\ncdc.sf &lt;- read_sf(\"data/opt/data/2023/vectorexample/cdc_nw.shp\")\nst_crs(cdc.sf)$epsg\n\n[1] NA"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-1",
    "href": "slides/12-slides.html#adding-functions-1",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nAlign the data\n\n\nst_crs(hospital.sf) &lt;- 4326\n\nhospital.sf.proj &lt;- hospital.sf %&gt;% \n  st_transform(., crs=st_crs(cdc.sf))\n\nst_crs(hospital.sf.proj) == st_crs(cdc.sf)\n\n[1] TRUE\n\nidentical(st_crs(hospital.sf.proj), st_crs(cdc.sf))\n\n[1] TRUE"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-2",
    "href": "slides/12-slides.html#adding-functions-2",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFilter cdc so it only has Idaho tracts\n\n\n\n\ncdc.idaho &lt;- cdc.sf %&gt;% \n  filter(STATEFP == \"16\")\n\n\n\nplot(st_geometry(cdc.idaho))"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-3",
    "href": "slides/12-slides.html#adding-functions-3",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nCalculate distance from hospitals\n\n\nnearest.hosp &lt;- st_nearest_feature(cdc.idaho, hospital.sf.proj)\nstr(nearest.hosp)\n\n int [1:191] 6 45 45 45 3 3 3 3 6 3 ...\n\nnearest.hosp.sf &lt;- hospital.sf.proj[nearest.hosp,]\nhospital.dist &lt;- st_distance(cdc.idaho, nearest.hosp.sf, by_element = TRUE)\nstr(hospital.dist)\n\n Units: [m] num [1:191] 29501 46541 39386 32726 23534 ..."
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-4",
    "href": "slides/12-slides.html#adding-functions-4",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFind top 10 counties based on distance\n\n\ncdc.idaho.hosp &lt;- cdc.idaho %&gt;% \n  mutate(., disthosp = hospital.dist)\n\ncdc.furthest &lt;- cdc.idaho.hosp %&gt;% \n  slice_max(., n=10, order_by= disthosp)\n\nhead(cdc.furthest$disthosp)\n\nUnits: [m]\n[1] 94506.47 83446.11 81134.60 70762.53 70425.16 70084.68"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-5",
    "href": "slides/12-slides.html#adding-functions-5",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nMap chronic heart disease risk\n\n\nlibrary(tmap)\n\ntm_shape(tigris::counties(\"ID\", progress_bar=FALSE)) +\n  tm_polygons() +\n  tm_shape(cdc.furthest) +\n  tm_polygons(\"disthosp\", title=\"Dist to Hospital (m2)\") +\n  tm_shape(hospital.sf.proj[cdc.idaho,]) +\n  tm_symbols(size=0.25)"
  },
  {
    "objectID": "slides/12-slides.html#what-do-we-need-to-know-1",
    "href": "slides/12-slides.html#what-do-we-need-to-know-1",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?\n\nWhere are the hospitals?\nWhat is the service area for each hospital?\nWhere do those service areas overlap?\nHow big is the overlap area?"
  },
  {
    "objectID": "slides/12-slides.html#pseudocode-1",
    "href": "slides/12-slides.html#pseudocode-1",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode\n\n1. Load the hospital dataset and add projection\n2. Buffer hospitals by service area\n3. Find intersection of service areas\n4. Calculate area of overlap"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-6",
    "href": "slides/12-slides.html#adding-functions-6",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nLoad the hospital dataset and add projection\n\n\nhospital.sf &lt;- read_csv(\"data/opt/data/2023/vectorexample/hospitals_pnw.csv\") %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"latitude\"))\n\nst_crs(hospital.sf) &lt;- 4326"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-7",
    "href": "slides/12-slides.html#adding-functions-7",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nBuffer hospitals by service area\n\n\n\n\nhospital.buf &lt;- hospital.sf %&gt;%\n  filter(STATEFP == \"16\") %&gt;% \n  st_buffer(., dist = units::set_units(30, \"kilometers\"))\n\n\n\nplot(st_geometry(hospital.buf))"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-8",
    "href": "slides/12-slides.html#adding-functions-8",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nFind intersection of service areas ::: columns ::: {.column width=“40%”}\n\n\nhospital.int &lt;- hospital.buf %&gt;% \n  st_intersection()\nall(st_is_valid(hospital.int))\n\n::: ::: {.column width=“40%”}"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-9",
    "href": "slides/12-slides.html#adding-functions-9",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions\n\nCalculate area of overlap"
  },
  {
    "objectID": "slides/12-slides.html#plotting-the-results",
    "href": "slides/12-slides.html#plotting-the-results",
    "title": "Operations With Vector Data II",
    "section": "Plotting the Results",
    "text": "Plotting the Results"
  },
  {
    "objectID": "slides/12-slides.html#example-questions-2",
    "href": "slides/12-slides.html#example-questions-2",
    "title": "Operations With Vector Data II",
    "section": "Example questions",
    "text": "Example questions\n\nWhat is the difference between the average risk of chronic heart disease in the counties served by at least two hospitals compared to those that aren’t served by any?"
  },
  {
    "objectID": "slides/12-slides.html#what-do-we-need-to-know-2",
    "href": "slides/12-slides.html#what-do-we-need-to-know-2",
    "title": "Operations With Vector Data II",
    "section": "What do we need to know?",
    "text": "What do we need to know?"
  },
  {
    "objectID": "slides/12-slides.html#pseudocode-2",
    "href": "slides/12-slides.html#pseudocode-2",
    "title": "Operations With Vector Data II",
    "section": "Pseudocode",
    "text": "Pseudocode"
  },
  {
    "objectID": "slides/12-slides.html#adding-functions-10",
    "href": "slides/12-slides.html#adding-functions-10",
    "title": "Operations With Vector Data II",
    "section": "Adding Functions",
    "text": "Adding Functions"
  },
  {
    "objectID": "slides/12-slides.html#plotting-the-results-1",
    "href": "slides/12-slides.html#plotting-the-results-1",
    "title": "Operations With Vector Data II",
    "section": "Plotting the Results",
    "text": "Plotting the Results"
  },
  {
    "objectID": "slides/10-slides.html#objectives",
    "href": "slides/10-slides.html#objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects"
  },
  {
    "objectID": "slides/10-slides.html#which-packages-have-plot-methods",
    "href": "slides/10-slides.html#which-packages-have-plot-methods",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Which packages have plot methods?",
    "text": "Which packages have plot methods?\n\n\n\nOften the fastest way to view data\nUse ?plot to see which packages export a method for the plot function\nOr you can use ?plot.*** to see which classes of objects have plot functions defined"
  },
  {
    "objectID": "slides/10-slides.html#plot-for-sf-objects",
    "href": "slides/10-slides.html#plot-for-sf-objects",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for sf objects",
    "text": "plot for sf objects\n\nCan plot outlines using plot(st_geometry(your.shapfile)) or plot(your.shapefile$geometry)\nPlotting attributes requires “extracting” the attributes (using plot(your.shapefile[\"ATTRIBUTE\"]))\nControlling aesthetics can be challenging\nlayering requires add=TRUE"
  },
  {
    "objectID": "slides/10-slides.html#plot-for-sf-objects-1",
    "href": "slides/10-slides.html#plot-for-sf-objects-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for sf objects",
    "text": "plot for sf objects\n\n\n\nplot(st_geometry(cejst))\n\n\n\n\n\n\nplot(cejst[\"EALR_PFS\"])"
  },
  {
    "objectID": "slides/10-slides.html#plot-for-spatrasters",
    "href": "slides/10-slides.html#plot-for-spatrasters",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for SpatRasters",
    "text": "plot for SpatRasters\n\nplot(rast.data)"
  },
  {
    "objectID": "slides/10-slides.html#plot-for-spatrasters-1",
    "href": "slides/10-slides.html#plot-for-spatrasters-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "plot for SpatRasters",
    "text": "plot for SpatRasters\n\nplot(rast.data[\"WHP_ID\"], col=heat.colors(24, rev=TRUE))"
  },
  {
    "objectID": "slides/10-slides.html#combining-the-two-with-addtrue",
    "href": "slides/10-slides.html#combining-the-two-with-addtrue",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Combining the two with add=TRUE",
    "text": "Combining the two with add=TRUE\n\nplot(rast.data[\"WHP_ID\"], col=heat.colors(24, rev=TRUE))\nplot(st_geometry(st_transform(cejst, crs=crs(rast.data))), add=TRUE)"
  },
  {
    "objectID": "slides/10-slides.html#grammar-of-graphics-wilkinson-2005",
    "href": "slides/10-slides.html#grammar-of-graphics-wilkinson-2005",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Grammar of Graphics (Wilkinson 2005)",
    "text": "Grammar of Graphics (Wilkinson 2005)\n\nGrammar: A set of structural rules that help establish the components of a language\nSystem and structure of language consist of syntax and semantics\nGrammar of Graphics: a framework that allows us to concisely describe the components of any graphic\nFollows a layered approach by using defined components to build a visualization\nggplot2 is a formal implementation in R"
  },
  {
    "objectID": "slides/10-slides.html#aesthetics-mapping-data-to-visual-elements",
    "href": "slides/10-slides.html#aesthetics-mapping-data-to-visual-elements",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Aesthetics: Mapping Data to Visual Elements",
    "text": "Aesthetics: Mapping Data to Visual Elements\n\n\n\n\nDefine the systematic conversion of data into elements of the visualization\nAre either categorical or continuous (exclusively)\nExamples include x, y, fill, color, and alpha\n\n\n\n\n\n\nFrom Wilke 2019"
  },
  {
    "objectID": "slides/10-slides.html#scales",
    "href": "slides/10-slides.html#scales",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Scales",
    "text": "Scales\n\nScales map data values to their aesthetics\nMust be a one-to-one relationship; each specific data value should map to only one aesthetic"
  },
  {
    "objectID": "slides/10-slides.html#using-tmap",
    "href": "slides/10-slides.html#using-tmap",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Using tmap",
    "text": "Using tmap\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\",\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/10-slides.html#using-tmap-1",
    "href": "slides/10-slides.html#using-tmap-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Using tmap",
    "text": "Using tmap"
  },
  {
    "objectID": "slides/10-slides.html#changing-aesthetics",
    "href": "slides/10-slides.html#changing-aesthetics",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics\n\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/10-slides.html#changing-aesthetics-1",
    "href": "slides/10-slides.html#changing-aesthetics-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Changing aesthetics",
    "text": "Changing aesthetics"
  },
  {
    "objectID": "slides/10-slides.html#adding-layers",
    "href": "slides/10-slides.html#adding-layers",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Adding layers",
    "text": "Adding layers\nORDER MATTERS\n\nst &lt;- tigris::states(progress_bar=FALSE) %&gt;% filter(STUSPS %in% c(\"ID\", \"WA\", \"OR\")) %&gt;% st_transform(., crs = st_crs(cejst))\npt &lt;- tm_shape(cejst) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_shape(st) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/10-slides.html#adding-layers-1",
    "href": "slides/10-slides.html#adding-layers-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Adding layers",
    "text": "Adding layers"
  },
  {
    "objectID": "slides/10-slides.html#integrating-rasters",
    "href": "slides/10-slides.html#integrating-rasters",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Integrating Rasters",
    "text": "Integrating Rasters\n\ncejst.proj &lt;- st_transform(cejst, crs=crs(rast.data)) %&gt;% filter(!st_is_empty(.))\nstates.proj &lt;- st_transform(st, crs=crs(rast.data))\npal8 &lt;- c(\"#33A02C\", \"#B2DF8A\", \"#FDBF6F\", \"#1F78B4\", \"#999999\", \"#E31A1C\", \"#E6E6E6\", \"#A6CEE3\")\npt &lt;- tm_shape(rast.data[\"category\"]) +\n  tm_raster(palette = pal8) +\n  tm_shape(cejst.proj) + \n  tm_polygons(col = \"EALR_PFS\", n=10,palette=viridis(10),\n              border.col = \"white\") + \n  tm_shape(states.proj) +\n  tm_borders(\"red\") +\n  tm_legend(outside = TRUE)"
  },
  {
    "objectID": "slides/10-slides.html#integrating-rasters-1",
    "href": "slides/10-slides.html#integrating-rasters-1",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Integrating Rasters",
    "text": "Integrating Rasters"
  },
  {
    "objectID": "slides/08-slides.html#objectives",
    "href": "slides/08-slides.html#objectives",
    "title": "Areal Data: Vector Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nUnderstand predicates and measures in the context of spatial operations in sf\nDefine valid geometries and approaches for assessing geometries in R\nUse st_* and sf_* to evaluate attributes of geometries and calculate measurements"
  },
  {
    "objectID": "slides/08-slides.html#revisiting-simple-features",
    "href": "slides/08-slides.html#revisiting-simple-features",
    "title": "Areal Data: Vector Data",
    "section": "Revisiting Simple Features",
    "text": "Revisiting Simple Features\n\n\n\n\nThe sf package relies on a simple feature data model to represent geometries\n\nhierarchical\nstandardized methods\ncomplementary binary and human-readable encoding\n\n\n\n\n\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above"
  },
  {
    "objectID": "slides/08-slides.html#revisiting-simple-features-1",
    "href": "slides/08-slides.html#revisiting-simple-features-1",
    "title": "Areal Data: Vector Data",
    "section": "Revisiting Simple Features",
    "text": "Revisiting Simple Features\n\nYou already know how to access some elements of a simple feature\nst_crs - returns the coordinate reference system\nst_bbox - returns the bounding box for the simple feature"
  },
  {
    "objectID": "slides/08-slides.html#standaridized-methods",
    "href": "slides/08-slides.html#standaridized-methods",
    "title": "Areal Data: Vector Data",
    "section": "Standaridized Methods",
    "text": "Standaridized Methods\n\nWe can categorize sf operations based on what they return and/or how many geometries they accept as input.\n\n\n\n\n\nOutput Categories\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nTransformations: create new geometries based on input geometries.\n\n\n\n\n\n\nInput Geometries\n\nUnary: operate on a single geometry at a time (meaning that if you have a MULTI* object the function works on each geometry individually)\nBinary: operate on pairs of geometries\nn-ary: operate on sets of geometries"
  },
  {
    "objectID": "slides/08-slides.html#remembering-valid-geometries",
    "href": "slides/08-slides.html#remembering-valid-geometries",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\n\nA linestring is simple if it does not intersect\n\n\n\nlibrary(sf)\nlibrary(tidyverse)\nls = st_linestring(rbind(c(0,0), c(1,1),  c(2,2), c(2,1), c(3,4)))\n\nls2 = st_linestring(rbind(c(0,0), c(1,1),  c(2,2), c(0,2), c(1,1), c(2,0)))"
  },
  {
    "objectID": "slides/08-slides.html#remembering-valid-geometries-1",
    "href": "slides/08-slides.html#remembering-valid-geometries-1",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\nValid polygons\n\nAre closed (i.e., the last vertex equals the first)\nHave holes (inner rings) that inside the the exterior boundary\nHave holes that touch the exterior at no more than one vertex (they don’t extend across a line)\n\nFor multipolygons, adjacent polygons touch only at points\n\nDo not repeat their own path"
  },
  {
    "objectID": "slides/08-slides.html#remembering-valid-geometries-2",
    "href": "slides/08-slides.html#remembering-valid-geometries-2",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\np1 = st_as_sfc(\"POLYGON((0 0, 0 10, 10 0, 10 10, 0 0))\")\np2 = st_as_sfc(\"POLYGON((0 0, 0 10, 5 5,  0 0))\")\np3 = st_as_sfc(\"POLYGON((5 5, 10 10, 10 0, 5 5))\")"
  },
  {
    "objectID": "slides/08-slides.html#remembering-valid-geometries-3",
    "href": "slides/08-slides.html#remembering-valid-geometries-3",
    "title": "Areal Data: Vector Data",
    "section": "Remembering Valid Geometries",
    "text": "Remembering Valid Geometries\n\np4 = st_as_sfc(c(\"POLYGON((0 0, 0 10, 5 5,  0 0))\", \"POLYGON((5 5, 10 10, 10 0, 5 5))\"))\nplot(p4, col=c( \"#7C4A89\", \"blue\"))"
  },
  {
    "objectID": "slides/08-slides.html#empty-geometries",
    "href": "slides/08-slides.html#empty-geometries",
    "title": "Areal Data: Vector Data",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nEmpty geometries arise when an operation produces NULL outcomes (like looking for the intersection between two non-intersecting polygons)\nsf allows empty geometries to make sure that information about the data type is retained\nSimilar to a data.frame with no rows or a list with NULL values\nMost vector operations require simple, valid geometries"
  },
  {
    "objectID": "slides/08-slides.html#using-unary-predicates",
    "href": "slides/08-slides.html#using-unary-predicates",
    "title": "Areal Data: Vector Data",
    "section": "Using Unary Predicates",
    "text": "Using Unary Predicates\n\nUnary predicates accept single geometries (or geometry collections)\nProvide helpful ways to check whether your data is ready to analyze\nUse the st_ prefix and return TRUE/FALSE\n\n\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nis_simple\nis the geometry self-intersecting (i.e., simple)?\n\n\nis_valid\nis the geometry valid?\n\n\nis_empty\nis the geometry column of an object empty?\n\n\nis_longlat\ndoes the object have geographic coordinates? (FALSE if coords are projected, NA if no crs)\n\n\nis(geometry, class)\nis the geometry of a particular class?"
  },
  {
    "objectID": "slides/08-slides.html#checking-geometries-with-unary-predicates",
    "href": "slides/08-slides.html#checking-geometries-with-unary-predicates",
    "title": "Areal Data: Vector Data",
    "section": "Checking Geometries With Unary Predicates",
    "text": "Checking Geometries With Unary Predicates\n\nBefore conducting costly analyses, it’s worth checking for:\n\n\n\nempty geometries, using any(st_is_empty(x)))\ncorrupt geometries, using any(is.na(st_is_valid(x)))\ninvalid geometries, using any(na.omit(st_is_valid(x)) == FALSE); in case of corrupt and/or invalid geometries,\nin case of invalid geometries, query the reason for invalidity by st_is_valid(x, reason = TRUE)\n\n\nInvalid geometries will require transformation (next week!)"
  },
  {
    "objectID": "slides/08-slides.html#checking-geometries-with-unary-predicates-1",
    "href": "slides/08-slides.html#checking-geometries-with-unary-predicates-1",
    "title": "Areal Data: Vector Data",
    "section": "Checking Geometries With Unary Predicates",
    "text": "Checking Geometries With Unary Predicates\n\n\n\n\n\n\n\n\nst_is_simple(ls)\n\n[1] TRUE\n\nst_is_simple(ls2)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\nst_is_valid(p1)\n\n[1] FALSE\n\nst_is_valid(p4)\n\n[1] TRUE TRUE"
  },
  {
    "objectID": "slides/08-slides.html#unary-predicates-and-real-data",
    "href": "slides/08-slides.html#unary-predicates-and-real-data",
    "title": "Areal Data: Vector Data",
    "section": "Unary Predicates and Real Data",
    "text": "Unary Predicates and Real Data\n\n\n\nlibrary(tigris)\nid.cty &lt;- counties(\"ID\", \n                   progress_bar=FALSE)\nst_crs(id.cty)$input\n\n[1] \"NAD83\"\n\nst_is_longlat(id.cty)\n\n[1] TRUE\n\nst_is_valid(id.cty)[1:5]\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\nall(st_is_valid(id.cty))\n\n[1] TRUE"
  },
  {
    "objectID": "slides/08-slides.html#binary-predicates-1",
    "href": "slides/08-slides.html#binary-predicates-1",
    "title": "Areal Data: Vector Data",
    "section": "Binary Predicates",
    "text": "Binary Predicates\n\n\nAccept exactly two geometries (or collections)\nAlso return logical outcomes\nBased on the Dimensionally Extended 9-Intersection Model (DE-9IM)\n\n\n\n\n\n\npredicate\nmeaning\ninverse of\n\n\n\n\ncontains\nNone of the points of A are outside B\nwithin\n\n\ncontains_properly\nA contains B and B has no points in common with the boundary of A\n\n\n\ncovers\nNo points of B lie in the exterior of A\ncovered_by\n\n\ncovered_by\nInverse of covers\n\n\n\ncrosses\nA and B have some but not all interior points in common\n\n\n\ndisjoint\nA and B have no points in common\nintersects\n\n\nequals\nA and B are topologically equal: node order or number of nodes may differ; identical to A contains B AND A within B\n\n\n\nequals_exact\nA and B are geometrically equal, and have identical node order\n\n\n\nintersects\nA and B are not disjoint\ndisjoint\n\n\nis_within_distance\nA is closer to B than a given distance\n\n\n\nwithin\nNone of the points of B are outside A\ncontains\n\n\ntouches\nA and B have at least one boundary point in common, but no interior points\n\n\n\noverlaps\nA and B have some points in common; the dimension of these is identical to that of A and B\n\n\n\nrelate\ngiven a mask pattern, return whether A and B adhere to this pattern"
  },
  {
    "objectID": "slides/08-slides.html#binary-predicates-2",
    "href": "slides/08-slides.html#binary-predicates-2",
    "title": "Areal Data: Vector Data",
    "section": "Binary Predicates",
    "text": "Binary Predicates\n\n\n\nid &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"ID\")\nor &lt;- states(progress_bar=FALSE) %&gt;% \n  filter(STUSPS == \"OR\")\nada.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Ada\")\n\n\n\nst_covers(id, ada.cty)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `covers'\n 1: 1\n\nst_covers(id, ada.cty, sparse=FALSE)\n\n     [,1]\n[1,] TRUE\n\nst_within(ada.cty, or)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `within'\n 1: (empty)\n\nst_within(ada.cty, or, sparse=FALSE)\n\n      [,1]\n[1,] FALSE"
  },
  {
    "objectID": "slides/08-slides.html#measures-1",
    "href": "slides/08-slides.html#measures-1",
    "title": "Areal Data: Vector Data",
    "section": "Measures",
    "text": "Measures\nUnary Measures\n\nReturn quantities of individual geometries\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\n\nUnary Measures\n\nst_area(id)\n\n2.15994e+11 [m^2]\n\nst_area(id.cty[1:5,])\n\nUnits: [m^2]\n[1] 2858212132 3380630278 1459359818 1726660484 1223521586\n\nst_dimension(id.cty[1:5,])\n\n[1] 2 2 2 2 2"
  },
  {
    "objectID": "slides/08-slides.html#binary-measures",
    "href": "slides/08-slides.html#binary-measures",
    "title": "Areal Data: Vector Data",
    "section": "Binary Measures",
    "text": "Binary Measures\n\nst_distance returns the distance between pairs of geometries\n\n\nkootenai.cty &lt;- id.cty %&gt;% \n  filter(NAME == \"Kootenai\")\nst_distance(kootenai.cty, ada.cty)\n\nUnits: [m]\n         [,1]\n[1,] 396433.8\n\nst_distance(id.cty)[1:5, 1:5]\n\nUnits: [m]\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,]      0.0 467635.7 277227.0 132998.0      0.0\n[2,] 467635.7      0.0 319706.4 656056.0 514306.9\n[3,] 277227.0 319706.4      0.0 377105.4 336146.8\n[4,] 132998.0 656056.0 377105.4      0.0 133045.5\n[5,]      0.0 514306.9 336146.8 133045.5      0.0"
  },
  {
    "objectID": "slides/06-slides.html#for-today",
    "href": "slides/06-slides.html#for-today",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "For today",
    "text": "For today\n\nIntroduce literate programming\nDescribe pseudocode and its utility for designing an analysis\nIntroduce Quarto as a means of documenting your work\nPractice workflow"
  },
  {
    "objectID": "slides/06-slides.html#why-do-we-need-reproducibility",
    "href": "slides/06-slides.html#why-do-we-need-reproducibility",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Why Do We Need Reproducibility?",
    "text": "Why Do We Need Reproducibility?\n\n\n\nNoise!!\nConfirmation bias\nHindsight bias\n\n\n\n\n\nMunafo et al. 2017. Nat Hum Beh."
  },
  {
    "objectID": "slides/06-slides.html#reproducibility-and-your-code",
    "href": "slides/06-slides.html#reproducibility-and-your-code",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Reproducibility and your code",
    "text": "Reproducibility and your code\n\nScripts: may make your code reproducible (but not your analysis)\nCommenting and formatting can help!\n\n\n```{r}\n#| eval: false\n## load the packages necessary\nlibrary(tidyverse)\n## read in the data\nlandmarks.csv &lt;- read_csv(\"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2023/assignment01/landmarks_ID.csv\")\n\n## How many in each feature class\ntable(landmarks.csv$MTFCC)\n```"
  },
  {
    "objectID": "slides/06-slides.html#reproducible-scripts",
    "href": "slides/06-slides.html#reproducible-scripts",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Reproducible scripts",
    "text": "Reproducible scripts\n\nComments explain what the code is doing\nOperations are ordered logically\nOnly relevant commands are presented\nUseful object and function names\nScript runs without errors (on your machine and someone else’s)"
  },
  {
    "objectID": "slides/06-slides.html#toward-efficient-reproducible-analyses",
    "href": "slides/06-slides.html#toward-efficient-reproducible-analyses",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Toward Efficient Reproducible Analyses",
    "text": "Toward Efficient Reproducible Analyses\n\nScripts can document what you did, but not why you did it!\nScripts separate your analysis products from your report/manuscript"
  },
  {
    "objectID": "slides/06-slides.html#what-is-literate-programming",
    "href": "slides/06-slides.html#what-is-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.\n\n— Donald Knuth, CSLI, 1984"
  },
  {
    "objectID": "slides/06-slides.html#what-is-literate-programming-1",
    "href": "slides/06-slides.html#what-is-literate-programming-1",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is literate programming?",
    "text": "What is literate programming?\n\nDocumentation containing code (not vice versa!)\nDirect connection between code and explanation\nConvey meaning to humans rather than telling computer what to do!\nMultiple “scales” possible"
  },
  {
    "objectID": "slides/06-slides.html#why-literate-programming",
    "href": "slides/06-slides.html#why-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Why literate programming?",
    "text": "Why literate programming?\n\nYour analysis scripts are computer software\nIntegrate math, figures, code, and narrative in one place\nExplaining something helps you learn it"
  },
  {
    "objectID": "slides/06-slides.html#planning-an-analysis",
    "href": "slides/06-slides.html#planning-an-analysis",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Planning an analysis",
    "text": "Planning an analysis\n\n\n\nOutline your project\nWrite pseudocode\nIdentify potential packages\nBorrow (and attribute) code from others (including yourself!)"
  },
  {
    "objectID": "slides/06-slides.html#pseudocode-and-literate-programming",
    "href": "slides/06-slides.html#pseudocode-and-literate-programming",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Pseudocode and literate programming",
    "text": "Pseudocode and literate programming\n\nAn informal way of writing the ‘logic’ of your program\nBalance between readability and precision\nAvoid syntactic drift"
  },
  {
    "objectID": "slides/06-slides.html#writing-pseudocode",
    "href": "slides/06-slides.html#writing-pseudocode",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Writing pseudocode",
    "text": "Writing pseudocode\n\n\n\nFocus on statements\nMathematical operations\nConditionals\nIteration\nExceptions"
  },
  {
    "objectID": "slides/06-slides.html#pseudocode-1",
    "href": "slides/06-slides.html#pseudocode-1",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Pseudocode",
    "text": "Pseudocode\n\nStart function\nInput information\nLogical test: if TRUE\n  (what to do if TRUE)\nelse\n  (what to do if FALSE)\nEnd function"
  },
  {
    "objectID": "slides/06-slides.html#what-is-quarto",
    "href": "slides/06-slides.html#what-is-quarto",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nA multi-language platform for developing reproducible documents\nA ‘lab notebook’ for your analyses\nAllows transparent, reproducible scientific reports and presentations"
  },
  {
    "objectID": "slides/06-slides.html#key-components",
    "href": "slides/06-slides.html#key-components",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Key components",
    "text": "Key components\n\nMetadata and global options: YAML\nText, figures, and tables: Markdown and LaTeX\nCode: knitr (or jupyter if you’re into that sort of thing)"
  },
  {
    "objectID": "slides/06-slides.html#yaml---yet-another-markup-language",
    "href": "slides/06-slides.html#yaml---yet-another-markup-language",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "YAML - Yet Another Markup Language",
    "text": "YAML - Yet Another Markup Language\n\nAllows you to set (or change) output format\nProvide options that apply to the entire document\nSpacing matters!"
  },
  {
    "objectID": "slides/06-slides.html#formatting-text",
    "href": "slides/06-slides.html#formatting-text",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Formatting Text",
    "text": "Formatting Text\n\nBasic formatting via Markdown\nFancier options using Divs and spans via Pandoc\nFenced Divs start and end with ::: (can be any number &gt;3 but must match)"
  },
  {
    "objectID": "slides/06-slides.html#adding-code-chunks",
    "href": "slides/06-slides.html#adding-code-chunks",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Adding Code Chunks",
    "text": "Adding Code Chunks\n\nUse 3x ``` on each end\nInclude the engine {r} (or python or Julia)\nInclude options beneath the “fence” using a hashpipe (#|)"
  },
  {
    "objectID": "slides/06-slides.html#additional-considerations",
    "href": "slides/06-slides.html#additional-considerations",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Additional considerations",
    "text": "Additional considerations\n\nFile locations and Quarto\nCaching for slow operations\nModularizing code and functional programming"
  },
  {
    "objectID": "slides/03-slides.html#todays-plan",
    "href": "slides/03-slides.html#todays-plan",
    "title": "Introduction to Spatial Data",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nWays to view the world\nWhat makes data (geo)spatial?\nCoordinate Reference Systems\nGeometries, support, and spatial messiness"
  },
  {
    "objectID": "slides/03-slides.html#as-a-series-of-objects",
    "href": "slides/03-slides.html#as-a-series-of-objects",
    "title": "Introduction to Spatial Data",
    "section": "…As a Series of Objects?",
    "text": "…As a Series of Objects?\n\n\n\n\nThe world is a series of entities located in space.\nUsually distinguishable, discrete, and bounded\nSome spaces can hold multiple entities, others are empty\nObjects are digital representations of entities"
  },
  {
    "objectID": "slides/03-slides.html#as-a-continuous-field",
    "href": "slides/03-slides.html#as-a-continuous-field",
    "title": "Introduction to Spatial Data",
    "section": "…As a Continuous Field",
    "text": "…As a Continuous Field\n\n\n\n\nThe earth is a single entity with properties that vary continuosly through space\nSpatial continuity: Every cell has a value (including “no data” or “not here”)\nSelf-definition: the values define the field\nSpace is tessellated: cells are mutually exclusive"
  },
  {
    "objectID": "slides/03-slides.html#spatial-data-as-a-stochastic-process",
    "href": "slides/03-slides.html#spatial-data-as-a-stochastic-process",
    "title": "Introduction to Spatial Data",
    "section": "Spatial data as a stochastic process",
    "text": "Spatial data as a stochastic process\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]"
  },
  {
    "objectID": "slides/03-slides.html#areal-data",
    "href": "slides/03-slides.html#areal-data",
    "title": "Introduction to Spatial Data",
    "section": "Areal Data",
    "text": "Areal Data\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\n\n\\(D\\) is fixed domain of countable units\nTypically involve some aggregation"
  },
  {
    "objectID": "slides/03-slides.html#geostatistical-data",
    "href": "slides/03-slides.html#geostatistical-data",
    "title": "Introduction to Spatial Data",
    "section": "Geostatistical data",
    "text": "Geostatistical data\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\n\n\n\nMitzi Morris\n\n\n\n\n\\(D\\) is a fixed subset of \\(\\mathbb{R}^d\\)\n\\(Z(\\mathbf{s})\\) could be observed at any location within \\(D\\).\nModels predict unobserved locations"
  },
  {
    "objectID": "slides/03-slides.html#point-patterns",
    "href": "slides/03-slides.html#point-patterns",
    "title": "Introduction to Spatial Data",
    "section": "Point patterns",
    "text": "Point patterns\n\n\\[\n{Z(\\mathbf{s}): \\mathbf{s} \\in D \\subset \\mathbb{R}^d}\n\\]\n\n\n\\(D\\) is random; where \\(\\mathbf{s}\\) depicts the location of events\n\n\n\n\nBen-Said, M. Ecol Process 10, 56 (2021)."
  },
  {
    "objectID": "slides/03-slides.html#what-is-a-data-model",
    "href": "slides/03-slides.html#what-is-a-data-model",
    "title": "Introduction to Spatial Data",
    "section": "What is a data model?",
    "text": "What is a data model?\n\n\nData: a collection of discrete values that describe phenomena\nYour brain stores millions of pieces of data\nComputers are not your brain\n\nNeed to organize data systematically\nBe able to display and access efficiently\nNeed to be able to store and access repeatedly\n\nData models solve this problem"
  },
  {
    "objectID": "slides/03-slides.html#types-of-spatial-data-models",
    "href": "slides/03-slides.html#types-of-spatial-data-models",
    "title": "Introduction to Spatial Data",
    "section": "2 Types of Spatial Data Models",
    "text": "2 Types of Spatial Data Models\n\nRaster: grid-cell tessellation of an area. Each raster describes the value of a single phenomenon. More next week…\nVector: (many) attributes associated with locations defined by coordinates"
  },
  {
    "objectID": "slides/03-slides.html#the-vector-data-model",
    "href": "slides/03-slides.html#the-vector-data-model",
    "title": "Introduction to Spatial Data",
    "section": "The Vector Data Model",
    "text": "The Vector Data Model\n\n\n\n\nVertices (i.e., discrete x-y locations) define the shape of the vector\nThe organization of those vertices define the shape of the vector\nGeneral types: points, lines, polygons\n\n\n\n\n\n\nImage Source: Colin Williams (NEON)"
  },
  {
    "objectID": "slides/03-slides.html#vectors-in-action",
    "href": "slides/03-slides.html#vectors-in-action",
    "title": "Introduction to Spatial Data",
    "section": "Vectors in Action",
    "text": "Vectors in Action\n\n\nUseful for locations with discrete, well-defined boundaries\nVery precise (not necessarily accurate)\n\n\n\nImage Source: QGIS User’s manual"
  },
  {
    "objectID": "slides/03-slides.html#the-raster-data-model",
    "href": "slides/03-slides.html#the-raster-data-model",
    "title": "Introduction to Spatial Data",
    "section": "The Raster Data Model",
    "text": "The Raster Data Model\n\n\n\n\nRaster data represent spatially continuous phenomena (NA is possible)\nDepict the alignment of data on a regular lattice (often a square)\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size"
  },
  {
    "objectID": "slides/03-slides.html#types-of-raster-data",
    "href": "slides/03-slides.html#types-of-raster-data",
    "title": "Introduction to Spatial Data",
    "section": "Types of Raster Data",
    "text": "Types of Raster Data\n\n\n\n\n\n\n\n\n\n\nRegular: constant cell size; axes aligned with Easting and Northing\nRotated: constant cell size; axes not aligned with Easting and Northing\nSheared: constant cell size; axes not parallel\nRectilinear: cell size varies along a dimension\nCurvilinear: cell size and orientation dependent on the other dimension"
  },
  {
    "objectID": "slides/03-slides.html#types-of-raster-data-1",
    "href": "slides/03-slides.html#types-of-raster-data-1",
    "title": "Introduction to Spatial Data",
    "section": "Types of Raster Data",
    "text": "Types of Raster Data\n\nContinuous: numeric data representing a measurement (e.g., elevation, precipitation)\nCategorical: integer data representing factors (e.g., land use, land cover)"
  },
  {
    "objectID": "slides/03-slides.html#location-vs.-place",
    "href": "slides/03-slides.html#location-vs.-place",
    "title": "Introduction to Spatial Data",
    "section": "Location vs. Place",
    "text": "Location vs. Place\n\n\n\n\n\nPlace: an area having unique physical and human characteristics interconnected with other places\nLocation: the actual position on the earth’s surface\nSense of Place: the emotions someone attaches to an area based on experiences\nPlace is location plus meaning\n\n\n\n\n\n\nnominal: (potentially contested) place names\nabsolute: the physical location on the earth’s surface"
  },
  {
    "objectID": "slides/03-slides.html#describing-absolute-locations",
    "href": "slides/03-slides.html#describing-absolute-locations",
    "title": "Introduction to Spatial Data",
    "section": "Describing Absolute Locations",
    "text": "Describing Absolute Locations\n\nCoordinates: 2 or more measurements that specify location relative to a reference system\n\n\n\n\n\nCartesian coordinate system\norigin (O) = the point at which both measurement systems intersect\nAdaptable to multiple dimensions (e.g. z for altitude)\n\n\n\n\n\n\nCartesian Coordinate System"
  },
  {
    "objectID": "slides/03-slides.html#locations-on-a-globe",
    "href": "slides/03-slides.html#locations-on-a-globe",
    "title": "Introduction to Spatial Data",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\n\n\nLatitude and Longitude"
  },
  {
    "objectID": "slides/03-slides.html#locations-on-a-globe-1",
    "href": "slides/03-slides.html#locations-on-a-globe-1",
    "title": "Introduction to Spatial Data",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\nGlobal Reference Systems (GRS)\nGraticule: the grid formed by the intersection of longitude and latitude\nThe graticule is based on an ellipsoid model of earth’s surface and contained in the datum"
  },
  {
    "objectID": "slides/03-slides.html#global-reference-systems",
    "href": "slides/03-slides.html#global-reference-systems",
    "title": "Introduction to Spatial Data",
    "section": "Global Reference Systems",
    "text": "Global Reference Systems\n\nThe datum describes which ellipsoid to use and the precise relations between locations on earth’s surface and Cartesian coordinates\n\n\nGeodetic datums (e.g., WGS84): distance from earth’s center of gravity\nLocal data (e.g., NAD83): better models for local variation in earth’s surface"
  },
  {
    "objectID": "slides/03-slides.html#describing-location-extent",
    "href": "slides/03-slides.html#describing-location-extent",
    "title": "Introduction to Spatial Data",
    "section": "Describing location: extent",
    "text": "Describing location: extent\n\n\nHow much of the world does the data cover?\nFor rasters, these are the corners of the lattice\nFor vectors, we call this the bounding box"
  },
  {
    "objectID": "slides/03-slides.html#describing-location-resolution",
    "href": "slides/03-slides.html#describing-location-resolution",
    "title": "Introduction to Spatial Data",
    "section": "Describing location: resolution",
    "text": "Describing location: resolution\n\n\n\n\nResolution: the accuracy that the location and shape of a map’s features can be depicted\nMinimum Mapping Unit: The minimum size and dimensions that can be reliably represented at a given map scale.\nMap scale vs. scale of analysis"
  },
  {
    "objectID": "slides/03-slides.html#projections",
    "href": "slides/03-slides.html#projections",
    "title": "Introduction to Spatial Data",
    "section": "Projections",
    "text": "Projections\n\n\n\n\nBut maps, screens, and publications are…\nProjections describe how the data should be translated to a flat surface\nRely on ‘developable surfaces’\nDescribed by the Coordinate Reference System (CRS)\n\n\n\n\n\n\nDevelopable Surfaces\n\n\n\n\n\nProjection necessarily induces some form of distortion (tearing, compression, or shearing)"
  },
  {
    "objectID": "slides/03-slides.html#coordinate-reference-systems",
    "href": "slides/03-slides.html#coordinate-reference-systems",
    "title": "Introduction to Spatial Data",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nSome projections minimize distortion of angle, area, or distance\nOthers attempt to avoid extreme distortion of any kind\nIncludes: Datum, ellipsoid, units, and other information (e.g., False Easting, Central Meridian) to further map the projection to the GCS\nNot all projections have/require all of the parameters"
  },
  {
    "objectID": "slides/03-slides.html#choosing-projections",
    "href": "slides/03-slides.html#choosing-projections",
    "title": "Introduction to Spatial Data",
    "section": "Choosing Projections",
    "text": "Choosing Projections\n\n\n\n\n\nEqual-area for thematic maps\nConformal for presentations\nMercator or equidistant for navigation and distance"
  },
  {
    "objectID": "slides/03-slides.html#geometries",
    "href": "slides/03-slides.html#geometries",
    "title": "Introduction to Spatial Data",
    "section": "Geometries",
    "text": "Geometries\n\n\n\nVectors store aggregate the locations of a feature into a geometry\nMost vector operations require simple, valid geometries\n\n\n\n\n\nImage Source: Colin Williams (NEON)"
  },
  {
    "objectID": "slides/03-slides.html#valid-geometries",
    "href": "slides/03-slides.html#valid-geometries",
    "title": "Introduction to Spatial Data",
    "section": "Valid Geometries",
    "text": "Valid Geometries\n\n\nA linestring is simple if it does not intersect\nValid polygons\nAre closed (i.e., the last vertex equals the first)\nHave holes (inner rings) that inside the the exterior boundary\nHave holes that touch the exterior at no more than one vertex (they don’t extend across a line) - For multipolygons, adjacent polygons touch only at points\nDo not repeat their own path"
  },
  {
    "objectID": "slides/03-slides.html#empty-geometries",
    "href": "slides/03-slides.html#empty-geometries",
    "title": "Introduction to Spatial Data",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nEmpty geometries arise when an operation produces NULL outcomes (like looking for the intersection between two non-intersecting polygons)\nsf allows empty geometries to make sure that information about the data type is retained\nSimilar to a data.frame with no rows or a list with NULL values\nMost vector operations require simple, valid geometries"
  },
  {
    "objectID": "slides/03-slides.html#support",
    "href": "slides/03-slides.html#support",
    "title": "Introduction to Spatial Data",
    "section": "Support",
    "text": "Support\n\nSupport is the area to which an attribute applies.\n\n\n\nFor vectors, the attribute-geometry-relationship can be:\nconstant = applies to every point in the geometry (lines and polygons are just lots of points)\nidentity = a value unique to a geometry\naggregate = a single value that integrates data across the geometry\nRasters can have point (attribute refers to the cell center) or cell (attribute refers to an area similar to the pixel) support"
  },
  {
    "objectID": "slides/03-slides.html#spatial-messiness",
    "href": "slides/03-slides.html#spatial-messiness",
    "title": "Introduction to Spatial Data",
    "section": "Spatial Messiness",
    "text": "Spatial Messiness\n\nQuantitative geography requires that our data are aligned\nAchieving alignment is part of reproducible workflows\nMaking principled decisions about projections, resolution, extent, etc"
  },
  {
    "objectID": "slides/01-slides.html#todays-plan",
    "href": "slides/01-slides.html#todays-plan",
    "title": "Getting Started",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nIntroductions\nWhy (not) R?\nCourse logistics and resources\nTesting out RStudio, git, and GitHub Classroom"
  },
  {
    "objectID": "slides/01-slides.html#about-me",
    "href": "slides/01-slides.html#about-me",
    "title": "Getting Started",
    "section": "About Me",
    "text": "About Me\n\n\n\n\nWhat I do\nMy path to this point\nWhy I teach this course"
  },
  {
    "objectID": "slides/01-slides.html#what-about-you",
    "href": "slides/01-slides.html#what-about-you",
    "title": "Getting Started",
    "section": "What about you?",
    "text": "What about you?\n\n\n\nYour preferred pronouns\nWhere are you from?\nWhat do you like most about Boise?\nWhat do you miss most about “home”?\nWhat is your research?"
  },
  {
    "objectID": "slides/01-slides.html#why-r",
    "href": "slides/01-slides.html#why-r",
    "title": "Getting Started",
    "section": "Why R?",
    "text": "Why R?\n\n\n\n\nOpen Source\nHuge useR community\nIntegrated analysis pipelines\nReproducible workflows\n\n\n\n\nCodePlot\n\n\n\nlibrary(maps)\nlibrary(socviz)\nlibrary(tidyverse)\nparty_colors &lt;- c(\"#2E74C0\", \"#CB454A\") \nus_states &lt;- map_data(\"state\")\nelection$region &lt;- tolower(election$state)\nus_states_elec &lt;- left_join(us_states, election)\np0 &lt;- ggplot(data = us_states_elec,\n             mapping = aes(x = long, y = lat,\n                           group = group, \n                           fill = party))\np1 &lt;- p0 + geom_polygon(color = \"gray90\", \n                        size = 0.1) +\n    coord_map(projection = \"albers\", \n              lat0 = 39, lat1 = 45) \np2 &lt;- p1 + scale_fill_manual(values = party_colors) +\n    labs(title = \"Election Results 2016\", \n         fill = NULL)"
  },
  {
    "objectID": "slides/01-slides.html#why-not-r-1",
    "href": "slides/01-slides.html#why-not-r-1",
    "title": "Getting Started",
    "section": "Why not R?",
    "text": "Why not R?\n\n## ---\n## Error: could not find function \"performance\"\n## ---\n##  [1] \"Error in if (str_count(string = f[[j]], \n##  pattern = \\\"\\\\\\\\S+\\\") == 1) \n##  { : \\n  argument is of length zero\"   \n## ---\n## Error in eval(expr, envir, enclos) : object 'x' not found\n## ---\n## Error in file(file, \"rt\") : cannot open the connection\n## ---\n\n\n\n\nCoding can be hard…\nMemory challenges\n\n\n\nSpeed\nDecision fatigue"
  },
  {
    "objectID": "slides/01-slides.html#getting-help",
    "href": "slides/01-slides.html#getting-help",
    "title": "Getting Started",
    "section": "Getting Help",
    "text": "Getting Help\n\n\n\nGoogle it!!\n\nUse the exact error message\nInclude the package name\ninclude “R” in the search\n\n\n\n\nStack Overflow\n\nReproducible examples\n\nPackage “issue” pages\nr_spatial slack channel\nCommon errors\n\n\n\n\nAsk Me"
  },
  {
    "objectID": "slides/01-slides.html#logistics",
    "href": "slides/01-slides.html#logistics",
    "title": "Getting Started",
    "section": "Logistics",
    "text": "Logistics\n\n\nMeet on Mondays and Wednesdays\n~55 min lecture, 20 min practice\n4 major sections\nReadings"
  },
  {
    "objectID": "slides/01-slides.html#course-webpage",
    "href": "slides/01-slides.html#course-webpage",
    "title": "Getting Started",
    "section": "Course Webpage",
    "text": "Course Webpage\nhttps://isdrfall23.classes.spaseslab.com/\n\n\nSyllabus\nSchedule\nLectures\nAssignments\nResources"
  },
  {
    "objectID": "slides/01-slides.html#assignments",
    "href": "slides/01-slides.html#assignments",
    "title": "Getting Started",
    "section": "Assignments",
    "text": "Assignments\n\nCheck out the syllabus for more on grading!\n\n\n\n\n\n\nSelf-reflections (2x)\n\nYour goals for the course\nEvaluation criteria\n\nCoding exercises (10x)\n\nProblem solving\nReproducible workflows\nMuscle memory\n\n\n\n\nCode Revisions (3x)\n\nDigging deeper\nCommon issues\nMore extensive feedback\n\nFinal project (1st draft, final draft)\n\nPractice a full analysis workflow\nIntegrate analysis & visuals to tell a story"
  },
  {
    "objectID": "slides/01-slides.html#orientation-to-rstudio-and-our-rstudio-server",
    "href": "slides/01-slides.html#orientation-to-rstudio-and-our-rstudio-server",
    "title": "Getting Started",
    "section": "Orientation to RStudio and our RStudio server",
    "text": "Orientation to RStudio and our RStudio server"
  },
  {
    "objectID": "slides/01-slides.html#introduce-yourself-to-git",
    "href": "slides/01-slides.html#introduce-yourself-to-git",
    "title": "Getting Started",
    "section": "Introduce yourself to Git",
    "text": "Introduce yourself to Git\n\nLots of ways, but one easy way is:\n\n\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\") #your info here\n\n\nGenerate a PAT token if you don’t have one (make sure you save it somewhere)\n\n\nusethis::create_github_token()"
  },
  {
    "objectID": "slides/01-slides.html#introduce-yourself-to-git-contd",
    "href": "slides/01-slides.html#introduce-yourself-to-git-contd",
    "title": "Getting Started",
    "section": "Introduce yourself to Git (cont’d)",
    "text": "Introduce yourself to Git (cont’d)\n\nStore your credentials for use (times out after 1 hr)\n\n\ngitcreds::gitcreds_set()\n\n\nVerify\n\n\ngitcreds::gitcreds_get()"
  },
  {
    "objectID": "slides/01-slides.html#joining-the-assignment-and-cloning-the-repo",
    "href": "slides/01-slides.html#joining-the-assignment-and-cloning-the-repo",
    "title": "Getting Started",
    "section": "Joining the assignment and cloning the repo",
    "text": "Joining the assignment and cloning the repo\n\nClick this link\nBring the project into RStudio\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\nPaste the link from the “Clone Repository” button into the “Repository URL” space"
  },
  {
    "objectID": "slides/01-slides.html#the-git-workflow",
    "href": "slides/01-slides.html#the-git-workflow",
    "title": "Getting Started",
    "section": "The git workflow",
    "text": "The git workflow\n\nMake sure to pull everytime you start working on a project\nMake some changes to code\nSave those changes\nCommit your changes\nPush your work to the remote!"
  },
  {
    "objectID": "slides/01-slides.html#checking-in",
    "href": "slides/01-slides.html#checking-in",
    "title": "Getting Started",
    "section": "Checking in",
    "text": "Checking in\n\nWhat are some advantages and disadvantages of using R for spatial analysis\nWhat can I clarify about the course?\nHow do you feel about git and github classroom? How can I make that easier for you?"
  },
  {
    "objectID": "resource/rmarkdown.html",
    "href": "resource/rmarkdown.html",
    "title": "Authoring in Rmarkdown and Quarto",
    "section": "",
    "text": "[Rmarkdown] and [Quarto] are two powerful ways to combine writing (or presentations) and analysis into a single reproducible workflow. Although they can be a little more cumbersome to use than traditional word processors (e.g., Word or Pages) or presentation software (e.g., PowerPoint or Keynotes), they have the benefit of allowing you to keep all of the pieces of your manuscripts or presentations in one place. Change some data or analysis? The whole manuscript or presentation should update without forcing you to try and find all of the places where that change might alter your writing or slides. It may take a little getting used to, but the fact that both Rmarkdown and Quarto can utilize the power of LaTeX typesetting means that you’ll ultimately be able to produce publication quality equations, tables, and figures all in one place.\nThis webpage and all of my slides were built with Quarto (and last year’s was built with Rmarkdown and blogdown). Having gone through the process of learning how make that work, I’m convinced that having a working knowledge of one or both of these is useful. As such, you’ll be using Rmarkdown or Quarto (your choice) to complete your assignments and render them to html. To help you get started here are a few links:"
  },
  {
    "objectID": "resource/lastyear.html",
    "href": "resource/lastyear.html",
    "title": "Last year’s class",
    "section": "",
    "text": "Last year was the first time I taught this class in it’s current format. I built a number of worked examples to try to clarify how different parts of a spatial workflow come together. Although the examples are far from perfect (I’m hoping this year’s are better), the page does have a number of potentially useful pieces. Check out the examples to access these."
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Resources",
    "section": "",
    "text": "I have included a bunch of extra resources and guides related to R and coding, potentially interesting data, and cool visualizations. Let me know when you find fun things to include here!"
  },
  {
    "objectID": "resource/data.html",
    "href": "resource/data.html",
    "title": "Fun datasets",
    "section": "",
    "text": "So much data, so little time… Here are some links to help you get started finding data for your geospatial projects"
  },
  {
    "objectID": "resource/data.html#spatial-data-repositories",
    "href": "resource/data.html#spatial-data-repositories",
    "title": "Fun datasets",
    "section": "Spatial Data Repositories",
    "text": "Spatial Data Repositories\n\nDataBasin: Lots of spatial data related to conservation issues across the US. The AdaptWest portal has tons of spatial data on climate change and its potential impacts.\nUS Protected Areas Database: PAD-US is America’s official national inventory of U.S. terrestrial and marine protected areas that are dedicated to the preservation of biological diversity and to other natural, recreation and cultural uses, managed for these purposes through legal or other effective means. PAD-US also includes the best available aggregation of federal land and marine areas provided directly by managing agencies, coordinated through the Federal Geographic Data Committee (FGDC) Federal Lands Working Group.\nUSGS Gap Analysis Project: A variety of datasets depicting land cover and species distributions."
  },
  {
    "objectID": "resource/data.html#general-data-repositories",
    "href": "resource/data.html#general-data-repositories",
    "title": "Fun datasets",
    "section": "General Data Repositories",
    "text": "General Data Repositories\n\nData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\nGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\nKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\nUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing."
  },
  {
    "objectID": "resource/data.html#political-science-and-economics-datasets",
    "href": "resource/data.html#political-science-and-economics-datasets",
    "title": "Fun datasets",
    "section": "Political science and economics datasets",
    "text": "Political science and economics datasets\nThere’s a wealth of data available for political science- and economics-related topics:\n\nFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\nThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\nErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\nInside AirBnB a Creative Commons-licensed dataset with a ton of spatially referenced info on AirBnBs in cities across the globe."
  },
  {
    "objectID": "resource/data.html#the-30daymapchallenge",
    "href": "resource/data.html#the-30daymapchallenge",
    "title": "Fun datasets",
    "section": "The #30daymapchallenge",
    "text": "The #30daymapchallenge\nThe #30daymapchallenge is a social mapping/cartography/data visualization challenge designed to encourage experimentation with different types of datasets and mapping approaches. Searching the hashtag on social media (especially Twitter) will bring up a bunch of cool examples. Here are a few repositories to help you get started:\n\nThe Official #30DayMapChallenge Repo has an archive of past challenges and a description of what this is all about.\nBob Rudis’ 2019 bookdown project Contains both code and useful information for generating the visualizations along with sources for data.\nAlexandra Kapp’s 2020 repository makes use of some of the newer animation and interactive visualization techniques.\nThe R-Spatial list of 2020 challenge repositories"
  },
  {
    "objectID": "lesson/quarto.html",
    "href": "lesson/quarto.html",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "lesson/quarto.html#quarto",
    "href": "lesson/quarto.html#quarto",
    "title": "Quarto and literate programming",
    "section": "",
    "text": "This is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "lesson/quarto.html#the-example",
    "href": "lesson/quarto.html#the-example",
    "title": "Quarto and literate programming",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.3     ✔ purrr   1.0.2\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "lesson/getting-setup.html",
    "href": "lesson/getting-setup.html",
    "title": "Getting Set Up",
    "section": "",
    "text": "This is an intro to git and github, most of which was originally created by Jessica Taylor and Nora Honkomp (2 former students in this class!)."
  },
  {
    "objectID": "lesson/getting-setup.html#lets-git-started",
    "href": "lesson/getting-setup.html#lets-git-started",
    "title": "Getting Set Up",
    "section": "Let’s “git” started",
    "text": "Let’s “git” started\nWe are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\nAccept the invitation to the assignment repo\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account."
  },
  {
    "objectID": "lesson/getting-setup.html#installload-required-package",
    "href": "lesson/getting-setup.html#installload-required-package",
    "title": "Getting Set Up",
    "section": "Install/Load Required Package",
    "text": "Install/Load Required Package\nLoad the following packages in RStudio. If you do not have them installed, you can do so using install.packages().\n\n\nCode\nlibrary(usethis)\nlibrary(gitcreds)\nlibrary(knitr)"
  },
  {
    "objectID": "lesson/getting-setup.html#create-a-github-account",
    "href": "lesson/getting-setup.html#create-a-github-account",
    "title": "Getting Set Up",
    "section": "Create a GitHub Account",
    "text": "Create a GitHub Account\nYou will need to access GitHub with an account for this tutorial. If you don’t already have an account, you can sign up for free here: https://github.com/\nYou will be asked to sign up using your email, a password you create, and a username. Your username is what will be visible to others that you collaborate with, so it’s a good idea to make it something straight forward and professional. You can skip personalization for now by scrolling to the bottom of the page."
  },
  {
    "objectID": "lesson/getting-setup.html#github-and-rstudio-server",
    "href": "lesson/getting-setup.html#github-and-rstudio-server",
    "title": "Getting Set Up",
    "section": "Github and RStudio Server",
    "text": "Github and RStudio Server\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\n\n\nBring the project into RStudio\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\nVerify that the “Git” tab is available and that your project is shown in the upper right-hand corner\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:"
  },
  {
    "objectID": "lesson/getting-setup.html#installing-git-for-your-local-machine",
    "href": "lesson/getting-setup.html#installing-git-for-your-local-machine",
    "title": "Getting Set Up",
    "section": "Installing Git (for your local machine)",
    "text": "Installing Git (for your local machine)\nYou will need the program Git for this tutorial. First, let’s check to see if Git is already installed.\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console (bottom-left panel). Enter the following:\n\n\nCode\nwhich git\ngit --version\n\n\n/usr/local/bin/git\ngit version 2.23.0\n\n\nYou will get something like the above output if Git is already installed. If it is not installed, you will get something like git: command not found.\n\nWindows\nDownload the program here: https://git-scm.com/downloads\n\n\nMac\nYou may have been prompted to install command line developer tools. You should accept this offer. If you were not prompted, use this command:\n\n\nCode\nxcode-select --install\n\n\n\n\nIntroduce Yourself to Git in R\nOnce Git is successfully installed and you have a GitHub account, you will need to let R/Git know your account information to access the remote repositories.\nEnter the following in the Console tab, substituting the user name and email with your GitHub user name and email:\n\n\nCode\nusethis::use_git_config(user.name = \"Bob Barker\", user.email = \"bobbarker@thepriceisright.org\")\n\n\n\n\nGet a Personal Access Token\nA personal access token (PAT) is used for GitHub as a type of authentication. You used to be able to use your username and password, but not any more. The token-based authentication has increased security.\nBefore generating a new PAT, check to see if you already have one. Run gitcreds_set() and one of two things will happen:\n\n\nR will prompt you to enter a token. This means you don’t already have one and need to create one. Hit Esc and run the second line of code, create_github_token(). This will bring you to GitHub where you can create a token. Save this token somewhere safe - you will not be able to access it via GitHub again.\nR will show you the saved credentials (username and password) and give you options to exit without changing (1: Abort), replace the credentials, or to see the current token. This means you already have a token and can keep it or change it if it has expired. If it expired and you need a new one, run create_github_token().\n\n\n\nCode\n## Run this to see if you already have credentials, or to change them\ngitcreds::gitcreds_set()\n## Only run this if you need to generate a token\ncreate_github_token()"
  },
  {
    "objectID": "lesson/getting-setup.html#version-control",
    "href": "lesson/getting-setup.html#version-control",
    "title": "Getting Set Up",
    "section": "Version Control",
    "text": "Version Control\nThe main function of Git is version control. This means Git will track the changes you make so you can revert to (or view) previous versions of the document. In order for this to work effectively, you need to start tracking your changes (make a repository) and frequently create versions with changes you have made (committing)."
  },
  {
    "objectID": "lesson/getting-setup.html#setting-up-a-local-repository",
    "href": "lesson/getting-setup.html#setting-up-a-local-repository",
    "title": "Getting Set Up",
    "section": "Setting Up a Local Repository",
    "text": "Setting Up a Local Repository\nAnother function of Git is to store all your relevant files in a repository, similar to a project. You can create a repository that is located only on your device (therefore it is considered “local”). This will allow you to track changes to a project on your computer and access previous versions of the document at any time. We will look at how to create a local repository using Rstudio, but know that it is also possible to do this using the terminal. (See page 96 of Gandrud (2015))\n\nIn Rstudio, select File in the top left corner, and then New Project\nWhen the New Project menu appears, select New Directory\n\n\n\n\n\n\n\n\n\n\n\nOn the following screen, select New Project\nFinally, type the name you want to use for your new project, browse to the location where you want it saved on your computer, and select the Create a git repository box.\n\n\nYou now have a folder with a .Rproj object and a .gitignore file. There is also a hidden .git folder that stores all the project information, including the version history files (commit history).\n\nAdding, Staging, and Committing\nYou can create new files or move existing files (including your data) into this project folder, and Git can track changes made to them. In order to do this, you will need to “add” files that have been created or moved into the project and “commit” these changes.\nFirst, you will need to save the new file or move an existing file into the project folder. Then, you will “add” those files to your commit by checking the boxes in the Git tab in your Rstudio window. If Git is already tracking a document, you will see an M in the status column, however, if the file hasn’t been added (and is therefore not being tracked), you will see a ? in the status column.\n\nIn order to save a version of your project as it is right now (with these added files), you will need to commit. To do this, select the Commit button, above the boxes you just checked. This will open a new window where you can see the list of files that you added.\nType a useful message in the Commit message box briefly describing the changes you made in this version. Then hit Commit.\n\nYou should see a window pop up telling you what changes were successfully made. If this window ever says “failed”, “execution halted”, or “aborted”, this means the commit did not work and you should read the message closely to determine why.\nSave and commit frequently in order to get the most use out of your version control."
  },
  {
    "objectID": "lesson/getting-setup.html#branches-and-merging",
    "href": "lesson/getting-setup.html#branches-and-merging",
    "title": "Getting Set Up",
    "section": "Branches and Merging",
    "text": "Branches and Merging\n\nBranches\nA new repository will have one branch called main. You can think of this as the master version. You can create additional branches which are an exact copy of the main branch where you can make changes without committing them to the master. This is useful when several people are working on the same thing, or if you want to try multiple approaches to the same file (i.e. test run some code).\nTo create a branch, select the button with small purple shapes on the right hand side of your Git tab in Rstudio.\n\nYou can now create a new branch, for example one called “test”. This is essentially a copy of everything that is in your repository, and making changes here will not affect the main branch.\nTo switch between branches, select the drop down menu to the right of the button you used to create a new branch.\n\nYou can easily switch to a different branch by selecting it on this menu.\nNote that any files you create on a side branch will not show up in the main branch unless you merge them.\nYou can have multiple branches coming off your main branch at once, so you can try multiple different approaches (or by multiple people) simultaneously. Anytime you make a new branch it will start as a duplicate of the main branch.\nYou can see a history of your commits on different branches by selecting the clock icon (designating “History”) near the Commit button on your Git tab. You will initially see the history for only the branch you are currently in, but if you select the drop down menu of branches at the top of this window, you can select another branch or all branches and see how the branches compare to one another.\n\n\n\nMerging\nIf you find a method that works in a side branch and you want to bring it in to the main branch, this is called a merge.\nTo merge a side branch with the main branch (changing the version of your repo main branch to match that of the successful side branch), we will use the terminal.\n\nMake sure you are on the main branch\nSelect the Terminal tab next to the Console tab in the section of Rstudio displaying your console.\nType git merge and then the name of the branch you want to merge into the main branch.\n\n\nThe repository of your main branch should now match the repository of whatever branch you merged with it.\nOne way to check if the branches merged the way you intended is to select the history button again (make sure you go to all branches) and check that the “HEAD” (which is the main branch) is at the same level as whichever branch you merged it with."
  },
  {
    "objectID": "lesson/getting-setup.html#setting-up-remote-repositories",
    "href": "lesson/getting-setup.html#setting-up-remote-repositories",
    "title": "Getting Set Up",
    "section": "Setting Up Remote Repositories",
    "text": "Setting Up Remote Repositories\nTo set up a remote repository, make sure you are able to log into GitHub. We will go over three different ways to make repositories: starting from scratch with a new repository, creating a repository for an already started project, and accessing a repository made by a collaborator.\n\nNew Repository\nTo create a brand new repository, click on the plus sign near your profile picture in the top right corner of GitHub and select New repository.\n\nOn the next page, type in a name for your repository and choose whether you want it to be public or private. It is recommended you select the box to create a README file. This will give you a place to describe the layout of your repository and the purpose of each file. Without a README file, visitors to your repository (and maybe future you) might not be able to figure out how to properly use the files in your repository, leading to your working being non-reproducible.\n\nNow you can click Create Repository.\nOnce your repository is created, you will see only the README file is present. To create other files, let’s create a directory for this repo on our computer.\n\nIn R studio, follow the instructions for creating a new project, but when you see the following menu, select Version Control this time.\n\n\n\n\nOn the next window, select Git.\nReturn to the GitHub page for the repository you created and select the green Code button.\nCopy the HTTPS link from the menu that pops up.\n\n\n\nReturn to Rstudio and paste the link in the Repository URL line at the top of the popped up window. Make sure to check the file path that is currently set and use the Browse button if you want to save the folder for this directory in a different place.\n\n\nYour repository is now set up in a new project in Rstudio, and you can begin by creating an Rmarkdown, R Script, etc.\nNote, you can edit the contents of the README file by opening it in Rstudio.\n\n\nFrom an Existing Project\nYou may find yourself wanting to create a repo on GitHub for a project you have already started working on. Fortunately, it is easy to start version control tracking on a project and add the project to a remote repository.\nHere are the steps to follow if you have a project folder with a .Rproj file in it and your directory is not already being tracked with Git:\n\nFollow the steps above for creating a New repository, but this time do not click the box to create a README file. This will bring you to a page with a “Quick Set Up” link. Keep this page open for later.\n\n\n\n\nGo to RStudio and open the .Rproj file that will be added to the GitHub repository./\nSelect the Terminal tab next to Console and enter the following lines of code.\n\n\n\nCode\n$ git init -b main\n\n\n“git” tells bash what program we want to use   “init” tells bash to initialize a Git repository fir this directory\n“-b main” is saying we want to create a branch called “main”\n\n\n\nCode\n$ git add .\n\n\nThis will add all of the files within the current folder to the repository. You may get a lot of warnings because you have a .Rproj file and potentially a .Rhistory file in this folder which are not usually ideal to track. We will take care of this by adding these file names to the .gitignore file shortly.\n\n\nCode\n$ git commit -m \"First commit\"\n\n\nThis line creates your first commit. You can change the commit message to anything that makes sense to you.\n\nGo back to GitHub and copy the Quick start link. Type the following code in the terminal, but replace &lt;REMOTE_URL&gt; with the copied link.\n\n\n\nCode\n$ git remote add origin &lt;REMOTE_URL&gt;\n# Replace \"&lt;REMOTE_URL&gt;\" with url from GitHub\n\n\n\nLast, we will push these changes to GitHub. We will further discuss what this means later, but for now run the following line in the terminal.\n\n\n\nCode\n$ git push origin main\n\n\nNow when you return to GitHub and refresh your repository page, you should see all of the files from your existing project.\nYou can add a README and .gitignore on the GitHub website by selecting “Add file” on the repository’s page. When you make a .gitignore file, it may suggest you use a template, in which case, select the R template from the drop down list and it will automatically fill the document with file types that should typically not be tracked.\n\n\nCloning a Repository\nIf you want to join a repository that has already been created, you can either find the repository by searching for it on GitHub (if it is publicly available) or contact the creator and have them add you as a collaborator to the repository. Either way, you will follow steps 1-5 in the “New Repository” section above, but this time the link is coming from the already created repository.\nIn cases where the repository is not accessible (you do not have cloning priveleges), you will have to create a pull request."
  },
  {
    "objectID": "lesson/getting-setup.html#pushing-and-pulling",
    "href": "lesson/getting-setup.html#pushing-and-pulling",
    "title": "Getting Set Up",
    "section": "“Pushing” and “Pulling”",
    "text": "“Pushing” and “Pulling”\nOnce you have your remote repository ready, you can make changes to the files and “add” and “commit” them like we did in the “Version Control” section above. However, now we must take steps to make sure our local version of the repository is up-to-date with the online version, and the versions that all other collaborators have on their computers.\nTo do this, we will need to “push” and “pull”. “Pulling” is when we bring recent and out-of-sync changes from the online version to our local device. “Pushing” is the opposite; we are sending our commits to the online version to update it with our recent changes. This will allow anyone else working in the repository to “pull” your changes onto their computer.\nTo push and pull, use the blue and green arrows on the Git tab in R studio.\n\nYou may also notice the next time you “commit” your changes, these same push and pull buttons are located above the “Commit Message” box on the pop up window.\nIt is good practice to “pull” each time you are about to start working in a repository and to push after you commit, or at least once at the end of your working period within a repository for the day. Keeping good “push” and “pull” habits will help you avoid merge conflicts with collaborators or yourself if you work on a project on more than one computer."
  },
  {
    "objectID": "lesson/getting-setup.html#adding-and-managing-collaborators",
    "href": "lesson/getting-setup.html#adding-and-managing-collaborators",
    "title": "Getting Set Up",
    "section": "Adding and Managing Collaborators",
    "text": "Adding and Managing Collaborators\nCollaborative coding is a huge benefit of GitHub. In order to invite your collaborators to clone your remote repository, you will need to know their GitHub username, or at least their email address.\n\nOn your repository page on GitHub, select Settings in the middle of the banner near the top of the page.\n\n\n\nIn the menu on the left, select the Collaborators page.\n\n\n\nEnter your password, and then select Add People under Manage Access\n\nAdd your collaborators one at a time. This will send them a message inviting them to join the repository.\nAs the owner of the repository, you will be able to remove people from the repository at any time."
  },
  {
    "objectID": "lesson/getting-setup.html#merge-conflicts",
    "href": "lesson/getting-setup.html#merge-conflicts",
    "title": "Getting Set Up",
    "section": "Merge Conflicts",
    "text": "Merge Conflicts\nWhen two people are working on a branch at the same time, changes were made and not pushed before someone else started working, or a collaborator forgets to pull before starting to make changes, a merge conflict may arise. Merge conflicts happen because Git is not sure how to combine the different changes that occurred in the same sections of the document. In some instances, Git is smart enough to figure it out and will merge the versions on its own. Other times, the merge conflict will be need to be fixed manually. Note that Git will not allow the push until the merge conflict is solved.\nIf a merge conflict occurs, you will see some specific things added to your code. There will be a line at the beginning that starts with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, a line at the end starting with &gt;&gt;&gt;&gt;&gt;&gt;&gt; and a line in the middle that just has =======. The two different version are shown above and below the middle line, labeled with which branch has which version. It is your job to decide how these versions fit together. Once you have modified the code to match the final version you want to keep, you can delete the three lines of code containing the &gt;, &lt; and = symbols and “stage” and “commit” your files as usual.\n\n\nOn GitHub\nYou can also merge branches and resolve merge conflicts on GitHub. You would push your branch to GitHub, then go to the GitHub repo webpage. You will need to create a pull request to merge your branch. If there is a merge conflict, you will need to click on “Resolve conflicts” before you can merge your branch. The syntax to edit the document and resolve the conflict is the same as above."
  },
  {
    "objectID": "lesson/getting-setup.html#cloning-branching-and-forking-oh-my",
    "href": "lesson/getting-setup.html#cloning-branching-and-forking-oh-my",
    "title": "Getting Set Up",
    "section": "Cloning, Branching, and Forking, Oh My!",
    "text": "Cloning, Branching, and Forking, Oh My!\nCloning, branching, and forking are functions that are similar, but they are not the same. When you clone a repository, you are connected to it and are working in that repository. You can commit changes and push them to the same repository. When you create a branch, you create a copy where you can work on a specific part of a document or run test code with the intent to merge it back to the main branch. Many branches are short-lived and deleted once their purpose has been served. Anyone that has access to the repository also has access to the branches in it. A fork creates a copy of the entire repository as well, however, the collaborators are disconnected from it. The intent is generally to diverge from the original repository and never be merged back into it."
  },
  {
    "objectID": "lesson/getting-setup.html#gitignore-and-large-files",
    "href": "lesson/getting-setup.html#gitignore-and-large-files",
    "title": "Getting Set Up",
    "section": ".gitignore and Large Files",
    "text": ".gitignore and Large Files\nGitHub does not allow repositories to be larger than 5GB. While we recommend keeping all of the files necessary to run your analysis together in the repo, this may not be possible if say for example your data files are larger than the size limit. If this is the case, you can manually distribute your data file a different way. (See https://git-lfs.github.com/ first though if this is an issue you are actually having.)\nOnce all collaborators (or just you) have the large data file in your Git repo, you may forget that you cannot send this to GitHub and accidentally try to commit and push it. Fortunately, Git will recognize the problem before it takes place and will give you the following warning:\n\nTo resolve this problem, you just need to tell Git to ignore the large file when you make your commit. First, determine which file(s) is/are too big by looking at their size. Then, navigate to your .gitignore document and open it. Last, add the name(s) of the file(s) that is/are too large. Now you will be able to commit and push the tracked files within your repository.\nYou can add any files you do not want to be tracked to .gitignore. An example is the html file generated by rendering this document. It is regenerated constantly, so there is no need to track the changes."
  },
  {
    "objectID": "lesson/getting-setup.html#amend-commits",
    "href": "lesson/getting-setup.html#amend-commits",
    "title": "Getting Set Up",
    "section": "Amend Commits",
    "text": "Amend Commits\nIf at any point you find you have made a commit that should not have been made (e.x. you accidentally added and committed files that are too large and now you aren’t able to push), you can easily fix this by making the necessary changes within your files, checking the “amend previous commit” box on the commit screen, and then committing as usual. This will overwrite your previous commit with the correct version you want to commit. If you need to amend an earlier commit (i.e. not the most recent commit), you will need to use the terminal (See Oh S#!*, Git!?!)."
  },
  {
    "objectID": "lesson/getting-setup.html#installing-a-git-client",
    "href": "lesson/getting-setup.html#installing-a-git-client",
    "title": "Getting Set Up",
    "section": "Installing a Git Client",
    "text": "Installing a Git Client\nWith all of the commits, branching, merging, and collaboration, it can be tricky to keep track of everything going on in your repository. Viewing the commit history in GitHub or under the Git tab in RStudio is useful, though it can still be a little hard to follow. Using a Git client software helps visualize the workflow and allows you to use commands in the graphic user interface (GUI) that you would normally need to type into the terminal.\n\nThere are a few Git Clients out there and you may want to try a few to see what works for you. I use GitKraken which you can download here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Intro to Spatial Data in R\n        ",
    "section": "",
    "text": "Intro to Spatial Data in R\n        \n        \n            Use R to load, visualize, and analyze spatial data\n        \n        \n            HES 505 • Fall 2023Human-Environment SystemsBoise State University\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nInstructor\n\n   Dr. Matt Williamson\n   4125 Environmental Research Building\n   mattwilliamson@boisestate.edu\n   MwilliamsonMatt\n   Schedule an appointment\n\n\n\nCourse details\n\n   Mondays and Wednesdays\n   August 21–December 13, 2023\n   1:30–2:45 PM\n   SMASH 116\n   Slack\n\n\n\nContacting me\nE-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 48 hours (really), but also remember that life can be busy and chaotic for everyone (including me!), so if I don’t respond right away, don’t worry!"
  },
  {
    "objectID": "example/getting-setup.html",
    "href": "example/getting-setup.html",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\n\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\n\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#lets-git-started",
    "href": "example/getting-setup.html#lets-git-started",
    "title": "Getting Setup",
    "section": "",
    "text": "We are using GitHub classroom for all of the assignments in this course. This allows each of you to have your own repositories for version control and backup of your code without the worries of stepping on someone else toes. The goal of this class is not to have you become a ‘master’ of all things git, but I am hoping you’ll learn the utility of version control and adopt as much of it as make sense for you and your workflows.\n\n\nThe first thing you’ll need to do is accept the invitation to ’assignment-1` repository (repo). This should automatically clone (make an exact copy) of the assignment repo in your personal account.\n\n\n\nUnfortunately, GitHub has ended its support for username/password remote authentication. Instead, it uses something called a Personal Access Token. You can read more about it here if you are interested, but the easiest way to deal with this is by following Jenny Bryan’s happygitwithr recommended approach:\n\nIntroduce yourself to git: There are a number of ways to do this, but I find this to be the easiest\n\n\n\nCode\nlibrary(usethis) #you may need to install this using install.packages('usethis')\nuse_git_config(user.name = \"Matt Williamson\", user.email = \"mattwilliamson@boisestate.edu\") #your info here\n\n\n\nGet a PAT if you don’t have one already (make sure you save it somewhere)\n\n\n\nCode\nusethis::create_github_token()\n\n\n\nStore your credential for use in RStudio\n\n\n\nCode\nlibrary(gitcreds) #may need to install this too\n\ngitcreds_set() #should prompt you for your pat - paste it here\n\n\n\nVerify that Rstudio has saved your credential\n\n\n\nCode\ngitcreds_get()\n\n\nR should return something that looks like this:\n\n\n\n\n\n\n\n\n\nGo to File&gt;New Project and choose the “Version Control” option\nSelect “Git” (Not Subversion)\npaste the link from the “Clone Repository” button into the “Repository URL” space\n\n\n\n\nAssuming all this has worked, you should be able to click on the “Git” tab and see something like this:\n\n\n\n\n\n\n\n\n\nEverytime you begin working on code, make sure you “Pull” from the remote repository to make sure you have the most recent version of things (this is especially important when you are collaborating with people).\nMake some changes to code\nSave those changes\n“Commit” those changes - Think of commits as ‘breadcrumbs’ they help you remember where you were in the coding process in case you need to revert back to a previous version. Your commit messages should help you remember what was ‘happening’ in the code when you made the commit. In general, you should save and commit fairly frequently and especially everytime you do something ‘consequential’. Git allows you to ‘turn back time’, but that’s only useful if you left enough information to get back to where you want to be.\nPush your work to the remote - when you’re done working on the project for the day, push your local changes to the remote. This will ensure that if you switch computers or if someone else is going to work on the project, you (or they) will have the most recent version. Plus, if you don’t do this, step 1 will really mess you up."
  },
  {
    "objectID": "example/getting-setup.html#quarto",
    "href": "example/getting-setup.html#quarto",
    "title": "Getting Setup",
    "section": "Quarto",
    "text": "Quarto\nThis is a Quarto document (in fact, this whole webpage and all of the slides were built with Quarto). Quarto uses the knitr package to render files containing R, python, and julia to Markdown as a means of rendering code, text, math, figures, and tables to a variety of formats.\n\n\n\n\n\nMarkdown is a simple formatting syntax for authoring HTML documents (it’s the basis for the Readme docs that GitHub creates for you). From there, RStudio calls pandoc to render the markdown file into your chosen output format. I’m telling you this because there will be times when some part of this pipeline may break and you’ll need to know where the errors might be coming from.\nYou can create new Quarto documents by going to File &gt;&gt; New File &gt;&gt; New Quarto Document (or Presentation). There are lots of new documents devoted to Quarto, but some of them may assume you have some familiarity with Markdown or Rmarkdown. As such, I’m keeping this links to helpful Rmarkdown resources like this cheatsheet and a much longer user’s guide in case you need more in-depth discussion of some of the ideas behind authoring in Quarto. I don’t expect you to become an expert in Quarto, but it is a helpful way to keep all of your thoughts and code together in a single, coherent document. Getting proficient in Quarto and git allows you to work with collaborators on an analysis, graphics, and manuscript all within a single platform. This fully-integrated workflow takes practice and patience (especially when you have collaborators that are new to this approach), this course is just an initial step down that path. I’ll do my best to keep it simple - please let me know if you have questions!"
  },
  {
    "objectID": "example/getting-setup.html#the-example",
    "href": "example/getting-setup.html#the-example",
    "title": "Getting Setup",
    "section": "The Example",
    "text": "The Example\n\nSetup\nThe University of Exeter has been conducting an ongoing survey to understand the age at which the belief in Santa Claus begins to drop off. A sample of the data is located in your assignment01 folder. Our task is to bring the data into R, conduct some preliminary exploration of the data, and then fit a model to the data to see if age predicts belief in Santa. We’ll start by branching off of the master Quarto doc in our GitHub repo and then work through the steps together.\n\n\nPseudocode\nBefore we get started, let’s sketch out the steps in our analysis using pseudocode. If you take a look at the tasks I’ve outlined above, you might construct your pseudocode like this:\n\n\nCode\nLOAD: all packages that we need for the analysis\nREAD: Data located in isthereasanta.txt\nCHECK: Data structure and values\nCLEAN: Are there odd values?\nPLOT: Age vs Belief\nMODEL: GLM of Age vs. belief\n\n\n\n\nProgramming\nNow that we have the basic steps in place, let’s transform the pseudocode into a repeatable Quarto document that explains what we’re doing, why, and what we found.\n\nLoad the packages\nPart of what makes R so powerful for data analysis is the number of ready-made functions and packages that are designed for all the things. That said, you can’t take advantage of that power if you don’t load them into your session so that their functions become available. In general, it’s best to do that first thing your document so that other folks can see what packages are necessary before you start running analyses. If you pay attention when these packages load, you may see warnings that a function is masked. This happens because two (or more) packages have functions with the same name. We can be explicit about which version we want by using packagename::functionname(). You’ll see that more later this semester.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nRead the Data\nBased on our pseudocode our first step is the read the data. We can create headings in Quarto using different numbers of # symbols to keep things organized. The code below uses ``` to create the code chunk and then {r} to tell Quarto which environment to use when running it. I’m specifying a filepath because I’m not working within our git repo, this isn’t great practice, but it’s necessary for the webpage to render correctly. We use paste0 to combine the filepath with the file name (isthereasanta.txt) then read in the data using read_table.\n\n\nCode\nfilepath &lt;- \"/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2022/assignment01/\"\n#READ\nsanta &lt;- read_table(paste0(filepath, \"isthereasanta.txt\"))\n\n\n\n\nCheck out the Data\nNow that we’ve got the data loaded and assigned it to the santa object. It’s always a good idea to take a look and make sure things look the way you expect, check for NAs, and get a basic understanding of the way your data is being represented by R. This process will get more involved once we start working with spatial data, but it’s good to get in the habit now. We’ll start by looking at the first few rows (using head()), then get a sense for the classes of data using str(), and check for any NAs.\n\n\nCode\nhead(santa)\nstr(santa)\nany(is.na(santa))\n\n\nYou’ll notice a few things. First, because we read this in using the read_table function, the result is a tibble. As such, head() returns both the data and the classes. This makes the result of str() largely redundant (note that if santa were a data.frame this would not be true). The combination of any() with is.na() asks whether any of the cells in santa have an NA value. You can see that there are NAs. Most statistical modeling functions in R don’t like NAs so we’ll try to clean those up here. Before we clean them, let’s try to learn what they are. We can use which() to identify the locations of the NAs.\n\n\nCode\nwhich(is.na(santa), arr.ind = TRUE)\n\n\nWe see that all of them are in the age column (our key predictor variable!). We could also have discovered this using summary().\n\n\nCode\nsummary(santa)\n\n\n\n\nClean the data\nDeciding how to clean NAs is an important decision. Many people choose to drop any incomplete records. We can do that with complete.cases() and see that the resulting object now has only 47 rows.\n\n\nCode\nsanta_complete_cases &lt;- santa[complete.cases(santa),]\n\n\nDropping the incomplete cases may seem like a “safe” approach, but what if there is some systematic reason for the data to be incomplete. Maybe older people are less likely to provide their age? If that’s the case, then dropping these cases may bias our dataset and the models that result. In that case, we may decide to “impute” values for the NAs based on some principled approach. We’ll talk more about what it means to take a principled approach to imputation later in this class. For now, let’s just try to strategies: 1 where we assign the mean() value of age and one where we assign the max() value (to reflect our hypothesis that older people may not provide their age). We’ll do this by using the ifelse() function. Note that we can only do this because all of the NAs are in a single column.\n\n\nCode\nsanta_mean &lt;- santa\nsanta_mean$Age &lt;- ifelse(is.na(santa_mean$Age), round(mean(santa_mean$Age, na.rm=TRUE),digits=0), santa_mean$Age)\n\nsanta_max &lt;- santa\nsanta_max$Age &lt;- ifelse(is.na(santa_max$Age), max(santa_max$Age, na.rm=TRUE), santa_max$Age)\n\n\n\n\nPlot the Data\nNow that we have a few clean datasets, let’s just take a quick look to see if our intuition is correct about the relationship between age and belief in santa. The idea isn’t so much to “prove” your hypothesis, but rather to get to know your data better as a means of identifying potential outliers and thinking about the distribution of your data.\n\n\nCode\nplot(Believe ~ Age, data=santa_complete_cases, main=\"Age vs. Belief in Santa (complete cases)\")\n\nplot(Believe ~ Age, data=santa_mean, main=\"Age vs. Belief in Santa (Age at mean)\")\n\nplot(Believe ~ Age, data=santa_max, main=\"Age vs. Belief in Santa (Age at max)\")\n\n\nThese plots highlight two things. First, because Believe is a logical variable, the only possible outcomes are 0 and 1. This means we can’t fit a typical linear regression (we’ll use a logistic regression instead). Also, we notice that our choice of imputation strategy makes a difference! Let’s fit some models and see what kind of difference it makes.\n\n\nFit Some Models\nWe’ll be using a generalized linear model for this analysis. The details will come up later, but for now, let’s keep it simple. The syntax for the glm() function is relatively straightforward. First we specify the model Believe ~ Age, then we tell it what family binomial(link=\"logit\"), then we remind R of the data. We use the binomial family because there are only 2 possible outcomes (TRUE and FALSE).\n\n\nCode\nfit_complete_cases &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_complete_cases)\nfit_mean &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_mean)\nfit_max &lt;- glm(Believe ~ Age, family=binomial(link=\"logit\"), data=santa_max)\n\nsummary(fit_complete_cases)$coef\nsummary(fit_mean)$coef\nsummary(fit_max)$coef\n\n\nWe see the older a person is, the less likely they are to believe in Santa! We also see that the choice of how we handle NAs affects the size of the effect, but not the direction. In class, we’ll write a function to simulate some new data based on this model and see if our results are robust to different assumptions.\n\n\n\nRendering the document\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document."
  },
  {
    "objectID": "content/32-content.html#objectives",
    "href": "content/32-content.html#objectives",
    "title": "Conclusion",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/30-content.html#objectives",
    "href": "content/30-content.html#objectives",
    "title": "Data Visualization and Maps II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/26-content.html#objectives",
    "href": "content/26-content.html#objectives",
    "title": "Movement and Networks II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/24-content.html#objectives",
    "href": "content/24-content.html#objectives",
    "title": "Statistical Modelling III",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/22-content.html#objectives",
    "href": "content/22-content.html#objectives",
    "title": "Statistical Modelling I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/20-content.html",
    "href": "content/20-content.html",
    "title": "Proximity and Areal Data",
    "section": "",
    "text": "Last class we started to explore ways to leverage spatial autocorrelation as a means of using interpolation to generate values at unobserved locations. We’ll continue that discussion using variograms and kriging. We then move to a discussion of areal data and the need to identify “neighbors” as a means of understanding how to weight observations when the actual point location of the observation may be unknown or impossible to assign."
  },
  {
    "objectID": "content/20-content.html#resources",
    "href": "content/20-content.html#resources",
    "title": "Proximity and Areal Data",
    "section": "Resources",
    "text": "Resources\n\n Ch. 7: Spatial Neighborhood Matrices in from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R gives a little gentler introduction to spatial neighbors specifically in the context of statistical models.\n Chapter 14 Proximity and Areal Data in Spatial Data Science by Edzer Pebesma and Roger Bivand provides explanations of how the spdep package can be used to construct neighborhood weights."
  },
  {
    "objectID": "content/20-content.html#objectives",
    "href": "content/20-content.html#objectives",
    "title": "Proximity and Areal Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDescribe and implement statistical approaches to interpolation\nDescribe the case for identifying neighbors with areal data\nImplement contiguity-based neighborhood detection approaches\nImplement graph-based neighborhood detection approaches\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nPanopto Recording"
  },
  {
    "objectID": "content/18-content.html",
    "href": "content/18-content.html",
    "title": "Combining Data and Point Patterns",
    "section": "",
    "text": "Today we’ll finish up our example of combining data for analysis and introduce point process models as a first version of spatial analysis. We’ll need a few new packages here, but many of the key data management processes will remain the same."
  },
  {
    "objectID": "content/18-content.html#resources",
    "href": "content/18-content.html#resources",
    "title": "Combining Data and Point Patterns",
    "section": "Resources",
    "text": "Resources\n\n The Chapters 17 and 18 on Spatial Point Processes and the spatstat package in Paula Moraga’s book Spatial Statistics for Data Science: Theory and Practice with R.\n Rings, circles, and null-models for point pattern analysis in ecology by (Wiegand and A. Moloney 2004) provides an introduction to metrics for spatial clustering with applications in ecology.\n Improving the usability of spatial point process methodology: an interdisciplinary dialogue between statistics and ecology by Janine Illian (a major contributor to modern point pattern analyses) and David Burslem (a Scottish plant ecologist) (Illian and Burslem 2017) is a fairly modern take on the challenges associated with point process modeling in ecology.\n Chapter 11: Point Pattern Analysis in Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provides a nice (and free) introduction to some of these introductory point process methods."
  },
  {
    "objectID": "content/18-content.html#objectives",
    "href": "content/18-content.html#objectives",
    "title": "Combining Data and Point Patterns",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nComplete the creation of a dataset for analysis using vector and raster data\nDefine a point process and their utility for ecological applications\nDefine first and second-order Complete Spatial Randomness\nUse several common functions to explore point patterns"
  },
  {
    "objectID": "content/18-content.html#slides",
    "href": "content/18-content.html#slides",
    "title": "Combining Data and Point Patterns",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Video"
  },
  {
    "objectID": "content/16-content.html",
    "href": "content/16-content.html",
    "title": "Integrating Rasters and Vector Data",
    "section": "",
    "text": "The goal of much of our spatial data “munging” is to create a dataframe that can be used in subsequent statistical analyses. It can be difficult to link all of the steps of filtering, selecting, extracting, etc into a coherent problem when you are just being exposed to the syntax (as we discovered last week). Today, I’ll try to use a motivating example to help you see a path forward."
  },
  {
    "objectID": "content/16-content.html#resources",
    "href": "content/16-content.html#resources",
    "title": "Integrating Rasters and Vector Data",
    "section": "Resources",
    "text": "Resources\n\n The Spatial Data Operations Chapter in (Lovelace et al. 2019) makes the concepts of a network concrete (literally) by using a transportation route example to illustrate the various components of a network analysis in R.\n Chapter 3. Processing Tabular Data from Geographic Data Science with R by Michael C. Wimberly has a nice introduction to many of the dplyr verbs for manipulating tabular data.\n Chapter 9. Combining Vector Data with Continuous Raster Data from Geographic Data Science with R by Michael C. Wimberly introduces data extraction and zonal statistics for raster data.\n Chapter 10. Combining Vector Data with Discrete Raster Data from Geographic Data Science with R by Michael C. Wimberly extends Chapter 9 for discrete rasters, but also adds some additional buffering and data manipulation syntax."
  },
  {
    "objectID": "content/16-content.html#objectives",
    "href": "content/16-content.html#objectives",
    "title": "Integrating Rasters and Vector Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nUse dplyr with predicates and measures to subset and manipulate data\nUse extract to access raster data\nUse zonal to summarize access data\nJoin data into a single analyzable dataframe"
  },
  {
    "objectID": "content/16-content.html#slides",
    "href": "content/16-content.html#slides",
    "title": "Integrating Rasters and Vector Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Zoom Recording"
  },
  {
    "objectID": "content/14-content.html",
    "href": "content/14-content.html",
    "title": "Operations with Raster Data II",
    "section": "",
    "text": "Now that we’ve done some “global” transformations of raster data using terra, we’ll look at some of the options for cell-wise transformations. Rather than manipulating the extent, resolution, or CRS of the raster data; we’ll actually be using functions to change the values of the cells themselves."
  },
  {
    "objectID": "content/14-content.html#readings",
    "href": "content/14-content.html#readings",
    "title": "Operations with Raster Data II",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today."
  },
  {
    "objectID": "content/14-content.html#objectives",
    "href": "content/14-content.html#objectives",
    "title": "Operations with Raster Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nUse moving windows as a means of smoothing raster data\nReclassify data using conditional statements and reclassification tables\nUse raster math as a means of creating new data based on an existing dataset."
  },
  {
    "objectID": "content/14-content.html#slides",
    "href": "content/14-content.html#slides",
    "title": "Operations with Raster Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto Slides"
  },
  {
    "objectID": "content/12-content.html",
    "href": "content/12-content.html",
    "title": "Operations With Vector Data II",
    "section": "",
    "text": "Now that you have the complete picture of predicates, measures, and transformers; it’s time to use them on some actual data. This lecture is meant to be the “practical” application of the ideas you’ve learned in our previous discussions of vector data and give you enough tools to begin to subset your data to the records and attributes of interest, calculate new spatial metrics, and generate new geometries based on existing data."
  },
  {
    "objectID": "content/12-content.html#readings",
    "href": "content/12-content.html#readings",
    "title": "Operations With Vector Data II",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)."
  },
  {
    "objectID": "content/12-content.html#objectives",
    "href": "content/12-content.html#objectives",
    "title": "Operations With Vector Data II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nTranslate pseudocode commands into functional workflows\nArticulate the importance of key arguments to sf functions\nGenerate new attributes and geometries from existing data."
  },
  {
    "objectID": "content/12-content.html#slides",
    "href": "content/12-content.html#slides",
    "title": "Operations With Vector Data II",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Introduction to Mapping Geographic Data",
    "section": "",
    "text": "Now that we’re getting into actual operations on spatial data and beginning to actually modify the geometries and attributes of spatial data, it’ll be important for you to be able to visualize the results. At this point, we’ll be focusing on rough visualization as a way of “gut-checking” the outcomes of your code. We’ll focus more on creating informative, aesthetically pleasing, publication quality visualizations."
  },
  {
    "objectID": "content/10-content.html#readings",
    "href": "content/10-content.html#readings",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Readings",
    "text": "Readings\n\n Ch.3 Tmap in a nutshell from “Elegant and informative maps with tmap” by Martijn Tennekes and Jakub Nowosad provides a great “quick start” for using the tmap package for visualizing spatial data.\n Making maps with R by (Lovelace et al. 2019) introduces the tmap package for making nice maps with relatively minimal syntax.\n Making maps in R by Emily Burchfield illustrates some quick mapping syntax with base plot, ggplot, and tmap. For now, just focus on the base plot and tmap sections as we’ll take on the ggplot stuff later in the course."
  },
  {
    "objectID": "content/10-content.html#objectives",
    "href": "content/10-content.html#objectives",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDescribe the basic components of data visualization as a foundation for mapping syntax\nUnderstand layering in both base plot and tmap\nMake basic plots of multiple spatial data objects"
  },
  {
    "objectID": "content/10-content.html#slides",
    "href": "content/10-content.html#slides",
    "title": "Introduction to Mapping Geographic Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto Slides"
  },
  {
    "objectID": "content/08-content.html",
    "href": "content/08-content.html",
    "title": "Areal Data: Vectors",
    "section": "",
    "text": "Now that you have started working with the various components of coordinates and coordinate reference systems, it’s time to start learning the fundamental aspects of working with vector data in sf and R. The syntax is a little confusing at first, but once you’ve gotten a sense for the logic behind it you should be able to start piecing together the functions necessary to implement the pseudocode you write for an analysis. We’ll spend more time on vector manipulation in the coming weeks so you’ll get plenty of practice with the ideas we introduce today."
  },
  {
    "objectID": "content/08-content.html#readings",
    "href": "content/08-content.html#readings",
    "title": "Areal Data: Vectors",
    "section": "Readings",
    "text": "Readings\nSame as last class really, but hopefully you’ll begin to understand them better…\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)"
  },
  {
    "objectID": "content/08-content.html#objectives",
    "href": "content/08-content.html#objectives",
    "title": "Areal Data: Vectors",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nImplement approaches for checking and reparing geometries in R\nUnderstand predicates and measures in the context of spatial operations in sf\nUse st_* to evaluate attributes of geometries and calculate measurements"
  },
  {
    "objectID": "content/08-content.html#slides",
    "href": "content/08-content.html#slides",
    "title": "Areal Data: Vectors",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording"
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "",
    "text": "Today we’ll focus on some of the tools for reproducible workflows using R. We’ll introduce Quarto as a means of authoring different kinds of documents. We’ll talk about literate programming and leaving breadcrumbs for yourself (and others). Finally, we’ll begin to work through the ideas of workflow planning"
  },
  {
    "objectID": "content/06-content.html#readings",
    "href": "content/06-content.html#readings",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Readings",
    "text": "Readings\n\n Authoring in Quarto - an intro to Quarto for developing different kinds of documents. Lots of other resources linked here!!\n Pseudocode: what it is and how to write it - A nice blogpost by Sara Metawalli the sketches out the logic of pseudocode and why it can be helpful.\n The Whole Game - from Wickham et al., R for Data Science (Wickham and Grolemund 2016). Focus on the sections that begin with “Workflow” to get a sense for how we’ll start putting the pieces together.\n Scripts, algorithms, and functions - chapter 11 in in Lovelace et al., Geocomputation with R (Lovelace et al. 2019) introduces some concepts behind geospatial programming. A few of these pieces will make more sense in the next few weeks, but the general advice on constructing code and planning analyses is useful now."
  },
  {
    "objectID": "content/06-content.html#objectives",
    "href": "content/06-content.html#objectives",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDevelop basic docs with Quarto\nUnderstand the basics of creating readable code\nUse pseudocode to sketch out a computational problem"
  },
  {
    "objectID": "content/06-content.html#slides",
    "href": "content/06-content.html#slides",
    "title": "Literate Programming, Quarto, and Workflows",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Introduction to Spatial Data",
    "section": "",
    "text": "Now that you have a little more background in the breadth of philosophies, methods, and questions that geography encompasses, it’s time to start familiarizing yourself with the nature of spatial data. This lecture will be a little more conceptual, but is designed to help you make some sense for how R thinks about spatial data (or at least how the package developers have been thinking about it)."
  },
  {
    "objectID": "content/03-content.html#readings",
    "href": "content/03-content.html#readings",
    "title": "Introduction to Spatial Data",
    "section": "Readings",
    "text": "Readings\n Types of Spatial Data from Paula Moraga’s new book Spatial Statistics for Data Science: Theory and Practice with R provides a nice overview of the types of spatial data from the perspective of a statistical analyst.\n Attributes and Support from Pebesma and Bivand’s Spatial Data Science with Applications in R gives more info and examples on the nature of the relationship between geometries and support.\n Scale and Projections from Mapping, Society, and Technology by Laura Matson and Melinda Kernik gives a nice overview of the challenges associated with representing location on Earth’s surface."
  },
  {
    "objectID": "content/03-content.html#objectives",
    "href": "content/03-content.html#objectives",
    "title": "Introduction to Spatial Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nContrast the different “views” of spatial data and their incorporation in GIS.\nIdentify key elements that make data “spatial”.\nArticulate the importance of coordinate reference systems.\nRecognize the relationship between geometries and support."
  },
  {
    "objectID": "content/03-content.html#slides",
    "href": "content/03-content.html#slides",
    "title": "Introduction to Spatial Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Getting Started",
    "section": "",
    "text": "Today we’ll focus on getting oriented to the course and the tools we’ll be using throughout the semester. Readings are designed to help understand some of the ‘rules’ of R syntax and develop an understanding for manipulating different types of data in R. I’ve added a few on open science and reproducibility because I think they help make the case for learning to build code-based workflows."
  },
  {
    "objectID": "content/01-content.html#readings",
    "href": "content/01-content.html#readings",
    "title": "Getting Started",
    "section": "Readings",
    "text": "Readings\n\nThe syllabus, content, examples, and assignments pages for this class\n Chapter 1 - 6 in Venables et al., An Introduction to R (Venables et al. 2009) - for a quick refresher on data types in R (it’s only 30 pages)\n Chapters 1-2 in Douglas et al., An Introduction to R - provides another intro to R that’s been updated and is an open-source book.\n Happy Git and GitHub for the useR - all you really need to know to be a proficient user of git for version control and reproducible workflows.\n Open science, reproducibility, and transparency in ecology by Powers and Hampton - discusses the importance of open science for ecologists.\n Practical Reproducibility in Geography and Geosciences by Nilst and Pebesma - describes the importance of reproducibility for geospatial analysis."
  },
  {
    "objectID": "content/01-content.html#objectives",
    "href": "content/01-content.html#objectives",
    "title": "Getting Started",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should:\n\nBe able to articulate the organization of the course, the approach to grading, and the requirements for the final project\nBe able to access the RStudio Server and Github classroom\nBe able to clone the first self-reflection and know the process for submitting assignments"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Getting Started",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "assignment/self-eval2.html",
    "href": "assignment/self-eval2.html",
    "title": "Self-reflection 2",
    "section": "",
    "text": "This is the second of three self-reflections that you’ll complete during the course. We’ll revisit your objectives and check-in on what I can do to help you get the most out of the second half of the course. This is our mid-semester “adjustment”. As such, I’m asking that you complete this in a timely fashion and turn it in by Oct 21. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval2.html#instructions",
    "href": "assignment/self-eval2.html#instructions",
    "title": "Self-reflection 2",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-2-xxx.qmd and give it a title (like M Williamson Self-Reflection 2). Make sure that you select the html output option.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-2-xx.qmd, and self-reflection-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Assignments",
    "section": "",
    "text": "The main goals of this class is class is to get you comfortable with the manipulation, analysis, and visualization of spatial data using the R computing environment. These assignments are designed to help you practice those skills and reflect on your progress. All of the assignments have their own repository in our GitHub classroom, so you’ll submit them there. I’ll transfer grades over to Canvas so you’ll have an idea where you’re at in the course, but we won’t use Canvas for much else."
  },
  {
    "objectID": "assignment/index.html#self-reflections",
    "href": "assignment/index.html#self-reflections",
    "title": "Assignments",
    "section": "Self-reflections",
    "text": "Self-reflections\nThis course is collaborative. I’m hoping to provide a broad suite of information that can help you analyze spatial data for your graduate research and beyond. That said, you know more than I do about your personal and professional objectives. During the course of the semester, I’ll ask you to submit 2 self-reflections. The first should help me get to know you and get us on the same page with respect to your goals for the course. The last provides you with an opportunity to reflect on your progress in the course relative to your own objectives and give me feedback on how I can improve the course. These self-reflections are the foundation of how you’ll be ‘graded’ in this course, so they are mandatory and need to be submitted by the due date"
  },
  {
    "objectID": "assignment/index.html#problem-sets",
    "href": "assignment/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem sets",
    "text": "Problem sets\nI’ve created ten assignments to practice outlining a workflow, writing R code, and troubleshooting errors. The assignments rely on these datatasetsare designed to provide “bite-sized” practice to help cement the topics covered in the (generally 2) lectures that each applies. That said, they may take some and I’d encourage you to start on them early so that we have time to work on any questions you have. Moreover, long coding days are the worst so working in bite size chunks can help keep things fun! Finally, working incrementally will also to encourage you to get in the habit of using git to keep track of your progress.\nI’ll be grading these according to: * Please Resubmit: This indicates that either your code does not run as written (i.e., your quarto document will not compile on my computer), you did not use Git as instructed, and/or that your responses to the questions I posed indicate that you do not quite understand the material as well as I would like. You’ll need to schedule an appointment to talk with me and we’ll work out what you need to do to get credit for the assignment. Although there isn’t a hard deadline for this resubmission, the assignments build on each other so it’s in your best interest to complete the resubmission before the next assignment. Failure to resubmit will result in no credit for the assignment.\n\nResubmit If You Like: This indicates that all of the code works as written and that you used Git, but that you may have missed some important concepts. Your are welcome to resubmit the assignment and address my comments to help polish the final product, but it is not required for you to get credit for the assignment.\nGood To Go: All of your code works, you completed the necessary Git steps, and all of the pieces are there and polished. I may have some minor comments, but I don’t need you to address them for this assignment.\n\n\n\n\n\n\n\nTip\n\n\n\nLate Work: There is no such thing as ‘late work’ with these assignments. Life happens, sometimes things take longer to finish than you expect. If you turn it in, I’ll give you feedback. That said, the assignments build on each other so it’s probably best to avoid falling too far behind."
  },
  {
    "objectID": "assignment/index.html#final-project",
    "href": "assignment/index.html#final-project",
    "title": "Assignments",
    "section": "Final project",
    "text": "Final project\nAt the end of the course, you will demonstrate your knowledge of spatial analysis workflows through a final project that requires you to integrate a variety of spatial datasets, analyze the data with respect to a question of interest, and create visuals that help you interpret the data.\nComplete details for the final project are here.\nThere is no final exam. This project is your final exam."
  },
  {
    "objectID": "assignment/data.html",
    "href": "assignment/data.html",
    "title": "Assignment Datasets",
    "section": "",
    "text": "In order to reduce the fatigue of learning new datasets for each assignment, we’ll try to keep things limited to a handful of point, vector, and raster datasets. I’m going to simplify them a bit for the sake of loading times, but you can take a look at the webpages for each dataset to get a sense for what the originals look like."
  },
  {
    "objectID": "assignment/data.html#fire-occurrence-data",
    "href": "assignment/data.html#fire-occurrence-data",
    "title": "Assignment Datasets",
    "section": "Fire Occurrence Data",
    "text": "Fire Occurrence Data\nThe US Forest Service maintains a geospatial archive of historic and ongoing fire occurrence ignition locations. It’s a massive dataset, but you can play around with a web-viewer to see the sorts of things they report."
  },
  {
    "objectID": "assignment/data.html#wildfire-risk-to-communities",
    "href": "assignment/data.html#wildfire-risk-to-communities",
    "title": "Assignment Datasets",
    "section": "Wildfire Risk to Communities",
    "text": "Wildfire Risk to Communities\nThis is a recent product that is being used to determine where priorities for fire mitigation and restoration should be located. They’ve been the tool underlying the most recent round of Community Wildfire Defense Grants."
  },
  {
    "objectID": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "href": "assignment/data.html#climate-and-economic-justice-screening-tool",
    "title": "Assignment Datasets",
    "section": "Climate and Economic Justice Screening Tool",
    "text": "Climate and Economic Justice Screening Tool\nThe CEJST combines a number of interesting datasets to identify locations where environmental justice concerns are likely to be highest. We’ll generally be using the underlying data (not the classification itself), but it’s worth checking out just to get a sense for what’s there.\nIn general, I’ll do my best to keep us using these datasets and give you a heads up and description if we need to deviate (e.g., for the networks unit)."
  },
  {
    "objectID": "assignment/12-nets.html",
    "href": "assignment/12-nets.html",
    "title": "Assignment 10: Movement and Networks",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/12-nets.html#instructions",
    "href": "assignment/12-nets.html#instructions",
    "title": "Assignment 10: Movement and Networks",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/10-secondrevision.html",
    "href": "assignment/10-secondrevision.html",
    "title": "Assignment 10: Revisiting your code (part 2)",
    "section": "",
    "text": "This is your second opportunity to reconsider your answers to the middle four assignments and evaluate what you might have done differently now that you’ve had a little more practice. I’ve also asked some specific questions based on common mistakes across the assignments. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/08-combinations.html",
    "href": "assignment/08-combinations.html",
    "title": "Assignment 7: Combining Data Types",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/08-combinations.html#instructions",
    "href": "assignment/08-combinations.html#instructions",
    "title": "Assignment 7: Combining Data Types",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/06-vectorops.html",
    "href": "assignment/06-vectorops.html",
    "title": "Assignment 6: Vector Operations",
    "section": "",
    "text": "Now that you’ve been introduced to predicates, measures, and transformers in the sf package. You should be able complete a relatively simple workflow for a spatial analysis. We’ll build on this again with raster data (using terra) next week and then integrate both data models the week after that. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/06-vectorops.html#instructions",
    "href": "assignment/06-vectorops.html#instructions",
    "title": "Assignment 6: Vector Operations",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-6-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-6-xxx.qmd and give it a title (like M Williamson Assignment 6). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-6-xx.qmd, and assignment-6-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/04-maps.html",
    "href": "assignment/04-maps.html",
    "title": "Assignment 4: Predicates and Measures",
    "section": "",
    "text": "This is the fourth assignment of the semester for HES 505.\nNow that you’ve learned a bit about predicates and measures it’s time to practice on some vector and raster data. By the end of this assignment, you should be able to:"
  },
  {
    "objectID": "assignment/04-maps.html#instructions",
    "href": "assignment/04-maps.html#instructions",
    "title": "Assignment 4: Predicates and Measures",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-4-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-4-xxx.qmd and give it a title (like M Williamson Assignment 4). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-4-xx.qmd, and assignment-4-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/04-maps.html#the-data",
    "href": "assignment/04-maps.html#the-data",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Data",
    "text": "The Data\nWe will be using the landmarks data table, the shapefile from the Climate and Economic Justice Screening Tool, and the wildfire hazard raster from the previous two assignments. I’ve moved the versions for this assignment into the opt/data/2023/assignment04/ folder to make life easier."
  },
  {
    "objectID": "assignment/04-maps.html#the-assignment",
    "href": "assignment/04-maps.html#the-assignment",
    "title": "Assignment 4: Predicates and Measures",
    "section": "The Assignment",
    "text": "The Assignment\n\nLoad the cejst_pnw.shp use the correct predicates to determine whether the geometries are valid and to check for empty geometries. If there are empty geometries, determine which rows have empty geometries (show your code).\nLoad the landmarks_ID.csv table and convert it to an sf object. Now filter to just the hospital records (MTFCC == \"K1231\") and calculate the distance between all of the hospitals in Idaho. Note that you’ll have to figure out the CRS for the landmarks dataset…\nFilter the cejst_pnw.shp to just those records from Ada County. Then filter again to return the row with the highest annual loss rate for agriculture (2 hints: you’ll need to look at the columns.csv file in the data folder to figure out which column is the expected agricultural loss rate and you’ll need to set na.rm=TRUEwhen looking for the maximum value). Calculate the area of the resulting polygon.\nFinally, look at the helpfile for the terra::adjacent command. How do you specify which cells you’d like to get the adjacency matrix for? How do you return only the cells touching your cells of interest? Use the example in the helpfile to illustrate how you’d do this on a toy dataset - this will help you learn to ask minimally reproducible examples."
  },
  {
    "objectID": "assignment/03-vector.html",
    "href": "assignment/03-vector.html",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "",
    "text": "This is the third assignment of the semester for HES 505. The last few lectures have focused on coordinates and gemoetries. In this assignment, we’ll ude the different functions for accessing and transforming the crs of different spatial objects. We’ll also use a little of the tidyverse to subset the data and access some of the geometry information for one of the observations in our dataset. You’ll need to use both the lectures and the recorded examples (or check out the tidyverse tutorials linked in the lectures). This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/03-vector.html#instructions",
    "href": "assignment/03-vector.html#instructions",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-3-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-3-xxx.qmd and give it a title (like M Williamson Assignment 3). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-3-xx.qmd, and assignment-3-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/03-vector.html#the-data",
    "href": "assignment/03-vector.html#the-data",
    "title": "Assignment 3: Coordinates and Geometries",
    "section": "The Data",
    "text": "The Data\nFor this assignment, you’ll be lookcing at 3 different datasets. One from the Center for Disease Control’s PLACES data describing the distribution of chronic health risks, one from the EPA describing exposure to PM2.5 (an important air pollutant), and one describing wildfire risk. You might imagine that as we become increasingly concerned with the evironmental justice concerns associated with fire, we might be concerned about whether more smoke increases the risk of chronic respiratory diseases. We won’t totally answer that question this week, but you’ll start to develop the workflow necessary to move towards that type of analysis. All of the data are on the server in the opt/data/2023/assignment03/ folder.\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/02-introspatial.html",
    "href": "assignment/02-introspatial.html",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "",
    "text": "This is the second assignment of the semester for HES 505.\nFor the rest of the course, I’ll be asking you to use pseudocode to plan your analysis steps before you start using any functions (or writing your own). Pseudocode allows you to think about the important steps of your process and identify your desired results before your start down the path of coding. You can think of pseudocode as an outline for syntax, much like the one you might use for writing an manuscript or report. Quarto documents are designed to let you both outline your report and plan your analysis all in the same place! This assingment is meant to give you some practice setting up your outlines before you start coding. By the end of this assignment you should be able to:"
  },
  {
    "objectID": "assignment/02-introspatial.html#instructions",
    "href": "assignment/02-introspatial.html#instructions",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-2-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-2-xxx.qmd and give it a title (like M Williamson Assignment 2). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-2-xx.qmd, and assignment-2-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/02-introspatial.html#the-assignment",
    "href": "assignment/02-introspatial.html#the-assignment",
    "title": "Assignment 2: Intro to Spatial Data",
    "section": "The Assignment",
    "text": "The Assignment\nFind a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative aaalysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n\nCreate a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\nCreate a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in clase (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\nAdd in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\nBased on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\nAdd a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose.\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/01-introsolutions.html",
    "href": "assignment/01-introsolutions.html",
    "title": "Assignment 1 Solutions: Introductory material",
    "section": "",
    "text": "How does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\n\nAlmost all of my research has to do with geographic analysis and basic geographic concepts. I suppose that’s why I teach this course. For me, one of the biggest challenges is figuring out how to a) choose a scale of analysis that matches the scale of the phenomena that I’m interested in and b) reconcile the fact that most of the data I have access to was rarely collected at that specific scale. I’m constantly looking for new methods to try and determine if and when my results are dependent upon some arbitrary choice of extent and resolution.\n\nWhat are the primary components that describe spatial data?\n\nI would say that the primary components are the coordinate reference system (because it helps us understand where we actually are on Earth), the extent of the data (because that helps me know what scale we’re working with and the size of the computational probelem), the resolution (same reason as extent), the geometry, and spatial support. I don’t think about this last one often enough, but it really is the key to honest interpretation of the spatial data that you have.\n\nWhat is the coordinate reference system and why is it important\n\nThe CRS consists of the information necessary to locate points in 2 or 3 dimensional space. Coordinates are only meaningful in the context of a CRS (i.e., (2,2) could describe any number of places in the world - we need to know the origin and the datum to actually know where that is). The CRS becomes particularly important when we need to align datasets that were not collected in the same CRS originally or when we need to transfer locations from the globe to a flat surface (e.g., map, screen, etc).\n\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\nHere’s a fun article on projections that shows what i’m talking about!\nRead in the cejst.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\nI can read in the data using st_read or read_sf\n\n```{r}\n#| message: false\nlibrary(sf)\n\ncejst.sf &lt;- read_sf(\"data/opt/data/2023/assignment01/cejst_nw.shp\")\ncejst.st &lt;- st_read(\"data/opt/data/2023/assignment01/cejst_nw.shp\")\n```\n\nReading layer `cejst_nw' from data source \n  `/Users/mattwilliamson/Websites/isdrfall23/assignment/data/opt/data/2023/assignment01/cejst_nw.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2590 features and 123 fields (with 19 geometries empty)\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7625 ymin: 41.988 xmax: -111.0435 ymax: 49.00249\nGeodetic CRS:  WGS 84\n\n\nYou can inspect the differences between the resulting object classes by calling class\n\n```{r}\n#| message: false\nlibrary(sf)\n\nclass(cejst.sf)\nclass(cejst.st)\n```\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n[1] \"sf\"         \"data.frame\"\n\n\nYou’ll notice that using st_read assigns the object to an sf and data.frame class meaning that functions defined for those two classes will work. Alternatively, read_sf assigns the object to sf, tbl_df, tbl, and data.frame classes meaning that a much broader set of functions can be run on the cejst.sf object.\nBecause the data are in wide format, we can assume that there is only 1 observation for each location (because sf requires that there is a geometry entry for every observation (even if it’s empty)). Probably the easiest way to get the number of observations is:\n\n```{r}\nnrow(cejst.sf)\n```\n\n[1] 2590\n\n\nSimilarly, if we wanted to know how many attributes are collected for each observation we could use ncol:\n\n```{r}\nncol(cejst.sf)\n```\n\n[1] 124\n\n\nNote that these are really only approximate estimates. There’s usually a lot of extra ID-style columns in spatial data such that the number of columns with useful information is less than the total number of columns, but we won’t worry about that for now."
  },
  {
    "objectID": "assignment/01-intro.html",
    "href": "assignment/01-intro.html",
    "title": "Assignment 1: Introductory material",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of geographic thought, core technical details of working with spatial data, and introduce R as a tool for end-to-end spatial workflows. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. There are additional step-by-step guides in the By the end of this assignment you should be able to:"
  },
  {
    "objectID": "assignment/01-intro.html#instructions",
    "href": "assignment/01-intro.html#instructions",
    "title": "Assignment 1: Introductory material",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/01-intro.html#questions-for-the-assignment",
    "href": "assignment/01-intro.html#questions-for-the-assignment",
    "title": "Assignment 1: Introductory material",
    "section": "Questions for the Assignment",
    "text": "Questions for the Assignment\n\nHow does geographic analysis fit into your goals for your research? Given our discussion of the aims and limitations of geographic analysis, are there particular issues that you would like to know more about or guard against?\nWhat are the primary components that describe spatial data?\nWhat is the coordinate reference system and why is it important\nFind two maps of the same area in different projections? How does the projection affect your perception of the data being displayed?\nRead in the cejst.shp file in the assignment01 folder. How many attributes describe each object? How many unique geometries are there? What is the coordinate reference system?\n\n\n\n\n\n\n\nNote\n\n\n\nSolutions are here"
  },
  {
    "objectID": "assignment/02-example.html",
    "href": "assignment/02-example.html",
    "title": "Quarto Example",
    "section": "",
    "text": "Introduction\nI want to reproduce the figure from (Dash Nelson 2016) depicting commute networks in the United States based on US Census data.\n\n\nMethods\nSome pseudocode:\n\n1. Retrieve ACS commute data\n2. Identify the source and destination networks\n3. Calculate the edge density for each source-destination pair\n4. Thin to a manageable number of nodes based on edge densities\n5. plot\n\nas code chunks\n\n```{r}\n#| eval: false\n#| label: getacs\n\n1. Retrieve ACS commute data\n```\n\n\n```{r}\n#| eval: false\n#| label: buildnet\n\n2. Identify the source and destination networks\n```\n\n\n```{r}\n#| eval: false\n#| label: edgedens\n\n3. Calculate the edge density for each source-destination pair\n```\n\n\n```{r}\n#| eval: false\n#| label: thinnodes\n\n4. Thin to a manageable number of nodes based on edge densities\n```\n\n\n```{r}\n#| eval: false\n#| label: buildplot\n\n5. Build plot\n```\n\n\n\nResults\n\n\n\nCommute Networks from Dash Nelson and Rae 2016\n\n\n\n\n\n\n\nReferences\n\nDash Nelson, A., Garrett AND Rae. 2016. An economic geography of the united states: From commutes to megaregions. PLOS ONE 11:1–23."
  },
  {
    "objectID": "assignment/02-introspatialsolutions.html",
    "href": "assignment/02-introspatialsolutions.html",
    "title": "Assignment 2 Solutions: Intro to Spatial Data",
    "section": "",
    "text": "Find a figure that you’d like to mimic with your research. The figure should be from a manuscript or report and present the results of a quantitative aaalysis (i.e., not a conceptual model or an image). Once you’ve found one you should:\n1. Create a section called “Introduction” in your Quarto document. In that section, you should give me the citation for the article and a brief description (similar to the caption) of the figure.\n\nIn order to do this, you’ll need to start a new Quarto document (File -&gt; New File -&gt; Quarto Document). Once you’ve done that Rstudio will open up a Quarto document with the yaml header already in place and a bunch of example text and code. Delete that. Then use Markdown syntax to specify headings (# for top level headings, ## for second level headings, etc.) So in this case, once you’ve gotten all of the example stuff deleted, you can use # Introduction to create a section header with the correct title. Adding citations requires you to create a separate, .bib file that lives in the same directory as your document and has your citation info in BIBTEX format. You can find more on that here.\n\n2. Create a second section called “Methods” and write out the steps necessary to create the figure. These should be similar to the pseudocode we discussed in class (e.g., “Load Data”, “Summarize by county”, “Run linear regression”, “Build Figure”). The methods section of the manuscript you’ve chosen should provide you with enough information to begin sketching this out. Don’t worry if you don’t know all of the steps, the goal is to get you thinking about the “mile markers” along the way to creating the figure.\n\nNow I’ll add # Methods as my next header and start to write out the steps I’d like the analysis to follow. There are lots of ways you might do this, depending on whether you want the pseudocode to be part of your final product. For now we’ll keep it simple and just use numbered steps like:\n\n\nLoad data\nFilter the correct rows\nSelect the right variables\nModel y as a function of x1, x2, x3…\nPlot\n\n3. Add in code blocks for each step in your pseudocode. Give each block a name that corresponds to your pseudocode steps.\n\nAdding in code blocks and giving is accomplished by setting up your code fence (```), identifying the language you want to use ({r}), and then setting code options with the hash-pipe (#|). You can see my example here\n\n4. Based on the webpage linked above and the “Execution Options” section linked there, add execution options to each block that ensure that the code block will be printed, but not evaluated.\n\nIn order to ensure that the code prints, you need to add the #| echo: true execution option. In order to prevent it from running, you want to set #| eval: false. You can see this in my example.\n\n5. Add a “Results” section and use the markdown command to include an image of the figure from the manuscript you chose. \n\nYou can add images in markdown by using the ![]() syntax where the file location is pasted in the parentheses and any caption is placed in the brackets."
  },
  {
    "objectID": "assignment/03-vectorsolutions.html",
    "href": "assignment/03-vectorsolutions.html",
    "title": "Assignment 3 Solutions: Coordinates and Geometries",
    "section": "",
    "text": "1. Write out the pseudocode that you would use to set up an analysis of the spatial correlations between chronic asthma risk, exposure to PM2.5, and wildfire. You don’t have to write functions or any actual code. Just write the steps and insert named code blocks for each step.\n\nThis one is probably a little tricky if you haven’t taken the time to check out the attributes of the data (which you should always do). That said, some pretty generic steps would be:\n\n\n1. Load each dataset\n2. Check geometry validity\n3. Align CRS\n4. Run Correlation\n5. Print Results\n\n\nThere are two key steps here, that you’ll repeat for any/all spatial analyses that you do: 1) checking for valid geometries and 2) making sure the data are aligned in a sensible CRS. I can add a code chunk for each now.\n\n\n1. Load each dataset\n\n\n2. Check geometry validity\n\n\n3. Align CRS\n\n\n4. Run Correlation\n\n\n5. Print Results\n\n2. Read in the cdc_nw.shp, pm_nw.shp, and wildfire_hazard_agg.tif files and print the coordinate reference system for each object. Do they match?\n\nHere I’m going to combine the load portion of my pseudocode with the validity since I can do that without creating additional object. I use the str() function to get a sense for what the data looks like and to understand what data classes I’m working with. Then, I use the all() function to make sure that all of the results of st_is_valid() are true. I don’t need to do that with the raster file as the geometry is implicit which means that it has to be topologically valid (this doesn’t mean that the numbers are accurate, it just means that the dataset conforms to the data model R expects). Then I’ll add another code to check the CRS of the different objects.\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\n\ncdc.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/cdc_nw.shp\")\nstr(cdc.nw)\n\nsf [2,005 × 78] (S3: sf/tbl_df/tbl/data.frame)\n $ STATEFP   : chr [1:2005] \"16\" \"16\" \"16\" \"16\" ...\n $ COUNTYFP  : chr [1:2005] \"025\" \"055\" \"055\" \"055\" ...\n $ TRACTCE   : chr [1:2005] \"970100\" \"000602\" \"000401\" \"000301\" ...\n $ GEOID     : chr [1:2005] \"16025970100\" \"16055000602\" \"16055000401\" \"16055000301\" ...\n $ NAME      : chr [1:2005] \"9701\" \"6.02\" \"4.01\" \"3.01\" ...\n $ NAMELSAD  : chr [1:2005] \"Census Tract 9701\" \"Census Tract 6.02\" \"Census Tract 4.01\" \"Census Tract 3.01\" ...\n $ MTFCC     : chr [1:2005] \"G5020\" \"G5020\" \"G5020\" \"G5020\" ...\n $ FUNCSTAT  : chr [1:2005] \"S\" \"S\" \"S\" \"S\" ...\n $ ALAND     : num [1:2005] 2.78e+09 5.54e+06 1.04e+08 9.33e+07 1.05e+07 ...\n $ AWATER    : num [1:2005] 11548054 488272 1356762 2372161 1149 ...\n $ INTPTLAT  : chr [1:2005] \"+43.5025515\" \"+47.7056992\" \"+47.6514553\" \"+47.7954122\" ...\n $ INTPTLON  : chr [1:2005] \"-114.7721349\" \"-116.9179618\" \"-117.0032478\" \"-116.9909923\" ...\n $ stateabbr : chr [1:2005] \"ID\" \"ID\" \"ID\" \"ID\" ...\n $ statedesc : chr [1:2005] \"Idaho\" \"Idaho\" \"Idaho\" \"Idaho\" ...\n $ countyname: chr [1:2005] \"Camas\" \"Kootenai\" \"Kootenai\" \"Kootenai\" ...\n $ countyfips: chr [1:2005] \"16025\" \"16055\" \"16055\" \"16055\" ...\n $ totalpopul: chr [1:2005] \"1117\" \"5727\" \"5834\" \"5316\" ...\n $ access2_cr: num [1:2005] 16.5 16.2 16.6 14.1 22.7 20 24.8 12.2 28.6 24.1 ...\n $ access2__2: chr [1:2005] \"(14.1, 18.8)\" \"(13.7, 19.3)\" \"(13.8, 19.6)\" \"(12.3, 16.1)\" ...\n $ arthritis_: num [1:2005] 27.2 26.3 24 25.4 26.9 21.9 22 28 22.6 22.8 ...\n $ arthriti_2: chr [1:2005] \"(25.9, 28.4)\" \"(25.1, 27.6)\" \"(22.8, 25.2)\" \"(24.5, 26.4)\" ...\n $ binge_crud: num [1:2005] 14.2 14.9 15.8 15.4 13 15 14.9 12.1 15 15.1 ...\n $ binge_cr_2: chr [1:2005] \"(13.8, 14.6)\" \"(14.2, 15.6)\" \"(15.3, 16.3)\" \"(15.0, 15.8)\" ...\n $ bphigh_cru: num [1:2005] 35 32 29.9 30.9 34.5 28.8 30.7 35.1 29.8 29.1 ...\n $ bphigh_c_2: chr [1:2005] \"(34.0, 36.0)\" \"(30.8, 33.2)\" \"(29.0, 30.8)\" \"(30.2, 31.6)\" ...\n $ bpmed_crud: num [1:2005] 72.9 71.4 69 71.1 74.1 69.6 66.6 77.4 68.5 69.2 ...\n $ bpmed_cr_2: chr [1:2005] \"(72.0, 73.9)\" \"(70.1, 72.7)\" \"(67.9, 70.0)\" \"(70.3, 71.9)\" ...\n $ cancer_cru: num [1:2005] 7 6.5 6 6.9 7.3 5.7 4.7 9.1 5.4 5.8 ...\n $ cancer_c_2: chr [1:2005] \"( 6.7,  7.4)\" \"( 6.2,  6.9)\" \"( 5.7,  6.3)\" \"( 6.7,  7.2)\" ...\n $ casthma_cr: num [1:2005] 9.9 10.4 10 9.5 10.2 10.4 11.1 9 10 10.1 ...\n $ casthma__2: chr [1:2005] \"( 9.4, 10.4)\" \"( 9.9, 11.0)\" \"( 9.5, 10.6)\" \"( 9.1,  9.9)\" ...\n $ cervical_c: num [1:2005] 78.9 75.7 77 79.5 77.8 75.1 71.7 82.3 75.6 76.6 ...\n $ cervical_2: chr [1:2005] \"(76.9, 80.7)\" \"(72.8, 78.8)\" \"(75.0, 79.3)\" \"(78.0, 81.1)\" ...\n $ chd_crudep: num [1:2005] 7.7 7.1 6.2 6.4 8 6.1 6.4 7.5 6.8 6.6 ...\n $ chd_crude9: chr [1:2005] \"( 7.1,  8.4)\" \"( 6.5,  7.7)\" \"( 5.7,  6.7)\" \"( 5.9,  6.8)\" ...\n $ checkup_cr: num [1:2005] 70.2 70.8 69.4 70.9 70.9 68.7 66.7 74.6 65.4 66.4 ...\n $ checkup__2: chr [1:2005] \"(69.0, 71.4)\" \"(69.7, 71.9)\" \"(68.3, 70.6)\" \"(70.1, 71.7)\" ...\n $ cholscreen: num [1:2005] 82.3 79.3 79.9 83.3 81.4 76.1 72.4 87.1 76.2 76.4 ...\n $ cholscre_2: chr [1:2005] \"(81.9, 82.7)\" \"(79.0, 79.7)\" \"(79.8, 80.0)\" \"(83.1, 83.5)\" ...\n $ colon_scre: num [1:2005] 63 66.9 66.5 70.1 63.8 63.7 56.8 69.6 58.1 60.1 ...\n $ colon_sc_2: chr [1:2005] \"(60.7, 65.6)\" \"(63.7, 70.1)\" \"(64.0, 69.0)\" \"(68.3, 71.7)\" ...\n $ copd_crude: num [1:2005] 8.1 7.8 6.8 6.4 8.4 6.8 8.5 6.5 7.5 7.3 ...\n $ copd_cru_2: chr [1:2005] \"( 7.2,  9.2)\" \"( 6.8,  8.9)\" \"( 5.9,  7.8)\" \"( 5.7,  7.0)\" ...\n $ corem_crud: num [1:2005] 33.5 35.7 36.7 39.4 43.5 43.7 37.4 50.2 32.6 34.6 ...\n $ corem_cr_2: chr [1:2005] \"(27.2, 40.2)\" \"(29.0, 42.9)\" \"(31.3, 42.5)\" \"(35.0, 43.6)\" ...\n $ corew_crud: num [1:2005] 34.8 34.8 36.5 38.4 34.8 35.8 31.6 39.3 28.7 28.7 ...\n $ corew_cr_2: chr [1:2005] \"(29.5, 40.4)\" \"(28.8, 41.4)\" \"(31.6, 41.6)\" \"(34.5, 42.2)\" ...\n $ csmoking_c: num [1:2005] 19.5 18.9 18.6 16.3 18.4 17.6 24.8 12.3 20.5 19 ...\n $ csmoking_2: chr [1:2005] \"(17.6, 21.5)\" \"(16.1, 21.7)\" \"(16.5, 20.7)\" \"(14.9, 17.7)\" ...\n $ dental_cru: num [1:2005] 59.3 60.3 62.3 67.3 59.4 60.6 49.9 72 53.9 56.9 ...\n $ dental_c_2: chr [1:2005] \"(56.0, 62.7)\" \"(56.6, 63.9)\" \"(58.3, 65.8)\" \"(64.9, 69.5)\" ...\n $ depression: num [1:2005] 19.8 21.3 20.8 19.8 21.2 22.2 23.6 18.6 19.7 20.1 ...\n $ depressi_2: chr [1:2005] \"(18.8, 20.9)\" \"(20.1, 22.5)\" \"(19.5, 22.1)\" \"(19.0, 20.6)\" ...\n $ diabetes_c: num [1:2005] 11.7 10.6 9.5 9.6 11.8 9.2 10.6 10.3 11.1 10.3 ...\n $ diabetes_2: chr [1:2005] \"(10.9, 12.4)\" \"( 9.8, 11.4)\" \"( 8.8, 10.2)\" \"( 9.0, 10.1)\" ...\n $ ghlth_crud: num [1:2005] 16.3 15.8 14.1 12.6 17.5 14.5 19.6 11.8 19.4 17.5 ...\n $ ghlth_cr_2: chr [1:2005] \"(14.4, 18.3)\" \"(13.9, 18.0)\" \"(12.4, 16.3)\" \"(11.3, 13.8)\" ...\n $ highchol_c: num [1:2005] 33.4 30 29 30.7 31.9 27.5 27.7 33.4 29 28.4 ...\n $ highchol_2: chr [1:2005] \"(32.6, 34.3)\" \"(29.0, 31.0)\" \"(28.2, 29.7)\" \"(30.1, 31.3)\" ...\n $ kidney_cru: num [1:2005] 3.3 3.1 2.7 2.7 3.6 2.9 3.1 3.3 3.1 3.1 ...\n $ kidney_c_2: chr [1:2005] \"( 3.1,  3.5)\" \"( 2.9,  3.4)\" \"( 2.5,  2.9)\" \"( 2.6,  2.9)\" ...\n $ lpa_crudep: num [1:2005] 25 25 23.4 21.3 25.9 22.8 28.3 19.1 30 27.6 ...\n $ lpa_crude9: chr [1:2005] \"(23.2, 26.9)\" \"(22.3, 27.5)\" \"(21.5, 25.3)\" \"(19.9, 22.6)\" ...\n $ mammouse_c: num [1:2005] 68.8 70.5 70.4 71.3 68.7 69.5 68.7 71.9 67.3 68.7 ...\n $ mammouse_2: chr [1:2005] \"(64.4, 73.1)\" \"(66.4, 74.5)\" \"(66.1, 74.7)\" \"(68.3, 74.1)\" ...\n $ mhlth_crud: num [1:2005] 13.8 15.2 14.8 13.5 14 15.2 17 11 15.2 15.2 ...\n $ mhlth_cr_2: chr [1:2005] \"(12.9, 14.6)\" \"(14.2, 16.3)\" \"(13.7, 15.9)\" \"(12.8, 14.2)\" ...\n $ obesity_cr: num [1:2005] 36.4 32.8 31.8 30.7 34.3 32.3 36.5 29.4 36.6 35 ...\n $ obesity__2: chr [1:2005] \"(34.6, 38.1)\" \"(31.4, 34.2)\" \"(30.4, 33.3)\" \"(29.7, 31.7)\" ...\n $ phlth_crud: num [1:2005] 12.7 12.7 11.5 10.8 12.9 11 13.7 10.1 13.2 12.4 ...\n $ phlth_cr_2: chr [1:2005] \"(11.6, 13.9)\" \"(11.5, 14.0)\" \"(10.4, 12.8)\" \"(10.0, 11.6)\" ...\n $ sleep_crud: num [1:2005] 33.5 32.5 32.4 31.4 32.4 32.5 35.4 28.9 32.3 31.6 ...\n $ sleep_cr_2: chr [1:2005] \"(32.0, 34.9)\" \"(31.0, 33.9)\" \"(30.8, 33.9)\" \"(30.3, 32.4)\" ...\n $ stroke_cru: num [1:2005] 3.6 3.4 2.9 2.8 3.9 3 3.5 3.4 3.4 3.3 ...\n $ stroke_c_2: chr [1:2005] \"( 3.2,  4.0)\" \"( 3.0,  3.8)\" \"( 2.6,  3.2)\" \"( 2.6,  3.1)\" ...\n $ teethlost_: num [1:2005] 16.1 17 15.7 12.5 18.4 16.4 23.6 10.3 22.4 20.7 ...\n $ teethlos_2: chr [1:2005] \"(12.4, 20.3)\" \"(12.1, 23.0)\" \"(11.9, 20.5)\" \"(10.1, 15.0)\" ...\n $ geometry  :sfc_MULTIPOLYGON of length 2005; first list element: List of 1\n  ..$ :List of 1\n  .. ..$ : num [1:2521, 1:2] -115 -115 -115 -115 -115 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:77] \"STATEFP\" \"COUNTYFP\" \"TRACTCE\" \"GEOID\" ...\n\nall(st_is_valid(cdc.nw))\n\n[1] TRUE\n\npm.nw &lt;- read_sf(\"data/opt/data/2023/assignment03/pm_nw.shp\")\nstr(pm.nw)\n\nsf [3,241 × 4] (S3: sf/tbl_df/tbl/data.frame)\n $ STATE_NAME: chr [1:3241] \"Idaho\" \"Idaho\" \"Idaho\" \"Idaho\" ...\n $ CNTY_NAME : chr [1:3241] \"Ada\" \"Ada\" \"Ada\" \"Ada\" ...\n $ PM25      : num [1:3241] 7.85 7.85 7.56 7.67 7.94 ...\n $ geometry  :sfc_MULTIPOLYGON of length 3241; first list element: List of 1\n  ..$ :List of 1\n  .. ..$ : num [1:88, 1:2] -12938469 -12938370 -12938267 -12938049 -12937973 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA\n  ..- attr(*, \"names\")= chr [1:3] \"STATE_NAME\" \"CNTY_NAME\" \"PM25\"\n\nall(st_is_valid(pm.nw))\n\n[1] TRUE\n\nwildfire.haz &lt;- rast(\"data/opt/data/2023/assignment03/wildfire_hazard_agg.tif\")\nstr(wildfire.haz)\n\nS4 class 'SpatRaster' [package \"terra\"]\n\n\n\nNow that I’ve gotten the data into my environment, I need to make sure that the CRS are alligned. I’ll demonstrate that with a few different approaches. You can use the logical == or the identical function to check, but remember that these fnctions are not specific to spatial objects, they evaluate things very literally. So even if the CRS is the same, if st_crs returns the CRS in one format (WKT) and crs returns it in another, you’ll get FALSE even if they are actually the same CRS - pay attention to that. You’ll notice that they aren’t identical; we’ll deal with that in the next question.\n\n\nst_crs(cdc.nw)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\nst_crs(pm.nw)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\ncrs(wildfire.haz)\n\n[1] \"PROJCRS[\\\"unnamed\\\",\\n    BASEGEOGCRS[\\\"NAD83\\\",\\n        DATUM[\\\"North American Datum 1983\\\",\\n            ELLIPSOID[\\\"GRS 1980\\\",6378137,298.257222101004,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4269]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\nidentical(st_crs(cdc.nw), st_crs(pm.nw))\n\n[1] FALSE\n\nst_crs(cdc.nw) == st_crs(pm.nw)\n\n[1] FALSE\n\n\n3. Re-project the cdc_nw.shp and pm_nw.shp shapefiles so that they have the same CRS as the wildfire_hazard_agg.tfi file. Verify that all the files have the same projection.\n\nNow we’ll use st_transform to get the two shapefiles aligned with the raster (because we generally want to avoid projecting rasters if we can). We can then use the same steps above to see if they’re aligned. Note that I’m using the terra::crs() function to make sure that the output is printed in exactly the same format\n\n\ncdc.nw.proj &lt;- cdc.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\npm.nw.proj &lt;- pm.nw %&gt;% st_transform(., crs=crs(wildfire.haz))\n\nidentical(crs(cdc.nw.proj), crs(wildfire.haz))\n\n[1] TRUE\n\nidentical(crs(pm.nw.proj), crs(wildfire.haz))\n\n[1] TRUE\n\n\n4. How does reprojecting change the coordinates of the bounding box for the two shapefiles? Show your code\n\nNow we just want to look at the bounding box of the data before and after it was projected. We can do this using st_bbox. One of the most obvious changes is that the units for cdc.nw have changed from degrees to meters (as evidenced by the much larger numbers). For the pm.nw object we can see that the raw coordinates indicate a shift to the west; however, because the origin for this crs has also changed, the states still show up in the correct place.\n\n\nst_bbox(cdc.nw)\n\n      xmin       ymin       xmax       ymax \n-124.74918   41.98818 -111.04349   49.00232 \n\nst_bbox(cdc.nw.proj)\n\n    xmin     ymin     xmax     ymax \n-2295337  2208890 -1189292  3177425 \n\nst_bbox(pm.nw)\n\n     xmin      ymin      xmax      ymax \n-13898126   5159210 -12361307   6275276 \n\nst_bbox(pm.nw.proj)\n\n    xmin     ymin     xmax     ymax \n-2300791  2208891 -1189293  3177426 \n\n\n5. What class of geometry does the pm_nw.shp have (show your code)? Now filter the pm_nw.shp file so that only the records from Ada County, Idaho are showing. Find the record with the lowest value for PM25. How many coordinates are associated with that geometry?\n\nThis one was probably a little tricky. First, to check the geometry type, we use st_geometry_type setting by_geometry to FALSE means we get the geometry type for the entire object instead of each observation. We then use a series of filter commands to get the records from Idaho and Ada county. Once we’ve narrowed the data to our correct region, we can filter again to find the row with the minimum value of PM25 (note that we have to set na.rm=TRUE so that we ignore the NA values). Then we just take the number of rows (nrow) of the result of st_coordinates to get the number of coordinates associated with that geometry.\n\n\nst_geometry_type(pm.nw, by_geometry = FALSE)\n\n[1] MULTIPOLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nada.pm &lt;- pm.nw %&gt;% \n  filter(STATE_NAME==\"Idaho\" & CNTY_NAME==\"Ada\") %&gt;% \n  filter(PM25 == min(PM25, na.rm = TRUE))\n\nada.pm\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -12935300 ymin: 5329192 xmax: -12910260 ymax: 5402433\nProjected CRS: WGS 84 / Pseudo-Mercator\n# A tibble: 1 × 4\n  STATE_NAME CNTY_NAME  PM25                                            geometry\n* &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;                                  &lt;MULTIPOLYGON [m]&gt;\n1 Idaho      Ada        6.68 (((-12935301 5391002, -12934885 5391290, -12934526…\n\nnrow(st_coordinates(ada.pm))\n\n[1] 1394"
  },
  {
    "objectID": "assignment/05-firstrevision.html",
    "href": "assignment/05-firstrevision.html",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "",
    "text": "Now that you’ve had some practice with R and the format of the course, it’s time to pause and take a moment to check in on what you’ve learned. Because we haven’t had a ton of coding yet, this review is a little more conceptual (rather than focusing on particular pieces you may have done incorrectly or inefficiently). My soluionts (or suggestions) for how I’d approach the first four assignments are posted (at the end of each assignment page). Your task here is to review those solutions and your own code and answer a few questions to demonstrate what you’ve learned so far and where I need to be more clear. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/05-firstrevision.html#instructions",
    "href": "assignment/05-firstrevision.html#instructions",
    "title": "Assignment 5: Revisiting your code (part 1)",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-revision-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file ar1_xxx.qmd and give it a title (like M Williamson Assignment Revision 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions in the assignment into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 3 commits is part of the assignment).\nRender the document (by clicking the “Render” button in RStudio) to html (you should now have at least 3 files in the repository: Readme.md, ar1_xx.qmd, and ar1_xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/07-rasterops.html",
    "href": "assignment/07-rasterops.html",
    "title": "Assignment 7: RBuilding spatial databases",
    "section": "",
    "text": "It’s time to put together the various vector, raster, and tabular code we’ve learned in order to build a dataframe that you can use for a subsequent statistical analysis. For this assignment we’ll be using the National Forest boundaries that we used in class (I’ve written a function for you to download and unzip this in the /code/ folder), a dataset describing the cost of natural disasters fromm 1999-2020 (described in this paper and available at /assignment07/ics209-plus-wf_incidents_1999to2020.csv folder. a land use dataset (/assignment07/land_use_pnw.tif), and the wildfire hazard dataset (/assignment07/wildfire_hazard_agg.tif). Your goal is to create a dataframe that includes the total cost of all disasters that have occurred within a National Forest Boundary along with several social and ecological variables that might help explain the difference in dollars expended to contain the hazard (typically fire). By the end of this assignment you should be able to:"
  },
  {
    "objectID": "assignment/07-rasterops.html#instructions",
    "href": "assignment/07-rasterops.html#instructions",
    "title": "Assignment 7: RBuilding spatial databases",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-7-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-7-xxx.qmd and give it a title (like M Williamson Assignment 7). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 3 more times (having 4 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-7-xx.qmd, and assignment-7-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/07-rasterops.html#the-assignment",
    "href": "assignment/07-rasterops.html#the-assignment",
    "title": "Assignment 7: RBuilding spatial databases",
    "section": "The Assignment",
    "text": "The Assignment\n\nYou’ll need to download the FS boundary shapefiles then load in all of the different spatial and tabular datasets.\nValidate your geometries and make sure all of your data is in the same CRS.\nSmooth the wildfire hazard and land use datasets using a 5s5 moving window; use the mean for the continuos dataset and the mode for the categorical dataset.\nEstimate the total cost of the incidents within each forest (PROJECTED_FINAL_IM_COST contains this value for each incident).\nNext join 3 attributes of your choosing from the CEJST and the extracted fire and land cover values to your dataframe.\nMake a set of maps that shows the Forest-level values for all of your selected variables."
  },
  {
    "objectID": "assignment/09-pointpatterns.html",
    "href": "assignment/09-pointpatterns.html",
    "title": "Assignment 8: Point Patterns and Interpolation",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/09-pointpatterns.html#instructions",
    "href": "assignment/09-pointpatterns.html#instructions",
    "title": "Assignment 8: Point Patterns and Interpolation",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/11-statmod.html",
    "href": "assignment/11-statmod.html",
    "title": "Assignment 9: Statistical Analyses",
    "section": "",
    "text": "The first part of the course was designed to introduce some of the foundations of working in R, developing programming workflows, and getting help. These topics are important for building robust spatial workflows, but their utility extends beyond that to most things you’ll do in R during the course of your graduate research. This homework is meant to help reinforce those concepts and identify any gaps that I need to fill in as we go. Make sure to check out the example too!. By the end of this assignment you should be able to:\nYou’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/11-statmod.html#instructions",
    "href": "assignment/11-statmod.html#instructions",
    "title": "Assignment 9: Statistical Analyses",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named assignment-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file assignment-1-xxx.qmd and give it a title (like M Williamson Assignment 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, assignment-1-xx.qmd, and assignment-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/13-thirdrevision.html",
    "href": "assignment/13-thirdrevision.html",
    "title": "Assignment 13: Revisiting your code (part 3)",
    "section": "",
    "text": "This is your final opportunity to reconsider your answers to the last few assignments and evaluate what you might have done differently now that you’ve had a little more practice. I’ve also asked some specific questions based on common mistakes across the assignments. You’ll still be using Quarto to complete This homework."
  },
  {
    "objectID": "assignment/final-proj.html",
    "href": "assignment/final-proj.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation technques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results"
  },
  {
    "objectID": "assignment/final-proj.html#overview",
    "href": "assignment/final-proj.html#overview",
    "title": "Final Project",
    "section": "",
    "text": "The final project is an opportunity to bring all of the things you’ve learned in the course into a single reproducible workflow to answer a question of your choosing. Because each of you are in different stages of collecting your own data and because confronting datasets that aren’t yours can help clarify important concepts and design elements, I’m asking you to develop an analysis of data that isn’t yours. This final project should help you demonstrate:\n\nProper data management, cleaning, and manipulation technques\nThe ability to summarize spatial data and apply different statistical analyses to it\nThe ability to evaluate the performance of statistical models\nThe ability to generate visualizations that support your analyses\nThe ability to integrate code, analysis, and visualization with text descrbing your approach and discussing your results"
  },
  {
    "objectID": "assignment/final-proj.html#requirements",
    "href": "assignment/final-proj.html#requirements",
    "title": "Final Project",
    "section": "Requirements",
    "text": "Requirements\nDatasets. The ability to manipulate and integrate a variety of data types, resolutions, and formats is a key component of this course. Your analysis should incorporate at least 5 datasets. The ultimate compostion of your database is up to you, but I’d like you to include 1 tabular dataset, 1 vector dataset, and 1 raster dataset. You should choose the other 2 (or more) to give you practice with the data types that are most relevant to your objectives and/or research.\nAnalyses. You’ve learned several classes of analyses (e.g., overlays, point-pattern, multivariate regression, and statistical learning). Apply at least one (preferably the one most tied to your own objectives and research) of these analyses to address your question. In the course of doing so, you’ll need to justify your choice, assess whether your data meets appropriate assumptions, and evaluate the implications of key assumptions you make. For example, if you’re conducting an overlay analysis, how does your choice of threshold affect the ultimate result? If you’ve fit a statistical model based on summary statistics (e.g, mean, median), how well does the model fit? How does the model change if you use different slices of the data?\nVisualizations. You should produce a minimum of 3 visualizations to accompany your analysis. One of these should be a publication quality location map. The others are up to you, but should a) help you tell the story of your analysis and b) help you meet your objectives for the course and your own research. These can be additional maps of results, figures that summarize your data or results in non-spatial ways, or interactive graphics that allow you to explore parts of your analysis.\nReporting You can generate a ‘manuscript’ style document (using Quarto) or a flexdashboard (using Quarto and shiny) as the final product. Your report should include:\n\nA brief (1-2 paragraphs) description of your question and why you’re interested in it.\nA Methods section with subsections describing the data sources, any processing steps you took and why, and your process for the analysis. Show your code and provide annotation to describe what you are attempting do with the various steps.\nA Results section that includes tabular results as well as any relevant visualizations that describe your data and analysis.\nA Discussion of your results that puts your results in the context of your question, considers alternative analysis strategies and why they may or may not be better than the approach you chose, describes additional data that might be important for your question, and considers the role of extent and resolution in your analysis."
  },
  {
    "objectID": "assignment/final-proj.html#assessment",
    "href": "assignment/final-proj.html#assessment",
    "title": "Final Project",
    "section": "Assessment",
    "text": "Assessment\nYou’ll submit a draft of the final report on December 7. I’ll give you feedback based on your project and on your objectives for the course. You’ll then have a chance to address my feedback before turning in your final draft on December 14. Your final self-assessment will ask you to reflect on your objectives for the course and evaluate the degree to which your final project demonstrates that you achieved your objectives. Thus, when you are designing your project, make sure that you have your initial objectives in mind.\n\n\n\n\n\n\nTip\n\n\n\nA note on grades: You will be responsible for assessing how well your assignment demonstrates that you achieved your objectives. I reserve the right to change the grade you’ve given yourself, but will provide clear justification for why I’m doing that. Without a completed self-assessment there is no grade for your final project, so please make sure you complete it."
  },
  {
    "objectID": "assignment/self-eval1.html",
    "href": "assignment/self-eval1.html",
    "title": "Self-reflection 1",
    "section": "",
    "text": "This is the first of three self-reflections that you’ll complete during the course. These provide an opportunity for me to learn more about you, check-in on how the course is going, and make sure that getting what you need from the course materials. This first reflection also helps us set the standards for your assessment in the course. As such, I’m asking that you complete this in a timely fashion and turn it in by Aug. 24. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval1.html#instructions",
    "href": "assignment/self-eval1.html#instructions",
    "title": "Self-reflection 1",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 2 more times (having 3 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "assignment/self-eval3.html",
    "href": "assignment/self-eval3.html",
    "title": "Self-reflection 3",
    "section": "",
    "text": "This is the final self-reflection for the course and provides a way of evaluating your final project relative to your course learning objectives. This final self-reflection is critical for assigning your grades on the final project and the course, in general. As such, I’m asking that you complete this in a timely fashion and turn it in by Dec. 16. You’ll need to accept the link to access the questions."
  },
  {
    "objectID": "assignment/self-eval3.html#instructions",
    "href": "assignment/self-eval3.html#instructions",
    "title": "Self-reflection 3",
    "section": "Instructions",
    "text": "Instructions\n\nAfter you’ve joined the assignment repository, you should have this file (named Readme.md) inside of a R project named self-reflection-1-xx where xx is your github username (or initials).\nOnce you’ve verified that you’ve correctly cloned the assignment repository, create a new Quarto document. Name this file self-reflection-1-xxx.qmd and give it a title (like M Williamson Self-Reflection 1). Make sure that you select the html output option (Quarto can do a lot of cool things, but the html format is the least-likely to cause you additional headaches). We’ll be using Quarto throughout the course so it’s worth checking out the other tutorials in the getting started section.\nCopy the questions below into your document and change the color of their text.\nSave the changes and make your first commit!\nAnswer the questions making sure save and commit at least 4 more times (having 5 commits is part of the assignment).\nRender the document to html (you should now have at least 3 files in the repository: Readme.md, self-reflection-1-xx.qmd, and self-reflection-1-xx.html). Commit these changes and push them to the repository on GitHub. You should see the files there when you go to github.com."
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Why Geographic Analysis",
    "section": "",
    "text": "Before we get inundated with technical details and syntax, I think it’s important to remind ourselves why we need geographical analysis. We’ll spend some time on the various conceptualizations of place and space, how those things show up in geographic data, and their implications for the kinds of science we can do when we’re using geographic data. This session is meant to provide a bit of philosophical foundation for you to keep in mind as you work through various parts of the analytic pipeline."
  },
  {
    "objectID": "content/02-content.html#readings",
    "href": "content/02-content.html#readings",
    "title": "Why Geographic Analysis",
    "section": "Readings",
    "text": "Readings\nThe following readings are intended to give you some sense of the discussion surrounding the role of spatial data in understanding the world. They are a mix of old favorites and relatively recent reviews. You don’t need to read all of them or memorize them, but they are worth a skim. I bet you’ll find something interesting.\n Conservation biogeography: assessment and prospects by Whitaker et al. (2005) provides an overview of the of geography in understanding ecosystem function and shaping conservation strategies.\n Economic geography, politics, and policy by Rickard (2020) provides a review of the role of geography in understanding responses to globalization.\n Revolutionary and counter revolutionary theory in geography and the problem of ghetto formation by David Harvey (1972) offers a scathing critique of quantitative geography (though Harvey was one of the founders of the field). See (Barnes 2009) for a relatively recent attempt to reconcile these views.\n Does scale exist? An epistemological scale continuum for complex human–environment systems by Steven Manson (2008) is one of my favorite summaries of the various definitions and confusion surrounding scale as a concept invoked in many disciplines.\n Spatial Scaling in Ecology by John Wiens (1989) describes the fundamental challenges of scale in Ecology."
  },
  {
    "objectID": "content/02-content.html#objectives",
    "href": "content/02-content.html#objectives",
    "title": "Why Geographic Analysis",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDefine what we mean by description, explanation, and prediction in geography.\nDescribe critiques and limitations of quantitative geographical analysis\nDefine ‘scale’ and its implications for geographic analysis\nPlace your research in the broader context of geographic analysis"
  },
  {
    "objectID": "content/02-content.html#slides",
    "href": "content/02-content.html#slides",
    "title": "Why Geographic Analysis",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Introduction to Spatial Data in R",
    "section": "",
    "text": "Now that we’ve covered some of the conceptual bases of spatial data and geographic analysis, it’s time to get started working with actual data in R. Today’s readings and lecture are focused on the basics of getting your data into the R environment and familiarizing yourself with the different components that make up spatial data objects. We’ll do fancier things in the weeks to come!"
  },
  {
    "objectID": "content/04-content.html#readings",
    "href": "content/04-content.html#readings",
    "title": "Introduction to Spatial Data in R",
    "section": "Readings",
    "text": "Readings\n Chapter 2 in Geocomputation with R (Lovelace et al. 2019) provides and overview of using sf for vector datasets and terra for raster data.\n Chapter 2 from Moraga (2023) explores similar topics, but provides more explanation about projections, coordinates, etc.\n Simple Features for R provides a more in-depth, technical discussion of how the sf package organizes spatial data and attributes.\n Ch 2.1-2.5 from Spatial Data Science with R and terra describes the basic functionality of terra for both vector and raster datasets. For reasons we’ll discuss in class, we will rarely use terra for vector data."
  },
  {
    "objectID": "content/04-content.html#objectives",
    "href": "content/04-content.html#objectives",
    "title": "Introduction to Spatial Data in R",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRead spatial data into your R environment.\nDescribe the various aspects of spatial data files and objects.\nGenerate simple summaries describing the spatial data object.\nDetermine the projection, extent, and resolution of spatial data objects."
  },
  {
    "objectID": "content/04-content.html#slides",
    "href": "content/04-content.html#slides",
    "title": "Introduction to Spatial Data in R",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/07-content.html",
    "href": "content/07-content.html",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "",
    "text": "Now that you have a bit of the fundamentals of geographic data and have had a chance to start using R, it’s time to get into more complicated workflows. To do that, you’ll have to have a bit more of a foundation in coordinates, coordinate reference systems, and geometries and how to access those in R. We’ll start there today and move into functions that change or relate geometries in the next few classes."
  },
  {
    "objectID": "content/07-content.html#readings",
    "href": "content/07-content.html#readings",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Chapter 2, Sections 1-3 and Chapter 3, Section 1 of Spatial Data Science by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)"
  },
  {
    "objectID": "content/07-content.html#objectives",
    "href": "content/07-content.html#objectives",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine coordinate, coordinate system, datum, and coordinate reference system\nAccess coordinate and geometry information for simple features in R\nUnderstand the rules for simple feature geometries\nAccess and transform the coordinate reference system for vector and raster data in R"
  },
  {
    "objectID": "content/07-content.html#slides",
    "href": "content/07-content.html#slides",
    "title": "Areal Data: Coordinates and Geometries",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording"
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Areal Data: Rasters",
    "section": "",
    "text": "Now that we’ve learned a bit about how to assess some of the important quantities of vector-based spatial data, we’ll try to apply a bit of the same logic to raster data. We’ll be using the terra package for the majority of raster options in this course primarily because it of its speed. That said, it is not a tidyverse package and so some of the intuition we used to organize the sf functions will be a little harder to extend here. I’ll do my best to help you make the links!!"
  },
  {
    "objectID": "content/09-content.html#readings",
    "href": "content/09-content.html#readings",
    "title": "Areal Data: Rasters",
    "section": "Readings",
    "text": "Readings\n\n Chapter 4 from Paula Moraga’s Spatial Statistics for Data Science: Theory and Practice with R provides a quick intro to using terra for raster and vector data.\n The terra reference page provides a brief overview of all of the functions and their categories. We’ll only focus on the SpatRaster methods.\n Raster Data Manipulation from the Spatial Data Science with R and terra ebook provides some nice examples of terra functions in the context of spatial workflows."
  },
  {
    "objectID": "content/09-content.html#objectives",
    "href": "content/09-content.html#objectives",
    "title": "Areal Data: Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAccess the elements that define a raster\nBuild rasters from scratch using matrix operations and terra\nEvaluate logical conditions with raster data\nCalculate different measures of raster data"
  },
  {
    "objectID": "content/09-content.html#slides",
    "href": "content/09-content.html#slides",
    "title": "Areal Data: Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/11-content.html",
    "href": "content/11-content.html",
    "title": "Operations With Vector Data I",
    "section": "",
    "text": "Now that we’ve spent some time getting used to the syntax of the sf package and used it to assess some of the characteristics of vector objects (e.g., through predicates and measures), we’ll move into transformations. Transformations allow you to actually manipulate the geometries of a vector object (without necessarily changing the attributes themselves) and are a powerful tool for geting disparate data into some logical alignment. That said, transforming geometries can be complicated and often has some unanticipated consequences. That’s why we spent a little bit of time learning the mapping syntax as a means for you to be able to check yourself."
  },
  {
    "objectID": "content/11-content.html#readings",
    "href": "content/11-content.html#readings",
    "title": "Operations With Vector Data I",
    "section": "Readings",
    "text": "Readings\n\n The introductory vignette for the sf package has a lot of useful info on sf objects and conventions.\n Section 2.2 on Vector Data and Sections 5.1-5.3 on Geographic Operations in Lovelace et al. (Lovelace et al. 2019) - for more details about vectors and geometric operations on vectors.\n Section 3.1 and 3.2 of Spatial Data Science, a bookdown project by Edzer Pebesma and Roger Bivand (of the sf, sp, rgeos, and rgdal packages)."
  },
  {
    "objectID": "content/11-content.html#objectives",
    "href": "content/11-content.html#objectives",
    "title": "Operations With Vector Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the unary, binary, and n-ary transformers\nArticulate common uses for unary and binary transformers\nUse unary transformations to fix invalid geometries\nImplement common binary transformers to align and combine data"
  },
  {
    "objectID": "content/11-content.html#slides",
    "href": "content/11-content.html#slides",
    "title": "Operations With Vector Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\n\n\n\nPanopto Vidoe Here"
  },
  {
    "objectID": "content/13-content.html",
    "href": "content/13-content.html",
    "title": "Operations with Raster Data I",
    "section": "",
    "text": "Now that we’ve learned about predicates and measures with raster data, it’s time to learn more about some of the transformations that we can conduct with terra. We’ll start with some of the basic transformations that operate on the entire dataset then move to some of the important cell-wise operations."
  },
  {
    "objectID": "content/13-content.html#readings",
    "href": "content/13-content.html#readings",
    "title": "Operations with Raster Data I",
    "section": "Readings",
    "text": "Readings\n\n The terra package vignette describes the new raster functions available in terra, their relationship to those in the raster package, and the changes in syntax between the two.\n The Raster GIS Operations in R with terra chapter from Jasper Slingsby’s “A Minimal Introduction to GIS (in R)” bookdown project has worked examples of many of the operations we’ll learn today."
  },
  {
    "objectID": "content/13-content.html#objectives",
    "href": "content/13-content.html#objectives",
    "title": "Operations with Raster Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nAlign rasters for spatial processing\nAdjust the resolution of raster data\nCombine (or reduce) rasters to match the extent of your analysis"
  },
  {
    "objectID": "content/13-content.html#slides",
    "href": "content/13-content.html#slides",
    "title": "Operations with Raster Data I",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to today’s Panopto Slides"
  },
  {
    "objectID": "content/15-content.html",
    "href": "content/15-content.html",
    "title": "Combining Spatial and Tabular Data",
    "section": "",
    "text": "Today we’ll begin exploring typical workflows for spatial analysis by working with attribute data. Attributes generally provide additional information about a location that we can use for visualization and analysis. Unlike spatial operations that we’ll explore next week, attribute data do not all require geographic information (but they do need some means of relating to a geography). These chapters are not ‘prerequisite’ reading for the week, but provide a lot of helpful background for attribute operations in R."
  },
  {
    "objectID": "content/15-content.html#resources",
    "href": "content/15-content.html#resources",
    "title": "Combining Spatial and Tabular Data",
    "section": "Resources",
    "text": "Resources\n\n The Tidy Data and Relational Data sections from R For Data Science (Wickham and Grolemund 2016) provide a great overview to data cleaning and manipulation functions available in the tidyverse.\n Doing things with multiple tables has a lot of nice visual examples of for using the _join functions in dplyr.\n This article (Di Minin et al. 2021) provides a recent recap of a variety of reasons why we may need to combine data from multiple, often disparate, sources.\n The Spatial Data Operations Chapter in (Lovelace et al. 2019) makes the concepts of a network concrete (literally) by using a transportation route example to illustrate the various components of a network analysis in R."
  },
  {
    "objectID": "content/15-content.html#objectives",
    "href": "content/15-content.html#objectives",
    "title": "Combining Spatial and Tabular Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine spatial analysis\nDescribe the steps in planning a spatial analysis\nUnderstand the structure of relational databases\nUse attributes and topology to subset data\nGenerate new features using geographic data\nJoin data based on attributes and location"
  },
  {
    "objectID": "content/15-content.html#slides",
    "href": "content/15-content.html#slides",
    "title": "Combining Spatial and Tabular Data",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides"
  },
  {
    "objectID": "content/17-content.html",
    "href": "content/17-content.html",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "",
    "text": "Note that the last 3 sections needed a bit of reorganization. I’ve moved to session 18 to keep the webpage aligned with the schedule."
  },
  {
    "objectID": "content/17-content.html#resources",
    "href": "content/17-content.html#resources",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "content/17-content.html#objectives",
    "href": "content/17-content.html#objectives",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:"
  },
  {
    "objectID": "content/17-content.html#slides",
    "href": "content/17-content.html#slides",
    "title": "Combining Vectors and Categorical Rasters",
    "section": "Slides",
    "text": "Slides\nThe slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/19-content.html",
    "href": "content/19-content.html",
    "title": "Interpolation",
    "section": "",
    "text": "Point patterns give us the foundation for beginning geostatistical analyses. In geostatistical analyses, we have observations or a spatial process from a limited sample of locations, but would like to be able to infer the values of that process across the entire study region (or at least an area larger than we initially sampled). Interpolation provides one simple way of doing this that relies on the notion that we can learn something about the process simply from our measurements and the location those measurements were taken. We can extend these approaches by adding additional covariates and model structures, but we’ll start simple for now."
  },
  {
    "objectID": "content/19-content.html#resources",
    "href": "content/19-content.html#resources",
    "title": "Interpolation",
    "section": "Resources",
    "text": "Resources\n\n Chapter 2: Scale in (Fletcher and Fortin 2018) provides a thorough introduction to the ecologist’s conceptualization of scale with R examples.\n This article by Steven Manson (Manson 2008) provides a more comprehensive view of conceptualizations of scale.\n The Hypothesis Testing and Autocorrelation chapters of Manuel Gimond’s Introduction to GIS and Spatial Analysis bookdown project provide concrete examples of attempts to find process from spatial patterns.\n Chapter 12: Spatial Interpolation in Spatial Data Science by Edzer Pebesma and Roger Bivand provides examples of different types of kriging and interpolation using sf and stars."
  },
  {
    "objectID": "content/19-content.html#objectives",
    "href": "content/19-content.html#objectives",
    "title": "Interpolation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\nDistinguish deterministic and stochastic processes\nDefine autocorrelation and describe its estimation\nArticulate the benefits and drawbacks of autocorrelation\nLeverage point patterns and autocorrelation to interpolate missing data\n\n\n View all slides in new window  Download PDF of all slides\n\n\n\n\nLink to Panopto Recording"
  },
  {
    "objectID": "content/21-content.html#objectives",
    "href": "content/21-content.html#objectives",
    "title": "Spatial Autocorrelation",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/23-content.html#objectives",
    "href": "content/23-content.html#objectives",
    "title": "Statistical Modelling II",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/25-content.html#objectives",
    "href": "content/25-content.html#objectives",
    "title": "Movement and Networks I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/29-content.html#objectives",
    "href": "content/29-content.html#objectives",
    "title": "Data Visualization and Maps I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/31-content.html#objectives",
    "href": "content/31-content.html#objectives",
    "title": "Introduction to Interactive Maps",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today you should be able to:\n\n View all slides in new window  Download PDF of all slides\n\nSlides Forthcoming"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings and slides",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture. On each class session page you’ll see buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n\n View all slides in new window  Download PDF of all slides\n\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes)."
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Examples",
    "section": "",
    "text": "This section has the code and description from the live-coding exercises that we’ll do in class."
  },
  {
    "objectID": "lesson/dataclasses.html",
    "href": "lesson/dataclasses.html",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\n\nCode\nnum &lt;- 2.2\nclass(num)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(num)\n\n\n[1] \"double\"\n\n\nCode\ny &lt;- 1:10 \ny\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nclass(y)\n\n\n[1] \"integer\"\n\n\nCode\ntypeof(y)\n\n\n[1] \"integer\"\n\n\nCode\nlength(y)\n\n\n[1] 10\n\n\nCode\nb &lt;- \"3\"\nclass(b)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(b)\n\n\n[1] FALSE\n\n\nCode\nc &lt;- as.numeric(b)\nclass(c)\n\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n\nCode\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.2\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.3\n\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\n\nCode\nc(series.2, series.3)\n\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\nCode\nclass(series.3)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(series.3)\n\n\n[1] \"double\"\n\n\nCode\nlength(series.3)\n\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\n\nCode\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nCode\nanyNA(x)\n\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n\nCode\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n\n[1] 2 3\n\n\nCode\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n\n[1] 3 2\n\n\nCode\na[3,1]\n\n\nx \n3 \n\n\nCode\nb &lt;- rbind(x, y)\ndim(b)\n\n\n[1] 2 3\n\n\nCode\nb[1,3]\n\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\n\nCode\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nCode\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[1]]\n\n\n[1] \"Waldo\"\n\n\nCode\nxlist[[3]]\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[3]][1]\n\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\n\nCode\nxlist[[3]][1,2]\n\n\n[1] 6\n\n\nCode\nxlist[3][1]\n\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\n\nCode\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\n\nCode\nis.list(dat)\n\n\n[1] TRUE\n\n\nCode\nclass(dat)\n\n\n[1] \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat) #gives the first 6 rows similar to tail()\n\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\n\nCode\ndim(dat)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat[1,3]\n\n\n[1] 11\n\n\nCode\ndat[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n\n[1] TRUE\n\n\nCode\nclass(dat.tib)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\n\nCode\ndim(dat.tib)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat.tib)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\n\nCode\ndat.tib[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat.tib$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data."
  },
  {
    "objectID": "lesson/dataclasses.html#data-types-and-structures",
    "href": "lesson/dataclasses.html#data-types-and-structures",
    "title": "Data Structures",
    "section": "",
    "text": "Okay, now that we have all of those details out of the way, let’s take a look at data structures in R. As we discussed,R has six basic types of data: numeric, integer, logical, complex, character, and raw. For this class, we won’t bother with complex or raw as you are unlikely to encounter them in your introductory spatial explorations.\n\nNumeric data are numbers that contain a decimal. They can also be whole numbers\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). Factors become important in the analyses and visualizations we’ll attempt later in the course.\n\nThere are a variety of ways to learn more about the structure of different data types:\n\nclass() - returns the type of object (high level)\ntypeof() - returns the type of object (low level)\nlength() tells you about the length of an object\nattributes() - does the object have any metadata\n\n\n\nCode\nnum &lt;- 2.2\nclass(num)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(num)\n\n\n[1] \"double\"\n\n\nCode\ny &lt;- 1:10 \ny\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nclass(y)\n\n\n[1] \"integer\"\n\n\nCode\ntypeof(y)\n\n\n[1] \"integer\"\n\n\nCode\nlength(y)\n\n\n[1] 10\n\n\nCode\nb &lt;- \"3\"\nclass(b)\n\n\n[1] \"character\"\n\n\nCode\nis.numeric(b)\n\n\n[1] FALSE\n\n\nCode\nc &lt;- as.numeric(b)\nclass(c)\n\n\n[1] \"numeric\"\n\n\n\n\n\nYou can store information in a variety of ways in R. The types we are most likely to encounter this semester are:\n\nVectors: a collection of elements that are typically character, logical, integer, or numeric.\n\n\n\nCode\n#sometimes we'll need to make sequences of numbers to facilitate joins\nseries &lt;- 1:10\nseries.2 &lt;- seq(10)\nseries.3 &lt;- seq(from = 1, to = 10, by = 0.1)\nseries\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.2\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseries.3\n\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\n\nCode\nc(series.2, series.3)\n\n\n  [1]  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0 10.0  1.0  1.1  1.2  1.3  1.4\n [16]  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9\n [31]  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4\n [46]  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9\n [61]  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9  7.0  7.1  7.2  7.3  7.4\n [76]  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4  8.5  8.6  8.7  8.8  8.9\n [91]  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9 10.0\n\n\nCode\nclass(series.3)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(series.3)\n\n\n[1] \"double\"\n\n\nCode\nlength(series.3)\n\n\n[1] 91\n\n\n\nMissing Data: R supports missing data in most of the data structures we use, but they can lead to some strange behaviors. Here are a few ways to find missing data:\n\n\n\nCode\nx &lt;- c(\"a\", NA, \"c\", \"d\", NA)\nis.na(x)\n\n\n[1] FALSE  TRUE FALSE FALSE  TRUE\n\n\nCode\nanyNA(x)\n\n\n[1] TRUE\n\n\n\nMatrices: are an extension of the numeric or character vectors. They are not a separate type of object but simply an atomic vector with dimensions; the number of rows and columns. As with atomic vectors, the elements of a matrix must be of the same data. Matrices are the foundation of rasters, which we’ll be discussing frequently throughout the course\n\n\n\nCode\n#matrices are filled columnwise in R\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\ndim(m)\n\n\n[1] 2 3\n\n\nCode\nx &lt;- 1:3\ny &lt;- 10:12\n\na &lt;- cbind(x, y)\ndim(a)\n\n\n[1] 3 2\n\n\nCode\na[3,1]\n\n\nx \n3 \n\n\nCode\nb &lt;- rbind(x, y)\ndim(b)\n\n\n[1] 2 3\n\n\nCode\nb[1,3]\n\n\nx \n3 \n\n\n\nLists: Lists essentially act like containers in R - they can hold a variety of different data types and structures including more lists. We use lists a lot for functional programming in R where we can apply a function to each element in a list. We’ll see this with extracting values from multiple rasters. We can extract elements of lists usin [] and [[]]\n\n\n\nCode\nx &lt;- list(1, \"a\", TRUE, 1+4i)\nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\n\nCode\n#adding names\nxlist &lt;- list(a = \"Waldo\", b = 1:10, data = head(mtcars))\nxlist\n\n\n$a\n[1] \"Waldo\"\n\n$b\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[1]]\n\n\n[1] \"Waldo\"\n\n\nCode\nxlist[[3]]\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nCode\nxlist[[3]][1]\n\n\n                   mpg\nMazda RX4         21.0\nMazda RX4 Wag     21.0\nDatsun 710        22.8\nHornet 4 Drive    21.4\nHornet Sportabout 18.7\nValiant           18.1\n\n\nCode\nxlist[[3]][1,2]\n\n\n[1] 6\n\n\nCode\nxlist[3][1]\n\n\n$data\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nData Frames: data frames resemble that tabular datasets you might be used to in spreadsheet programs and are probably one of the most common types of data in R. A data frame is a special type of list where every element has the same length (but can have different types of data). We’ll be reading in a number of data frames for this first assignment.\n\n\n\nCode\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\n\nCode\nis.list(dat)\n\n\n[1] TRUE\n\n\nCode\nclass(dat)\n\n\n[1] \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat) #compact summary of the structure of a dataframe\n\n\n'data.frame':   10 obs. of  3 variables:\n $ id: chr  \"a\" \"b\" \"c\" \"d\" ...\n $ x : int  1 2 3 4 5 6 7 8 9 10\n $ y : int  11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat) #gives the first 6 rows similar to tail()\n\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\n\nCode\ndim(dat)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat[1,3]\n\n\n[1] 11\n\n\nCode\ndat[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\n\nTibbles: are similar to data frames, but allow for lists within columns. They are designed for use with the tidyverse (which we’ll explore more in future classes), but the primary reason for introducing them here is because they are the foundation of sf objects which we’ll use frequently in the weeks to come.\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndat.tib &lt;- tibble(dat)\nis.list(dat.tib)\n\n\n[1] TRUE\n\n\nCode\nclass(dat.tib)\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nCode\n#lots of ways to look at data in data frames\nstr(dat.tib) #compact summary of the structure of a dataframe\n\n\ntibble [10 × 3] (S3: tbl_df/tbl/data.frame)\n $ id: chr [1:10] \"a\" \"b\" \"c\" \"d\" ...\n $ x : int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ y : int [1:10] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\nhead(dat.tib) #gives the first 6 rows similar to tail()\n\n\n# A tibble: 6 × 3\n  id        x     y\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 a         1    11\n2 b         2    12\n3 c         3    13\n4 d         4    14\n5 e         5    15\n6 f         6    16\n\n\nCode\ndim(dat.tib)\n\n\n[1] 10  3\n\n\nCode\ncolnames(dat.tib)\n\n\n[1] \"id\" \"x\"  \"y\" \n\n\nCode\n## accessing elements of a dataframe\ndat.tib[1,3]\n\n\n# A tibble: 1 × 1\n      y\n  &lt;int&gt;\n1    11\n\n\nCode\ndat.tib[[\"y\"]]\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nCode\ndat.tib$y\n\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nMany of the packages used for spatial operations in R rely on special objects (e.g., sf, SpatRasters) that are combinations of these various elemental data types. That is why we are taking a little time to understand them before jumping into spatial data."
  },
  {
    "objectID": "lesson/index.html",
    "href": "lesson/index.html",
    "title": "Lessons",
    "section": "",
    "text": "This section has some worked examples demonstrating the use of different packages and giving some ‘roadmaps’ for completing different spatial operations. These are mostly my opinions, your mileage may vary, but I’ll try to justify why I do things the way that I do so that you can make an informed choice when you decide to deviate from that path."
  },
  {
    "objectID": "lesson/vector_intro.html",
    "href": "lesson/vector_intro.html",
    "title": "Intro to Vector Data",
    "section": "",
    "text": "Today we’ll build on the introductory discussion we were having about vector operations and the sf package. We’ll build a few vectors from scratch and then move on to explore a broader suite of common vector operations implemented by the sf package."
  },
  {
    "objectID": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "href": "lesson/vector_intro.html#a-reminder-about-vector-geometries-in-r",
    "title": "Intro to Vector Data",
    "section": "A reminder about vector geometries in R",
    "text": "A reminder about vector geometries in R\nYou’ll recall that the sf package organizes the different types of vectors (e.g., points, lines, polygons) in to a hierarchical structure organized by complexity of geometries contained within an R object. For example, a single point will be a POINT, several points will be a MULTIPOINT, and an object containing points, polygons, and lines will be a GEOMETRYCOLLECTION. We need to be aware of what types of geometries and objects we have becasue some operations are restricted to particular types of objects or geometries as indicated by errors that read:\nError in UseMethod(\"st_crs&lt;-\") :    no applicable method for 'st_crs&lt;-' applied to an object of class \"c('XY', 'POINT', 'sfg')\"\nwhich indicates that the function (st_crs) does not have a method defined for the type of object it’s being applied to. Note that the function inside UseMethod will be replaced by whichever function you’re attempting to apply to your object and the object of class component will vary based on the function and the object class.\n\n\n\n\n\n\n\ntype\ndescription\n\n\n\n\nPOINT\nsingle point geometry\n\n\nMULTIPOINT\nset of points\n\n\nLINESTRING\nsingle linestring (two or more points connected by straight lines)\n\n\nMULTILINESTRING\nset of linestrings\n\n\nPOLYGON\nexterior ring with zero or more inner rings, denoting holes\n\n\nMULTIPOLYGON\nset of polygons\n\n\nGEOMETRYCOLLECTION\nset of the geometries above\n\n\n\nAs is, these geometries are built on vertices with coordinates that are based on the Cartesian plane and thus are “spatial”, but not georeferenced or geographic. In order to convert these sf geometries to a geogrphic object (i.e., one with a CRS and whose location depicts and actual spot on the earth’s surface), we use st_sfc() to create a simple feature geography list column (see ?st_sfc for an example of this workflow)."
  },
  {
    "objectID": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#conventions-in-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Conventions in sf and the tidyverse",
    "text": "Conventions in sf and the tidyverse\nOne of the benefits of the sf package is that it is designed to interface with the tidyverse suite of packages. One of the appealing parts of working with tidyverse packages is that they share an underlying philosophy, data structure, and grammar. This can make life a lot easier as you move from getting your data into R, constructing a set of covariates (including those derived from spatial data), analyzing, and plotting (or mapping) those data. People have strong opinions about the tidyverse, but I find it to be an (eventually) useful way for people to gain some intuition for working in R. One of the grammatical conventions used in the tidyverse suite of packages is the use _ in function calls (this is known as snake case should you ever need to know that at a dinner party). The _ is typically used to separate the verb in a function call from its predicate. For example, bind_rows() in the dplyr package “binds” (the verb) rows (the predicate) wheras bind_cols() binds columns. For the sf package it’s slightly different in that most of the functions begin with a st_ or sf_ prefix to indicate that the function is designed to work on spatial objects followed by a word (or words) describing what the operation does (e.g., st_centroid() returns a MULTIPOINT object with each point located at the centroid of a polygon). We can classify these functions based on what they are expected to return:\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nTransformations: create new geometries based on input geometries.\n\nWe can also distinguish these functions based on how many geometries that operate on:\n\nUnary: operate on a single geometry at a time (meaning that if you have a MULTI* object the function works on each geometry individually)\nBinary: operate on pairs of geometries\nn-ary: operate on sets of geometries\n\nWe’ll focus on the unary operators for now, but the binary and n-ary operators will become more important as we move to develop databases for spatial analysis.\n\nUnary predicates\nUnary predicates are helpful ‘checks’ to make sure the object you are working with has the properties you might expect. Are the geometries valid? Is the data projected? Because we are asking a set of TRUE/FALSE questions, these functions are specified as st_is_:\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nsimple\nis the geometry self-intersecting (i.e., simple)?\n\n\nvalid\nis the geometry valid?\n\n\nempty\nis the geometry column of an object empty?\n\n\nlonglat\ndoes the object have geographic coordinates? (FALSE if coords are projected, NA if no crs)\n\n\nis(geometry, class)\nis the geometry of a particular class?\n\n\n\n\n\nCode\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n\n\nReading layer `nc' from data source \n  `/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library/sf/shape/nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nCode\nst_is_longlat(nc)\n\n\n[1] TRUE\n\n\nCode\nst_is_valid(nc)\n\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\n\nUnary measures\nMeasures return a quantity that describes the geometry\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndimension\n0 for points, 1 for linear, 2 for polygons, possibly NA for empty geometries\n\n\narea\nthe area of a geometry\n\n\nlength\nthe length of a linear geometry\n\n\n\ndistance is a binary measure that returns the distance between pairs of geometries either within a single object or between features in multiple objects\n\n\nCode\nhead(st_area(nc))\n\n\nUnits: [m^2]\n[1] 1137107793  610916077 1423145355  694378925 1520366979  967504822\n\n\nCode\nst_distance(nc)[1:5,1:5]\n\n\nUnits: [m]\n         [,1]     [,2]     [,3]      [,4]      [,5]\n[1,]      0.0      0.0  25591.8 439493.26 299049.94\n[2,]      0.0      0.0      0.0 408416.68 268284.09\n[3,]  25591.8      0.0      0.0 366648.94 226461.23\n[4,] 439493.3 408416.7 366648.9      0.00  67066.43\n[5,] 299049.9 268284.1 226461.2  67066.43      0.00\n\n\n\n\nUnary transformers\nUnary transformations work on a per object basis and return a new geometry for each geometry. These are a few of the most common, we’ll encounter a few more as the semester continues.\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is this larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (last week)\n\n\ncollection_extract\nwith subgeometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n\n\n\nCode\nplot(st_geometry(nc))\nplot(st_geometry(st_centroid(nc)), add=TRUE, col='red')\n\n\nWarning: st_centroid assumes attributes are constant over geometries"
  },
  {
    "objectID": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "href": "lesson/vector_intro.html#using-sf-and-the-tidyverse",
    "title": "Intro to Vector Data",
    "section": "Using sf and the tidyverse",
    "text": "Using sf and the tidyverse\nAs I mentioned, one of the benefits of using the sf package is that commands from the other tidyverse package have defined methods for spatial objects. The dplyr package has a ton of helpful functions for maniputlating data in R. For example, we might select a single row from a shapefile based on the value of its attributes by using the dplyr::filter() command:\n\n\nCode\ndurham.cty &lt;- nc %&gt;% \n  filter(., NAME == \"Durham\")\n## We can also use the bracket approach\ndurham.cty2 &lt;- nc[nc$NAME == \"Durham\",]\n\nplot(st_geometry(nc))\nplot(st_geometry(durham.cty), add=TRUE, col=\"blue\")\n\n\n\n\n\nOr perhaps we only want a few of the columns in the dataset (because shapefiles always have lots of extra stuff). We can use dplyr::select() to choose columns by name:\n\n\nCode\nnc.select &lt;- nc %&gt;% \n  select(., c(\"CNTY_ID\", \"NAME\", \"FIPS\"))\nplot(nc.select)\n\n\n\n\n\nNotice that the geometries are sticky, this will be important later"
  },
  {
    "objectID": "resource/git.html",
    "href": "resource/git.html",
    "title": "Helpful git links",
    "section": "",
    "text": "Getting in the habit of using version control can be challenging, especially if you are collaborating with others. The challenge gets worse when some of those collaborators are not familiar with the importance of version control. Here are a few links to try and make your (and their) transition a little smoother."
  },
  {
    "objectID": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "href": "resource/git.html#installing-git-and-making-it-play-nice-with-r",
    "title": "Helpful git links",
    "section": "Installing Git and making it play nice with R",
    "text": "Installing Git and making it play nice with R\nHappy git with R is Jenny Bryan’s: extremely helpful introduction to git and incorporating it into your R workflow."
  },
  {
    "objectID": "resource/git.html#getting-the-hang-of-git",
    "href": "resource/git.html#getting-the-hang-of-git",
    "title": "Helpful git links",
    "section": "Getting the hang of git",
    "text": "Getting the hang of git\nUnderstanding the logic of git: provides a relatively accessible explanation of the various operations in git and links that to commonly used syntax.\nOh Sh@t, Git?!?: A less technical, more irreverant introduction to git workflows and fixing the inevitable challenges of version control. (G-rated version available at Dang it, Git?!?)."
  },
  {
    "objectID": "resource/install.html",
    "href": "resource/install.html",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "",
    "text": "You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code."
  },
  {
    "objectID": "resource/install.html#rstudio-on-your-computer",
    "href": "resource/install.html#rstudio-on-your-computer",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "RStudio on your computer",
    "text": "RStudio on your computer\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\n\nInstall R\nFirst you need to install R itself (the engine).\n\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\nClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\nIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is 4.2.1) and download it.\nIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\nDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\nIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\n\n\nInstall RStudio\nNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\n\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\nThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\nDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\nDouble click on RStudio to run it (check your applications folder or start menu).\n\n\nInstall tidyverse\nThe tidyverse consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nYou can install packages manually in RStudio, but this can be a bit fragile, especially for some of the spatial packages. Instead of using the RStudio GUI we’ll just install thins at the prompt. To install the tidyverse pacakge (and all of its associated dependencies) run the following: install.packages(\"tidyverse\")."
  },
  {
    "objectID": "resource/install.html#footnotes",
    "href": "resource/install.html#footnotes",
    "title": "Installing R, RStudio, and tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎"
  },
  {
    "objectID": "resource/r.html",
    "href": "resource/r.html",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, self-contained tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.\n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.\n[Robert’s Page]"
  },
  {
    "objectID": "resource/r.html#learning-r",
    "href": "resource/r.html#learning-r",
    "title": "Getting started with R Spatial",
    "section": "",
    "text": "I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nThese resources are also really really helpful:\n\n\n\nAn Introduction to R: The definitive introductory text by Venables, Smith, and the R Core Team.\nSwirl: A set of free, self-contained tutorials that run from within your RStudio terminal.\nR for Data Science: A free online book for learning the basics of R and the tidyverse.\nR and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things.\nStat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online.\nSTA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online.\nCSE 631: Principles & Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.\n\n\n\n\n\nsf cheatsheet: An at-a-glance description of the various sf verbs and their application.\nGeocomputation with R: Online version of the textbook by Lovelace, Nowosad, and Muenchow.\n[Robert’s Page]"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester!\n\nContent (): This page contains the readings, slides, and recorded lectures for the week. Read and watch these before our in-person class.\nLesson (): This page contains additional annotated R code and other supplementary information that you can use as a reference for your assignments and project. This is only a reference page—you don’t have to necessarily do anything here, but it will be helpful as you work on your assignments.\nExample (): This page the scripts that we work on in class as a reminder of some of the live-coding exercises. These are provided as a reference to help you link your notes to the syntax we use in class.\nAssignment (): This page contains the instructions for each assignment. Assignments are due by 11:59 PM on the day they’re listed.\n\n\n\n\n\n\n\nSubscribe!\n\n\n\nYou can subscribe to this calendar URL in Outlook, Google Calendar, or Apple Calendar:\n\n\n\n Download\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nAugust 21(Session 1)\n\n\nIntroduction to the course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 23(Session 2)\n\n\nWhy Geographic Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 25\n\n\n Self-Evaluation 1 due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 28(Session 3)\n\n\nIntroduction to Spatial Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 30(Session 4)\n\n\nIntroduction to Spatial Data with R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugust 31\n\n\n Homework 1  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 4\n\n\nNo Class(Labor Day)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 6(Session 6)\n\n\nLiterate Programming, Quarto, Workflows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 7\n\n\n Homework 2  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Operations in R\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nSeptember 11(Session 7)\n\n\nAreal Data: Coordinates and Geometries(remote)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 13(Session 8)\n\n\nAreal Data: Vector Data(remote)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 14\n\n\n Homework 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 18(Session 9)\n\n\nAreal Data: Rasters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 20(Session 10)\n\n\nMapping Geographic Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 21\n\n\n Homework 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 25(Session 11)\n\n\nOperations With Vectors I(remote)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 27(Session 12)\n\n\nOperations With Vectors II(remote)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember 28\n\n\n Assignment Revision 1  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 2(Session 13)\n\n\nOperations With Rasters I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 4(Session 14)\n\n\nOperations With Rasters II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 5\n\n\n Homework 5  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 9(Session 15)\n\n\nCombining Tabular Data and Spatial Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 11(Session 16)\n\n\nCombining Vectors and Continuous Rasters(remote)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 15\n\n\n Homework 6  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 16(Session 17)\n\n\nCombining Vectors and Categorical Rasters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Workflows for Spatial Data\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nOctober 18(Session 18)\n\n\nPoint Patterns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 19\n\n\n Homework 7  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 23(Session 19)\n\n\nInterpolation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 25(Session 20)\n\n\nProximity and Areal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 26\n\n\n Homework 8  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober 30(Session 21)\n\n\nSpatial Autocorrelation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 1(Session 22)\n\n\nStatistical Modelling I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 2\n\n\n Assignment Revision 2  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 6(Session 23)\n\n\nStatistical Modelling II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 8(Session 24)\n\n\nStatistical Modelling III\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 9\n\n\n Homework 9  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 13(Session 25)\n\n\nMovement and Networks I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 15(Session 26)\n\n\nMovement and Networks II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 16\n\n\n Homework 10  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 20\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 22\n\n\nNo Class(Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Spatial Data\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nNovember 27(Session 29)\n\n\nData Visualization and Maps I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 29(Session 30)\n\n\nData Visualization and Maps II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember 30\n\n\n Assignment Revision 3  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 4(Session 31)\n\n\nIntroduction to Interactive Maps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrapup\n\n\n\n\n\n\n\nTitle\n\n\nContent\n\n\nLesson\n\n\nExample\n\n\nAssignment\n\n\n\n\n\n\nDecember 5\n\n\n Draft Final Project Due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 6(Session 32)\n\n\nConclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 7\n\n\n Final Project Draft  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 8\n\n\nFinal Project Workday(optional)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 14\n\n\n Final Project Due  (submit by 11:59 PM)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember 15\n\n\n Final Self-Evaluation Due  (submit by 11:59 PM)"
  },
  {
    "objectID": "slides/02-slides.html#checking-in",
    "href": "slides/02-slides.html#checking-in",
    "title": "Why Geographic Analysis",
    "section": "Checking in",
    "text": "Checking in\n\nWhat are some advantages and disadvantages of using R for spatial analysis\nWhat can I clarify about the course?\nHow do you feel about git and github classroom? How can I make that easier for you?"
  },
  {
    "objectID": "slides/02-slides.html#todays-plan",
    "href": "slides/02-slides.html#todays-plan",
    "title": "Why Geographic Analysis",
    "section": "Today’s Plan",
    "text": "Today’s Plan\n\n\nWhat can we do with geographic information?\nConceptual challenges\nAnalytical challenges\nCritiques of quantitative geography"
  },
  {
    "objectID": "slides/02-slides.html#what-is-geography",
    "href": "slides/02-slides.html#what-is-geography",
    "title": "Why Geographic Analysis",
    "section": "What is geography",
    "text": "What is geography\n\nGeo: land, earth, terrain\nGraph: writing, discourse\nTuan: Space (extent) and Place (location)\nAnalysis of the effects of extent and location on events or features"
  },
  {
    "objectID": "slides/02-slides.html#five-themes-in-geography",
    "href": "slides/02-slides.html#five-themes-in-geography",
    "title": "Why Geographic Analysis",
    "section": "Five Themes in Geography",
    "text": "Five Themes in Geography\n\n\n\nLocation\nPlace\nRegion\nMovement\nHuman-Environment Interaction\n\n\n\n\n\nWGBH Educational Foundation"
  },
  {
    "objectID": "slides/02-slides.html#location",
    "href": "slides/02-slides.html#location",
    "title": "Why Geographic Analysis",
    "section": "Location",
    "text": "Location\nThe place (on Earth) of a particular geographic feature"
  },
  {
    "objectID": "slides/02-slides.html#location-1",
    "href": "slides/02-slides.html#location-1",
    "title": "Why Geographic Analysis",
    "section": "Location",
    "text": "Location\nThe place (on Earth) of a particular geographic feature"
  },
  {
    "objectID": "slides/02-slides.html#place",
    "href": "slides/02-slides.html#place",
    "title": "Why Geographic Analysis",
    "section": "Place",
    "text": "Place\nWhat is a location like?\n\n\n\n\n\n\n\n\n\n\n\n\nForest cover map by Robert Simmons via Wikimedia Commons; temp map from GISgeography.com"
  },
  {
    "objectID": "slides/02-slides.html#place-1",
    "href": "slides/02-slides.html#place-1",
    "title": "Why Geographic Analysis",
    "section": "Place",
    "text": "Place\nWhat is a location like?"
  },
  {
    "objectID": "slides/02-slides.html#region",
    "href": "slides/02-slides.html#region",
    "title": "Why Geographic Analysis",
    "section": "Region",
    "text": "Region\nHow are different areas similar or different?"
  },
  {
    "objectID": "slides/02-slides.html#region-1",
    "href": "slides/02-slides.html#region-1",
    "title": "Why Geographic Analysis",
    "section": "Region",
    "text": "Region\nHow are different areas similar or different?"
  },
  {
    "objectID": "slides/02-slides.html#movement",
    "href": "slides/02-slides.html#movement",
    "title": "Why Geographic Analysis",
    "section": "Movement",
    "text": "Movement\nHow do genes, individuals, populations, ideas, goods, etc traverse the landscape."
  },
  {
    "objectID": "slides/02-slides.html#movement-1",
    "href": "slides/02-slides.html#movement-1",
    "title": "Why Geographic Analysis",
    "section": "Movement",
    "text": "Movement\nHow do genes, individuals, populations, ideas, goods, etc traverse the landscape.\n\n\n\n\n\n\n\n\n\n\n\n\nBoth maps courtesy of High Country News"
  },
  {
    "objectID": "slides/02-slides.html#human-environment-interactions",
    "href": "slides/02-slides.html#human-environment-interactions",
    "title": "Why Geographic Analysis",
    "section": "Human-Environment Interactions",
    "text": "Human-Environment Interactions\nHow do people relate to and change the physical world to meet their needs?\n\n\n\n\n\n\n\n\n\n\n\n\nSmoke map courtesy of Capital Public Radio; Nightlights courtesy of NASA Earth Observatory."
  },
  {
    "objectID": "slides/02-slides.html#description",
    "href": "slides/02-slides.html#description",
    "title": "Why Geographic Analysis",
    "section": "Description",
    "text": "Description\n\n\n\nCoordinates\nDistances\nNeighbors\nSummary statistics\n\n\n\n\n\n\n\ncourtesy of innovative GIS"
  },
  {
    "objectID": "slides/02-slides.html#description-1",
    "href": "slides/02-slides.html#description-1",
    "title": "Why Geographic Analysis",
    "section": "Description",
    "text": "Description\n\n\n\nRange Maps\nHotspots\nIndices"
  },
  {
    "objectID": "slides/02-slides.html#explanation-and-inference",
    "href": "slides/02-slides.html#explanation-and-inference",
    "title": "Why Geographic Analysis",
    "section": "Explanation and Inference",
    "text": "Explanation and Inference\n\nCognitive Description: collection ordering and classification of data\nCause and Effect: design-based or model-based testing of the factors that give rise to geographic distributions\nSystems Analysis: describes the entire complex set of interactions that structure an activity"
  },
  {
    "objectID": "slides/02-slides.html#prediction",
    "href": "slides/02-slides.html#prediction",
    "title": "Why Geographic Analysis",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\n\nExtend description or explanation into unmeasured space\nStationarity: the rules governing a process do not drift over space-time"
  },
  {
    "objectID": "slides/02-slides.html#scale",
    "href": "slides/02-slides.html#scale",
    "title": "Why Geographic Analysis",
    "section": "Scale",
    "text": "Scale\n\nWhat do we even mean?\n\n\n\n\n\n\nGrain: the smallest unit of measurement\nExtent: the areal coverage of the measurement\n\n\n\n\n\nFrom Manson 2008"
  },
  {
    "objectID": "slides/02-slides.html#scale-1",
    "href": "slides/02-slides.html#scale-1",
    "title": "Why Geographic Analysis",
    "section": "Scale",
    "text": "Scale\n\nEven if it exists, how do we know we are measuring at the right scale?"
  },
  {
    "objectID": "slides/02-slides.html#fallacies",
    "href": "slides/02-slides.html#fallacies",
    "title": "Why Geographic Analysis",
    "section": "Fallacies",
    "text": "Fallacies\n\n\n\nLocational Fallacy: Error due to the spatial characterization chosen for elements of study\nAtomic Fallacy: Applying conclusions from individuals to entire spatial units\nEcological Fallacy: Applying conclusions from aggregated information to individuals"
  },
  {
    "objectID": "slides/02-slides.html#measurement-error-and-mismatch",
    "href": "slides/02-slides.html#measurement-error-and-mismatch",
    "title": "Why Geographic Analysis",
    "section": "Measurement Error and Mismatch",
    "text": "Measurement Error and Mismatch"
  },
  {
    "objectID": "slides/02-slides.html#spatial-autocorrelation",
    "href": "slides/02-slides.html#spatial-autocorrelation",
    "title": "Why Geographic Analysis",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/02-slides.html#stationarity",
    "href": "slides/02-slides.html#stationarity",
    "title": "Why Geographic Analysis",
    "section": "Stationarity",
    "text": "Stationarity\nThe rules governing a process do not drift over space-time\n\n\n\nFirst Order effects: any event has an equal probability of occurring in a location\nSecond Order effects: the location of one event is independent of the other events\n\n\n\n\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/02-slides.html#not-all-geography-needs-to-be-quantitative",
    "href": "slides/02-slides.html#not-all-geography-needs-to-be-quantitative",
    "title": "Why Geographic Analysis",
    "section": "Not all geography needs to be quantitative",
    "text": "Not all geography needs to be quantitative\n\nAbstraction removes the interesting part\nWhat “is” may require assumptions we don’t want to accept\nWholly dependent on the military-industrial complex"
  },
  {
    "objectID": "slides/04-slides.html#objectives",
    "href": "slides/04-slides.html#objectives",
    "title": "Reading Spatial Data in R",
    "section": "Objectives",
    "text": "Objectives\n\nRevisit the components of spatial data\nDescribe some of the key considerations for thinking about spatial data\nIntroduce the two primary R packages for spatial workflows\nLearn to read and explore spatial objects in R"
  },
  {
    "objectID": "slides/04-slides.html#describing-absolute-locations",
    "href": "slides/04-slides.html#describing-absolute-locations",
    "title": "Reading Spatial Data in R",
    "section": "Describing Absolute Locations",
    "text": "Describing Absolute Locations\n\nCoordinates: 2 or more measurements that specify location relative to a reference system\n\n\n\n\n\nCartesian coordinate system\norigin (O) = the point at which both measurement systems intersect\nAdaptable to multiple dimensions (e.g. z for altitude)\n\n\n\n\n\n\nCartesian Coordinate System"
  },
  {
    "objectID": "slides/04-slides.html#locations-on-a-globe",
    "href": "slides/04-slides.html#locations-on-a-globe",
    "title": "Reading Spatial Data in R",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\n\n\nLatitude and Longitude"
  },
  {
    "objectID": "slides/04-slides.html#locations-on-a-globe-1",
    "href": "slides/04-slides.html#locations-on-a-globe-1",
    "title": "Reading Spatial Data in R",
    "section": "Locations on a Globe",
    "text": "Locations on a Globe\n\nThe earth is not flat…\nGlobal Reference Systems (GRS)\nGraticule: the grid formed by the intersection of longitude and latitude\nThe graticule is based on an ellipsoid model of earth’s surface and contained in the datum"
  },
  {
    "objectID": "slides/04-slides.html#global-reference-systems",
    "href": "slides/04-slides.html#global-reference-systems",
    "title": "Reading Spatial Data in R",
    "section": "Global Reference Systems",
    "text": "Global Reference Systems\n\nThe datum describes which ellipsoid to use and the precise relations between locations on earth’s surface and Cartesian coordinates\n\n\nGeodetic datums (e.g., WGS84): distance from earth’s center of gravity\nLocal data (e.g., NAD83): better models for local variation in earth’s surface"
  },
  {
    "objectID": "slides/04-slides.html#describing-location-extent",
    "href": "slides/04-slides.html#describing-location-extent",
    "title": "Reading Spatial Data in R",
    "section": "Describing location: extent",
    "text": "Describing location: extent\n\n\nHow much of the world does the data cover?\nFor rasters, these are the corners of the lattice\nFor vectors, we call this the bounding box"
  },
  {
    "objectID": "slides/04-slides.html#describing-location-resolution",
    "href": "slides/04-slides.html#describing-location-resolution",
    "title": "Reading Spatial Data in R",
    "section": "Describing location: resolution",
    "text": "Describing location: resolution\n\n\n\n\nResolution: the accuracy that the location and shape of a map’s features can be depicted\nMinimum Mapping Unit: The minimum size and dimensions that can be reliably represented at a given map scale.\nMap scale vs. scale of analysis"
  },
  {
    "objectID": "slides/04-slides.html#projections",
    "href": "slides/04-slides.html#projections",
    "title": "Reading Spatial Data in R",
    "section": "Projections",
    "text": "Projections\n\n\n\n\nBut maps, screens, and publications are…\nProjections describe how the data should be translated to a flat surface\nRely on ‘developable surfaces’\nDescribed by the Coordinate Reference System (CRS)\n\n\n\n\n\n\nDevelopable Surfaces\n\n\n\n\n\nProjection necessarily induces some form of distortion (tearing, compression, or shearing)"
  },
  {
    "objectID": "slides/04-slides.html#coordinate-reference-systems",
    "href": "slides/04-slides.html#coordinate-reference-systems",
    "title": "Reading Spatial Data in R",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nSome projections minimize distortion of angle, area, or distance\nOthers attempt to avoid extreme distortion of any kind\nIncludes: Datum, ellipsoid, units, and other information (e.g., False Easting, Central Meridian) to further map the projection to the GCS\nNot all projections have/require all of the parameters"
  },
  {
    "objectID": "slides/04-slides.html#choosing-projections",
    "href": "slides/04-slides.html#choosing-projections",
    "title": "Reading Spatial Data in R",
    "section": "Choosing Projections",
    "text": "Choosing Projections\n\n\n\n\n\nEqual-area for thematic maps\nConformal for presentations\nMercator or equidistant for navigation and distance"
  },
  {
    "objectID": "slides/04-slides.html#geometries",
    "href": "slides/04-slides.html#geometries",
    "title": "Reading Spatial Data in R",
    "section": "Geometries",
    "text": "Geometries\n\n\n\nVectors store aggregate the locations of a feature into a geometry\nMost vector operations require simple, valid geometries\n\n\n\n\n\nImage Source: Colin Williams (NEON)"
  },
  {
    "objectID": "slides/04-slides.html#valid-geometries",
    "href": "slides/04-slides.html#valid-geometries",
    "title": "Reading Spatial Data in R",
    "section": "Valid Geometries",
    "text": "Valid Geometries\n\n\nA linestring is simple if it does not intersect\nValid polygons\nAre closed (i.e., the last vertex equals the first)\nHave holes (inner rings) that inside the the exterior boundary\nHave holes that touch the exterior at no more than one vertex (they don’t extend across a line) - For multipolygons, adjacent polygons touch only at points\nDo not repeat their own path"
  },
  {
    "objectID": "slides/04-slides.html#empty-geometries",
    "href": "slides/04-slides.html#empty-geometries",
    "title": "Reading Spatial Data in R",
    "section": "Empty Geometries",
    "text": "Empty Geometries\n\nEmpty geometries arise when an operation produces NULL outcomes (like looking for the intersection between two non-intersecting polygons)\nsf allows empty geometries to make sure that information about the data type is retained\nSimilar to a data.frame with no rows or a list with NULL values\nMost vector operations require simple, valid geometries"
  },
  {
    "objectID": "slides/04-slides.html#support",
    "href": "slides/04-slides.html#support",
    "title": "Reading Spatial Data in R",
    "section": "Support",
    "text": "Support\n\nSupport is the area to which an attribute applies.\n\n\n\nFor vectors, the attribute-geometry-relationship can be:\nconstant = applies to every point in the geometry (lines and polygons are just lots of points)\nidentity = a value unique to a geometry\naggregate = a single value that integrates data across the geometry\nRasters can have point (attribute refers to the cell center) or cell (attribute refers to an area similar to the pixel) support"
  },
  {
    "objectID": "slides/04-slides.html#spatial-messiness",
    "href": "slides/04-slides.html#spatial-messiness",
    "title": "Reading Spatial Data in R",
    "section": "Spatial Messiness",
    "text": "Spatial Messiness\n\nQuantitative geography requires that our data are aligned\nAchieving alignment is part of reproducible workflows\nMaking principled decisions about projections, resolution, extent, etc"
  },
  {
    "objectID": "slides/04-slides.html#data-types-and-r-packages",
    "href": "slides/04-slides.html#data-types-and-r-packages",
    "title": "Reading Spatial Data in R",
    "section": "Data Types and R Packages",
    "text": "Data Types and R Packages\n\n\nData Types\n\nVector Data\n\nPoint features\nLine features\nArea features (polygons)\n\nRaster Data\n\nSpatially continuous field\nBased on pixels (not points)"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-spreadsheets",
    "href": "slides/04-slides.html#reading-in-spatial-data-spreadsheets",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: spreadsheets",
    "text": "Reading in Spatial Data: spreadsheets\n\n\nMost basic form of spatial data\nNeed x (longitude) and y (latitude) as columns\nNeed to know your CRS\nread_*** necessary to bring in the data\n\n\n\nlibrary(tidyverse)\nlibrary(sf)\n\nfile.to.read &lt;- read_csv(file = \"path/to/your/file\", \n                         col_names = TRUE, col_types = NULL, \n                         na =na = c(\"\", \"NA\"))\n\nfile.as.sf &lt;- st_as_sf(file.to.read, \n                       coords = c(\"longitude\", \"latitude\"), \n                       crs=4326)"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-shapefiles",
    "href": "slides/04-slides.html#reading-in-spatial-data-shapefiles",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: shapefiles",
    "text": "Reading in Spatial Data: shapefiles\n\nALL FILES NEED TO BE IN THE SAME FOLDER\n\n\n\n\n\n.shp is the shapefile itself\n.prj contains the CRS information\n.dbf contains the attributes\n.shx contains the indices for matching attributes to geometries\nother extensions contain metadata\n\n\n\n\n\nst_read and read_sf in the sf package will read shapefiles into R\nread_sf leaves character vectors alone (often beneficial)\nst_read can handle other datatypes (like geodatabases)\nReturns slightly different classes"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-shapefiles-1",
    "href": "slides/04-slides.html#reading-in-spatial-data-shapefiles-1",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: shapefiles",
    "text": "Reading in Spatial Data: shapefiles\n\n\n\nlibrary(sf)\nshapefile.inR &lt;- read_sf(dsn = \"path/to/file.shp\", \n                         layer=NULL, geometry_column=...)"
  },
  {
    "objectID": "slides/04-slides.html#reading-in-spatial-data-rasters",
    "href": "slides/04-slides.html#reading-in-spatial-data-rasters",
    "title": "Reading Spatial Data in R",
    "section": "Reading in Spatial Data: rasters",
    "text": "Reading in Spatial Data: rasters\n\nrast will read rasters using the terra package\nAlso used to create rasters from scratch\nReturns SpatRaster object\n\n\n\n\nlibrary(sf)\nraster.inR &lt;- rast(x = \"path/to/file.shp\", \n                         lyrs=NULL)"
  },
  {
    "objectID": "slides/04-slides.html#introducing-the-data",
    "href": "slides/04-slides.html#introducing-the-data",
    "title": "Reading Spatial Data in R",
    "section": "Introducing the Data",
    "text": "Introducing the Data\n\nGood idea to get to know your data before manipulating it\nstr, summary, nrow, ncol are good places to start\nst_crs (for sf class objects) and crs (for SpatRaster objects)\nWe’ll practice a few of these now…"
  },
  {
    "objectID": "slides/04-slides.html#saving-your-data",
    "href": "slides/04-slides.html#saving-your-data",
    "title": "Reading Spatial Data in R",
    "section": "Saving your data",
    "text": "Saving your data\n\nwrite_sf for sf objects; writeRaster for SpatRasters\n\n\nlibrary(sf)\nlibrary(terra)\n\nwrite_sf(object = object.to.save, dsn = \"path/to/save/object\", append = FALSE)\nwriteRaster(x=object, filename = \"path/to/save\")"
  },
  {
    "objectID": "slides/07-slides.html#objectives",
    "href": "slides/07-slides.html#objectives",
    "title": "Coordinates and Geometries",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the linkage between location, coordinates, coordinate reference systems, and geometry\nAccess and manipulate geometries in R with sf (and terra)\nDefine geometry in the context of vector objects and troubleshoot common problems\nChange the CRS for vectors and rasters (and understand the implications)"
  },
  {
    "objectID": "slides/07-slides.html#getting-more-acquainted-with-r",
    "href": "slides/07-slides.html#getting-more-acquainted-with-r",
    "title": "Coordinates and Geometries",
    "section": "Getting more acquainted with R",
    "text": "Getting more acquainted with R\n\nObjects, classes, functions, oh my…\nIntuition for the tidyverse\nGetting used to pipes (%&gt;% or |&gt;)\nLearning to prototype"
  },
  {
    "objectID": "slides/07-slides.html#kinds-of-errors",
    "href": "slides/07-slides.html#kinds-of-errors",
    "title": "Coordinates and Geometries",
    "section": "2 Kinds of Errors",
    "text": "2 Kinds of Errors\n\nSyntax Errors: Your code won’t actually run\nSemantic Errors: Your code runs without error, but the result is unexpected"
  },
  {
    "objectID": "slides/07-slides.html#asking-better-questions-getting-better-answers",
    "href": "slides/07-slides.html#asking-better-questions-getting-better-answers",
    "title": "Coordinates and Geometries",
    "section": "Asking better questions, getting better answers",
    "text": "Asking better questions, getting better answers\n\nPlaces to get help (Google, Slack, Stack Overflow, Github Issue pages)\nWhat are you trying to do? (the outcome you want/expect)\nWhat isn’t working? (the code and steps you’ve tried so far)\nWhy aren’t common solutions working? (proof that you’ve done your due diligence)"
  },
  {
    "objectID": "slides/07-slides.html#reproducible-examples",
    "href": "slides/07-slides.html#reproducible-examples",
    "title": "Coordinates and Geometries",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\nDon’t require someone to have your data or your computer\nMinimal amount of information and code to reproduce your error\nIncludes both code and your operating environment info\nMore info\nAn example with spatial data"
  },
  {
    "objectID": "slides/07-slides.html#reference-systems",
    "href": "slides/07-slides.html#reference-systems",
    "title": "Coordinates and Geometries",
    "section": "Reference Systems",
    "text": "Reference Systems\n\nTo locate an object or quantity, we need:\n\nA fixed origin (or datum) to measure distances to/from\nA measurement unit (or scale) that defines the units of distance\nDatum + scale = reference system"
  },
  {
    "objectID": "slides/07-slides.html#coordinate-reference-systems",
    "href": "slides/07-slides.html#coordinate-reference-systems",
    "title": "Coordinates and Geometries",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nMap the location on an object to earth (geodetic) or flat (projected) surfaces\nCoordinate System - the mathematical rules that specify how coordinates are assigned to points\nDatum - the parameter or set of parameters that define the position of the origin, the scale, and the orientation of a coordinate system\nCoordinate Reference Systems - a coordinate system that is related to an object by a datum"
  },
  {
    "objectID": "slides/07-slides.html#accessing-crs-with-r",
    "href": "slides/07-slides.html#accessing-crs-with-r",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\nsf::st_crs() for vector data\nterra::crs() for raster data\nstored in WKT, epsg, or proj4string (deprecated)\nThe EPSG website is a great reference for getting projection info"
  },
  {
    "objectID": "slides/07-slides.html#accessing-crs-with-r-1",
    "href": "slides/07-slides.html#accessing-crs-with-r-1",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\ndir.for.files &lt;- \"/Users/mattwilliamson/Library/CloudStorage/GoogleDrive-mattwilliamson@boisestate.edu/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2023/assignment01/\"\nvector.data &lt;- sf::st_read(dsn = paste0(dir.for.files, \"cejst_nw.shp\"), quiet=TRUE)\nsf::st_crs(x = vector.data)$input\n\n[1] \"WGS 84\"\n\nsf::st_crs(x = vector.data)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\nsf::st_crs(x = vector.data)$wkt\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    ID[\\\"EPSG\\\",4326]]\""
  },
  {
    "objectID": "slides/07-slides.html#accessing-crs-with-r-2",
    "href": "slides/07-slides.html#accessing-crs-with-r-2",
    "title": "Coordinates and Geometries",
    "section": "Accessing CRS with R",
    "text": "Accessing CRS with R\n\nraster.data &lt;- terra::rast(x = paste0(dir.for.files, \"wildfire_hazard_agg.tif\"))\nterra::crs(raster.data, describe=TRUE, proj=TRUE)\n\n     name authority code area         extent\n1 unnamed      &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA, NA, NA, NA\n                                                                                                 proj\n1 +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"
  },
  {
    "objectID": "slides/07-slides.html#what-if-you-dont-know-the-crs",
    "href": "slides/07-slides.html#what-if-you-dont-know-the-crs",
    "title": "Coordinates and Geometries",
    "section": "What if you don’t know the CRS?",
    "text": "What if you don’t know the CRS?\n\n\nSometimes you receive data that is missing the projection\nYou can assign it (with caution)\nYou can guess it using crsuggest::guess_crs()\n\n\n\nlibrary(sf)\nlibrary(mapview)\nlocations &lt;- data.frame(\n  X = c(1200822.97857801, 1205015.51644983, 1202297.44383987, 1205877.68696743, \n        1194763.21511923, 1195463.42403192, 1199836.01037452, 1207081.96500368, \n        1201924.15986897),\n  Y = c(1246476.31475063, 1248612.72571423, 1241479.45996392, 1243898.58428024, \n        1246033.7550009, 1241827.7730307, 1234691.50899912, 1251125.67808482, \n        1252188.4333016),\n  id = 1:9\n)\n\nlocations_sf &lt;- st_as_sf(locations, coords = c(\"X\", \"Y\"))"
  },
  {
    "objectID": "slides/07-slides.html#guessing-crs",
    "href": "slides/07-slides.html#guessing-crs",
    "title": "Coordinates and Geometries",
    "section": "Guessing CRS",
    "text": "Guessing CRS\n\nlibrary(crsuggest)\nguess_crs(locations_sf, \"Chennai, India\", n_return = 5)\n\n# A tibble: 5 × 2\n  crs_code dist_km\n  &lt;chr&gt;      &lt;dbl&gt;\n1 7785        4.07\n2 24344     806.  \n3 32644     806.  \n4 32244     806.  \n5 32444     806.  \n\nst_crs(locations_sf) &lt;- 7785"
  },
  {
    "objectID": "slides/07-slides.html#changing-the-crs",
    "href": "slides/07-slides.html#changing-the-crs",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS",
    "text": "Changing the CRS\n\nRequires recomputing coordinates\nCoordinate Conversion - No change to the datum; lossless\nCoordinate Transformation - New datum; relies on models; some error involved"
  },
  {
    "objectID": "slides/07-slides.html#changing-the-crs-in-r",
    "href": "slides/07-slides.html#changing-the-crs-in-r",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nsf::st_transform for vectors\nterra::project for rasters\nProjecting Rasters Causes Distortion"
  },
  {
    "objectID": "slides/07-slides.html#changing-the-crs-in-r-1",
    "href": "slides/07-slides.html#changing-the-crs-in-r-1",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nvector.data.proj &lt;- vector.data %&gt;% \n  sf::st_transform(., crs = 3083)\nst_crs(vector.data.proj)$input\n\n[1] \"EPSG:3083\"\n\nvector.data.proj.rast &lt;- vector.data %&gt;% \n  sf::st_transform(., crs = crs(raster.data))\nst_crs(vector.data.proj.rast)$proj4string\n\n[1] \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\""
  },
  {
    "objectID": "slides/07-slides.html#changing-the-crs-in-r-2",
    "href": "slides/07-slides.html#changing-the-crs-in-r-2",
    "title": "Coordinates and Geometries",
    "section": "Changing the CRS in R",
    "text": "Changing the CRS in R\n\nraster.data.proj &lt;- project(x = raster.data, y = \"EPSG:3083\")\ncrs(raster.data.proj, describe=TRUE, proj=TRUE)\n\n                                     name authority code\n1 NAD83 / Texas Centric Albers Equal Area      EPSG 3083\n                         area                        extent\n1 United States (USA) - Texas -106.66, -93.50, 36.50, 25.83\n                                                                                                            proj\n1 +proj=aea +lat_0=18 +lon_0=-100 +lat_1=27.5 +lat_2=35 +x_0=1500000 +y_0=6000000 +datum=NAD83 +units=m +no_defs\n\nraster.data.proj.vect &lt;- project(x = raster.data, y = vect(vector.data))\ncrs(raster.data.proj.vect, describe=TRUE, proj=TRUE)\n\n    name authority code area         extent                                proj\n1 WGS 84      EPSG 4326 &lt;NA&gt; NA, NA, NA, NA +proj=longlat +datum=WGS84 +no_defs"
  },
  {
    "objectID": "slides/07-slides.html#the-vector-data-model",
    "href": "slides/07-slides.html#the-vector-data-model",
    "title": "Coordinates and Geometries",
    "section": "The Vector Data Model",
    "text": "The Vector Data Model\n\n\n\n\nCoordinates define the Vertices (i.e., discrete x-y locations) that comprise the geometry\nThe organization of those vertices define the shape of the vector\nGeneral types: points, lines, polygons"
  },
  {
    "objectID": "slides/07-slides.html#representing-vector-data-in-r",
    "href": "slides/07-slides.html#representing-vector-data-in-r",
    "title": "Coordinates and Geometries",
    "section": "Representing vector data in R",
    "text": "Representing vector data in R\n\n\n\n\n\nFrom Lovelace et al.\n\n\n\n\n\nsf hierarchy reflects increasing complexity of geometry\n\nst_point, st_linestring, st_polygon for single features\nst_multi* for multiple features of the same type\nst_geometrycollection for multiple feature types\nst_as_sfc creates the geometry list column for many sf operations"
  },
  {
    "objectID": "slides/07-slides.html#points",
    "href": "slides/07-slides.html#points",
    "title": "Coordinates and Geometries",
    "section": "Points",
    "text": "Points\n\nlibrary(sf)\nproj &lt;- st_crs('+proj=longlat +datum=WGS84')\nlong &lt;- c(-116.7, -120.4, -116.7, -113.5, -115.5, -120.8, -119.5, -113.7, -113.7, -110.7)\nlat &lt;- c(45.3, 42.6, 38.9, 42.1, 35.7, 38.9, 36.2, 39, 41.6, 36.9)\nst_multipoint(cbind(long, lat)) %&gt;% st_sfc(., crs = proj)\n\nGeometry set for 1 feature \nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -120.8 ymin: 35.7 xmax: -110.7 ymax: 45.3\nGeodetic CRS:  +proj=longlat +datum=WGS84"
  },
  {
    "objectID": "slides/07-slides.html#points-1",
    "href": "slides/07-slides.html#points-1",
    "title": "Coordinates and Geometries",
    "section": "Points",
    "text": "Points\n\nplot(st_multipoint(cbind(long, lat)) %&gt;% \n                   st_sfc(., crs = proj))"
  },
  {
    "objectID": "slides/07-slides.html#lines",
    "href": "slides/07-slides.html#lines",
    "title": "Coordinates and Geometries",
    "section": "Lines",
    "text": "Lines\n\nlon &lt;- c(-116.8, -114.2, -112.9, -111.9, -114.2, -115.4, -117.7)\nlat &lt;- c(41.3, 42.9, 42.4, 39.8, 37.6, 38.3, 37.6)\nlonlat &lt;- cbind(lon, lat)\npts &lt;- st_multipoint(lonlat)\n\nsfline &lt;- st_multilinestring(list(pts[1:3,], pts[4:7,]))\nstr(sfline)\n\nList of 2\n $ : num [1:3, 1:2] -116.8 -114.2 -112.9 41.3 42.9 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"lon\" \"lat\"\n $ : num [1:4, 1:2] -111.9 -114.2 -115.4 -117.7 39.8 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:2] \"lon\" \"lat\"\n - attr(*, \"class\")= chr [1:3] \"XY\" \"MULTILINESTRING\" \"sfg\""
  },
  {
    "objectID": "slides/07-slides.html#lines-1",
    "href": "slides/07-slides.html#lines-1",
    "title": "Coordinates and Geometries",
    "section": "Lines",
    "text": "Lines\n\nplot(st_multilinestring(list(pts[1:3,], pts[4:7,])))"
  },
  {
    "objectID": "slides/07-slides.html#polygons",
    "href": "slides/07-slides.html#polygons",
    "title": "Coordinates and Geometries",
    "section": "Polygons",
    "text": "Polygons\n\nouter = matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)\nhole1 = matrix(c(1,1,1,2,2,2,2,1,1,1),ncol=2, byrow=TRUE)\nhole2 = matrix(c(5,5,5,6,6,6,6,5,5,5),ncol=2, byrow=TRUE)\ncoords = list(outer, hole1, hole2)\npl1 = st_polygon(coords)"
  },
  {
    "objectID": "slides/07-slides.html#polygons-1",
    "href": "slides/07-slides.html#polygons-1",
    "title": "Coordinates and Geometries",
    "section": "Polygons",
    "text": "Polygons\n\nplot(pl1)"
  },
  {
    "objectID": "slides/07-slides.html#common-problems-with-vector-data",
    "href": "slides/07-slides.html#common-problems-with-vector-data",
    "title": "Coordinates and Geometries",
    "section": "Common Problems with Vector Data",
    "text": "Common Problems with Vector Data\n\n\n\n\nVectors and scale\nSlivers and overlaps\nUndershoots and overshoots\nSelf-intersections and rings\n\n\n\n\n\n\nTopology Errors - Saylor Acad.\n\n\n\n\n\nWe’ll use st_is_valid() to check this, but fixing can be tricky"
  },
  {
    "objectID": "slides/07-slides.html#fixing-problematic-topology",
    "href": "slides/07-slides.html#fixing-problematic-topology",
    "title": "Coordinates and Geometries",
    "section": "Fixing Problematic Topology",
    "text": "Fixing Problematic Topology\n\nst_make_valid() for simple cases\nst_buffer with dist=0\nMore complex errors need more complex approaches"
  },
  {
    "objectID": "slides/07-slides.html#a-note-on-vectors",
    "href": "slides/07-slides.html#a-note-on-vectors",
    "title": "Coordinates and Geometries",
    "section": "A Note on Vectors",
    "text": "A Note on Vectors\n\nMoving forward we will rely primarily on the sf package for vector manipulation. Some packages require objects to be a different class. terra, for example, relies on SpatVectors. You can use as() to coerce objects from one type to another (assuming a method exists). You can also explore other packages. Many packages provide access to the ‘spatial’ backbones of R (like geos and gdal), they just differ in how the “verbs” are specified. For sf operations the st_ prefix is typical. For rgeos operations, the g prefix is common."
  },
  {
    "objectID": "slides/09-slides.html#revisiting-the-raster-data-model",
    "href": "slides/09-slides.html#revisiting-the-raster-data-model",
    "title": "Areal Data: Rasters",
    "section": "Revisiting the Raster Data Model",
    "text": "Revisiting the Raster Data Model\n\n\n\n\nVector data describe the “exact” locations of features on a landscape (including a Cartesian landscape)\nRaster data represent spatially continuous phenomena (NA is possible)\nDepict the alignment of data on a regular lattice (often a square)\n\nOperations mimic those for matrix objects in R\n\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size"
  },
  {
    "objectID": "slides/09-slides.html#rasters-with-terra",
    "href": "slides/09-slides.html#rasters-with-terra",
    "title": "Areal Data: Rasters",
    "section": "Rasters with terra",
    "text": "Rasters with terra\n\nsyntax is different for terra compared to sf\nRepresentation in Environment is also different\nCan break pipes, Be Explicit"
  },
  {
    "objectID": "slides/09-slides.html#rasters-by-construction-1",
    "href": "slides/09-slides.html#rasters-by-construction-1",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction",
    "text": "Rasters by Construction\n\n\n\nmtx &lt;- matrix(1:16, nrow=4)\nmtx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nrstr &lt;- terra::rast(mtx)\nrstr\n\nclass       : SpatRaster \ndimensions  : 4, 4, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 4, 0, 4  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        : lyr.1 \nmin value   :     1 \nmax value   :    16 \n\n\n\n\n\n\n\n\n\n\n\nNote: you must have raster or terra loaded for plot() to work on Rast* objects; otherwise you get Error in as.double(y) : cannot coerce type 'S4' to vector of type 'double'"
  },
  {
    "objectID": "slides/09-slides.html#rasters-by-construction-origin",
    "href": "slides/09-slides.html#rasters-by-construction-origin",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction: Origin",
    "text": "Rasters by Construction: Origin\n\nOrigin defines the location of the intersection of the x and y axes\n\n\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, ncols=10)\nr[] &lt;- runif(ncell(r))\norigin(r)\n\n[1] 0.05 0.00\n\nr2 &lt;- r\norigin(r2) &lt;- c(2,2)"
  },
  {
    "objectID": "slides/09-slides.html#rasters-by-construction-resolution",
    "href": "slides/09-slides.html#rasters-by-construction-resolution",
    "title": "Areal Data: Rasters",
    "section": "Rasters by Construction: Resolution",
    "text": "Rasters by Construction: Resolution\n\n\nGeometry is implicit; the spatial extent and number of rows and columns define the cell size\nResolution (res) defines the length and width of an individual pixel\n\n\n\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, \n          ncols=10)\nres(r)\n\n[1] 1.35 1.00\n\nr2 &lt;- rast(xmin=-4, xmax = 5, \n           ncols=10)\nres(r2)\n\n[1] 0.9 1.0\n\n\n\n\nr &lt;- rast(xmin=-4, xmax = 9.5, \n          res=c(0.5,0.5))\nncol(r)\n\n[1] 27\n\nr2 &lt;- rast(xmin=-4, xmax = 9.5, \n           res=c(5,5))\nncol(r2)\n\n[1] 3"
  },
  {
    "objectID": "slides/09-slides.html#extending-predicates",
    "href": "slides/09-slides.html#extending-predicates",
    "title": "Areal Data: Rasters",
    "section": "Extending predicates",
    "text": "Extending predicates\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nterra does not follow the same hierarchy as sf so a little trickier"
  },
  {
    "objectID": "slides/09-slides.html#unary-predicates-in-terra",
    "href": "slides/09-slides.html#unary-predicates-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Unary predicates in terra",
    "text": "Unary predicates in terra\n\nCan tell us qualities of a raster dataset\nMany similar operations for SpatVector class (note use of .)\n\n\n\n\n\n\n\n\n\npredicate\nasks…\n\n\n\n\nis.lonlat\nDoes the object have a longitude/latitude CRS?\n\n\ninMemory\nis the object stored in memory?\n\n\nis.factor\nAre there categorical layers?\n\n\nhasValues\nDo the cells have values?"
  },
  {
    "objectID": "slides/09-slides.html#unary-predicates-in-terra-1",
    "href": "slides/09-slides.html#unary-predicates-in-terra-1",
    "title": "Areal Data: Rasters",
    "section": "Unary predicates in terra",
    "text": "Unary predicates in terra\n\n\n\n\nglobal: tests if the raster covers all longitudes (from -180 to 180 degrees) such that the extreme columns are in fact adjacent\n\n\nr &lt;- rast()\nis.lonlat(r)\n\n[1] TRUE\n\nis.lonlat(r, global=TRUE)\n\n[1] TRUE\n\n\n\n\n\n\nperhaps: If TRUE and the crs is unknown, the method returns TRUE if the coordinates are plausible for longitude/latitude\n\n\ncrs(r) &lt;- \"\"\nis.lonlat(r)\n\n[1] NA\n\nis.lonlat(r, perhaps=TRUE, warn=FALSE)\n\n[1] TRUE\n\n\n\ncrs(r) &lt;- \"+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84\"\nis.lonlat(r)\n\n[1] FALSE"
  },
  {
    "objectID": "slides/09-slides.html#binary-predicates-in-terra",
    "href": "slides/09-slides.html#binary-predicates-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Binary predicates in terra",
    "text": "Binary predicates in terra\n\nTake exactly 2 inputs, return 1 matrix of cell locs where value is TRUE\nadjacent: identifies cells adajcent to a set of raster cells"
  },
  {
    "objectID": "slides/09-slides.html#unary-measures-in-terra",
    "href": "slides/09-slides.html#unary-measures-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Unary measures in terra",
    "text": "Unary measures in terra\n\nSlightly more flexible than sf\nOne result for each layer in a stack\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ncellSize\narea of individual cells\n\n\nexpanse\nsummed area of all cells\n\n\nvalues\nreturns all cell values\n\n\nncol\nnumber of columns\n\n\nnrow\nnumber of rows\n\n\nncell\nnumber of cells\n\n\nres\nresolution\n\n\next\nminimum and maximum of x and y coords\n\n\norigin\nthe orgin of a SpatRaster\n\n\ncrs\nthe coordinate reference system\n\n\ncats\ncategories of a categorical raster"
  },
  {
    "objectID": "slides/09-slides.html#binary-measures-in-terra",
    "href": "slides/09-slides.html#binary-measures-in-terra",
    "title": "Areal Data: Rasters",
    "section": "Binary measures in terra",
    "text": "Binary measures in terra\n\nReturns a matrix or SpatRaster describing the measure\n\n\n\n\n\n\n\n\n\nmeasure\nreturns\n\n\n\n\ndistance\nshortest distance to non-NA or vector object\n\n\ngridDistance\nshortest distance through adjacent grid cells\n\n\ncostDistance\nShortest distance considering cell-varying friction\n\n\ndirection\nazimuth to cells that are not NA"
  },
  {
    "objectID": "slides/11-slides.html#objectives",
    "href": "slides/11-slides.html#objectives",
    "title": "Operations With Vector Data I",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nRecognize the unary, binary, and n-ary transformers\nArticulate common uses for unary and binary transformers\nUse unary transformations to fix invalid geometries\nImplement common binary transformers to align and combine data"
  },
  {
    "objectID": "slides/11-slides.html#revisiting-predicates-and-measures",
    "href": "slides/11-slides.html#revisiting-predicates-and-measures",
    "title": "Operations With Vector Data I",
    "section": "Revisiting predicates and measures",
    "text": "Revisiting predicates and measures\n\nPredicates: evaluate a logical statement asserting that a property is TRUE\nMeasures: return a numeric value with units based on the units of the CRS\nUnary, binary, and n-ary distinguish how many geometries each function accepts and returns"
  },
  {
    "objectID": "slides/11-slides.html#transformations",
    "href": "slides/11-slides.html#transformations",
    "title": "Operations With Vector Data I",
    "section": "Transformations",
    "text": "Transformations\n\n\nTransformations: create new geometries based on input geometries"
  },
  {
    "objectID": "slides/11-slides.html#unary-transformations",
    "href": "slides/11-slides.html#unary-transformations",
    "title": "Operations With Vector Data I",
    "section": "Unary Transformations",
    "text": "Unary Transformations\n\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\ncentroid\nof type POINT with the geometry’s centroid\n\n\nbuffer\nthat is larger (or smaller) than the input geometry, depending on the buffer size\n\n\njitter\nthat was moved in space a certain amount, using a bivariate uniform distribution\n\n\nwrap_dateline\ncut into pieces that do no longer cover the dateline\n\n\nboundary\nwith the boundary of the input geometry\n\n\nconvex_hull\nthat forms the convex hull of the input geometry\n\n\nline_merge\nafter merging connecting LINESTRING elements of a MULTILINESTRING into longer LINESTRINGs.\n\n\nmake_valid\nthat is valid\n\n\nnode\nwith added nodes to linear geometries at intersections without a node; only works on individual linear geometries\n\n\npoint_on_surface\nwith a (arbitrary) point on a surface\n\n\npolygonize\nof type polygon, created from lines that form a closed ring"
  },
  {
    "objectID": "slides/11-slides.html#fixing-geometries",
    "href": "slides/11-slides.html#fixing-geometries",
    "title": "Operations With Vector Data I",
    "section": "Fixing geometries",
    "text": "Fixing geometries\n\nWhen all(st_is_valid(your.shapefile)) returns FALSE\n\n\n\n\n\nst_make_valid has two methods:\n\noriginal converts rings into noded lines and extracts polygons\nstructured makes rings valid first then merges/subtracts from existing polgyons\nVerify that the output is what you expect!!\n\n\n\n\n\n```{r}\nx = st_sfc(st_polygon(list(rbind(c(0,0),c(0.5,0),c(0.5,0.5),c(0.5,0),c(1,0),c(1,1),c(0,1),c(0,0)))))\nst_is_valid(x)\n```\n\n[1] FALSE"
  },
  {
    "objectID": "slides/11-slides.html#fixing-geometries-with-st_make_valid",
    "href": "slides/11-slides.html#fixing-geometries-with-st_make_valid",
    "title": "Operations With Vector Data I",
    "section": "Fixing geometries with st_make_valid",
    "text": "Fixing geometries with st_make_valid\n\n\n\n\n\n\n\n\n\n```{r}\ny &lt;- x %&gt;% st_make_valid()\nst_is_valid(y)\n```\n\n[1] TRUE"
  },
  {
    "objectID": "slides/11-slides.html#fixing-geometries-with-st_buffer",
    "href": "slides/11-slides.html#fixing-geometries-with-st_buffer",
    "title": "Operations With Vector Data I",
    "section": "Fixing Geometries with st_buffer",
    "text": "Fixing Geometries with st_buffer\n\n\n-st_buffer enforces valid geometries as an output\n\nSetting a 0 distance buffer leaves most geometries unchanged\nNot all transformations do this\n\n\n\n```{r}\nz &lt;- x %&gt;% st_buffer(., dist=0)\n\nst_is_valid(z)\n```\n\n[1] TRUE"
  },
  {
    "objectID": "slides/11-slides.html#changing-crs-with-st_transform",
    "href": "slides/11-slides.html#changing-crs-with-st_transform",
    "title": "Operations With Vector Data I",
    "section": "Changing CRS with st_transform",
    "text": "Changing CRS with st_transform\n\nYou’ve already been using this!!\nDoes not guarantee valid geometries (use check = TRUE if you want this)\nWe’ll try to keep things from getting too complicated"
  },
  {
    "objectID": "slides/11-slides.html#converting-areas-to-points-with-st_centroid-or-st_point_on_surface",
    "href": "slides/11-slides.html#converting-areas-to-points-with-st_centroid-or-st_point_on_surface",
    "title": "Operations With Vector Data I",
    "section": "Converting areas to points with st_centroid or st_point_on_surface",
    "text": "Converting areas to points with st_centroid or st_point_on_surface\n\n\n\n\nFor “sampling” other datasets\nTo simplify distance calculations\nTo construct networks\n\n\n\n\nid.counties &lt;- tigris::counties(state = \"ID\", progress_bar=FALSE)\nid.centroid &lt;- st_centroid(id.counties)\nid.pointonsurf &lt;- st_point_on_surface(id.counties)"
  },
  {
    "objectID": "slides/11-slides.html#creating-sampling-areas",
    "href": "slides/11-slides.html#creating-sampling-areas",
    "title": "Operations With Vector Data I",
    "section": "Creating “sampling areas”",
    "text": "Creating “sampling areas”\n\nUncertainty in your point locations\nIncorporate a fixed range around each point\nCombine multiple points into a single polygon\n\n\nhospitals.id &lt;- landmarks.id.csv %&gt;% \n  st_as_sf(., coords = c(\"longitude\", \"lattitude\")) %&gt;% \n  filter(., MTFCC == \"K1231\")\nst_crs(hospitals.id) &lt;- 4326"
  },
  {
    "objectID": "slides/11-slides.html#creating-sampling-areas-1",
    "href": "slides/11-slides.html#creating-sampling-areas-1",
    "title": "Operations With Vector Data I",
    "section": "Creating sampling areas",
    "text": "Creating sampling areas\n\nhospital.buf &lt;- hospitals.id %&gt;%\n  st_buffer(., dist=10000)\n\nhospital.mcp &lt;- hospitals.id %&gt;% \n  st_convex_hull(.)"
  },
  {
    "objectID": "slides/11-slides.html#other-unary-transformations",
    "href": "slides/11-slides.html#other-unary-transformations",
    "title": "Operations With Vector Data I",
    "section": "Other Unary Transformations",
    "text": "Other Unary Transformations\n\n\n\n\n\n\n\n\ntransformer\nreturns a geometry …\n\n\n\n\nsegmentize\na (linear) geometry with nodes at a given density or minimal distance\n\n\nsimplify\nsimplified by removing vertices/nodes (lines or polygons)\n\n\nsplit\nthat has been split with a splitting linestring\n\n\ntransform\ntransformed or convert to a new coordinate reference system (chapter @ref(cs))\n\n\ntriangulate\nwith Delauney triangulated polygon(s) (figure @ref(fig:vor))\n\n\nvoronoi\nwith the Voronoi tessellation of an input geometry (figure @ref(fig:vor))\n\n\nzm\nwith removed or added Z and/or M coordinates\n\n\ncollection_extract\nwith subgeometries from a GEOMETRYCOLLECTION of a particular type\n\n\ncast\nthat is converted to another type\n\n\n+\nthat is shifted over a given vector\n\n\n*\nthat is multiplied by a scalar or matrix"
  },
  {
    "objectID": "slides/11-slides.html#binary-transformers-1",
    "href": "slides/11-slides.html#binary-transformers-1",
    "title": "Operations With Vector Data I",
    "section": "Binary Transformers",
    "text": "Binary Transformers\n\n\n\n\n\n\n\n\n\nfunction\nreturns\ninfix operator\n\n\n\n\nintersection\nthe overlapping geometries for pair of geometries\n&\n\n\nunion\nthe combination of the geometries; removes internal boundaries and duplicate points, nodes or line pieces\n|\n\n\ndifference\nthe geometries of the first after removing the overlap with the second geometry\n/\n\n\nsym_difference\nthe combinations of the geometries after removing where they intersect; the negation (opposite) of intersection\n%/%\n\n\ncrop\ncrop an sf object to a specific rectangle"
  },
  {
    "objectID": "slides/11-slides.html#binary-transformers-2",
    "href": "slides/11-slides.html#binary-transformers-2",
    "title": "Operations With Vector Data I",
    "section": "Binary Transformers",
    "text": "Binary Transformers"
  },
  {
    "objectID": "slides/11-slides.html#common-uses-of-binary-transformers",
    "href": "slides/11-slides.html#common-uses-of-binary-transformers",
    "title": "Operations With Vector Data I",
    "section": "Common Uses of Binary Transformers",
    "text": "Common Uses of Binary Transformers\n\nRelating partially overlapping datasets to each other\nReducing the extent of vector objects"
  },
  {
    "objectID": "slides/11-slides.html#n-ary-transformers",
    "href": "slides/11-slides.html#n-ary-transformers",
    "title": "Operations With Vector Data I",
    "section": "N-ary Transformers",
    "text": "N-ary Transformers\n\nSimilar to Binary (except st_crop)\nunion can be applied to a set of geometries to return its geometrical union\nintersection and difference take a single argument, but operate (sequentially) on all pairs, triples, quadruples, etc."
  },
  {
    "objectID": "slides/13-slides.html#objectives",
    "href": "slides/13-slides.html#objectives",
    "title": "Operations on Raster Data I",
    "section": "Objectives",
    "text": "Objectives\n\nBy the end of today, you should be able to:\n\nAlign rasters for spatial processing\nAdjust the resolution of raster data\nCombine (or reduce) rasters to match the extent of your analysis"
  },
  {
    "objectID": "slides/13-slides.html#projecting-raster-data",
    "href": "slides/13-slides.html#projecting-raster-data",
    "title": "Operations on Raster Data I",
    "section": "Projecting raster data",
    "text": "Projecting raster data\n\n\n\n\nTransformation from lat/long to planar CRS involves some loss of precision\nNew cell values estimated using overlap with original cells\nInterpolation for continuous data, nearest neighbor for categorical data\nEqual-area projections are preferred; especially for large areas\n\n\n\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nr &lt;- rast(xmin=-110, xmax=-90, ymin=40, ymax=60, ncols=40, nrows=40)\nvalues(r) &lt;- 1:ncell(r)\nplot(r)"
  },
  {
    "objectID": "slides/13-slides.html#projecting-raster-data-1",
    "href": "slides/13-slides.html#projecting-raster-data-1",
    "title": "Operations on Raster Data I",
    "section": "Projecting raster data",
    "text": "Projecting raster data\n\n\n\nsimple method; alignment not guaranteed\n\n\nnewcrs &lt;- \"+proj=robin +datum=WGS84\"\npr1 &lt;- terra::project(r, newcrs)\nplot(pr1)\n\n\n\n\n\n\nproviding a template to ensure alignment"
  },
  {
    "objectID": "slides/13-slides.html#aligning-data-resample",
    "href": "slides/13-slides.html#aligning-data-resample",
    "title": "Operations on Raster Data I",
    "section": "Aligning Data: resample",
    "text": "Aligning Data: resample\n\nr &lt;- rast(nrow=3, ncol=3, xmin=0, xmax=10, ymin=0, ymax=10)\nvalues(r) &lt;- 1:ncell(r)\ns &lt;- rast(nrow=25, ncol=30, xmin=1, xmax=11, ymin=-1, ymax=11)\nx &lt;- resample(r, s, method=\"bilinear\")"
  },
  {
    "objectID": "slides/13-slides.html#downscaling-and-upscaling",
    "href": "slides/13-slides.html#downscaling-and-upscaling",
    "title": "Operations on Raster Data I",
    "section": "Downscaling and Upscaling",
    "text": "Downscaling and Upscaling\n\nAligning data for later analysis\nRemembering scale\nThinking about support"
  },
  {
    "objectID": "slides/13-slides.html#changing-resolutions",
    "href": "slides/13-slides.html#changing-resolutions",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions",
    "text": "Changing resolutions\n\naggregate, disaggregate, resample allow changes in cell size\naggregate requires a function (e.g., mean() or min()) to determine what to do with the grouped values\nresample allows changes in cell size and shifting of cell centers (slower)"
  },
  {
    "objectID": "slides/13-slides.html#changing-resolutions-aggregate",
    "href": "slides/13-slides.html#changing-resolutions-aggregate",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions: aggregate",
    "text": "Changing resolutions: aggregate\n\n\n\nr &lt;- rast()\nr\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \n\nvalues(r) &lt;- 1:ncell(r)\nplot(r)\n\n\n\n\n\n\nra &lt;- aggregate(r, 20)\nra\n\nclass       : SpatRaster \ndimensions  : 9, 18, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource(s)   : memory\nname        :   lyr.1 \nmin value   :  3430.5 \nmax value   : 61370.5 \n\nplot(ra)"
  },
  {
    "objectID": "slides/13-slides.html#changing-resolutions-disagg",
    "href": "slides/13-slides.html#changing-resolutions-disagg",
    "title": "Operations on Raster Data I",
    "section": "Changing resolutions: disagg",
    "text": "Changing resolutions: disagg\n\n\n\nra &lt;- aggregate(r, 20)\nplot(ra)\n\n\n\n\n\n\nrd &lt;- disagg(r, 20)\n\n\n|---------|---------|---------|---------|\n=====\n                                          \n\nrd\n\nclass       : SpatRaster \ndimensions  : 3600, 7200, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : spat_4QyAlALziT1WmUe_97640.tif \nname        : lyr.1 \nmin value   :     1 \nmax value   : 64800 \n\nplot(rd)"
  },
  {
    "objectID": "slides/13-slides.html#dealing-with-different-extents",
    "href": "slides/13-slides.html#dealing-with-different-extents",
    "title": "Operations on Raster Data I",
    "section": "Dealing with Different Extents",
    "text": "Dealing with Different Extents\n\n\n\nRaster extents often larger than our analysis\nReducing memory and computational resources\nMaking attractive maps"
  },
  {
    "objectID": "slides/13-slides.html#using-terracrop",
    "href": "slides/13-slides.html#using-terracrop",
    "title": "Operations on Raster Data I",
    "section": "Using terra::crop()",
    "text": "Using terra::crop()\n\n\n\n\n\n\n\n\n\n\nCoordinate Reference System must be the same for both objects\nCrop is based on the (converted) SpatExtent of the 2nd object\nsnap describes how y will be aligned to the raster\nReturns all data within the extent"
  },
  {
    "objectID": "slides/13-slides.html#using-terracrop-1",
    "href": "slides/13-slides.html#using-terracrop-1",
    "title": "Operations on Raster Data I",
    "section": "Using terra::crop()",
    "text": "Using terra::crop()\n\n\n\n\nlibrary(sf)\nlibrary(terra)\nlibrary(spDataLarge)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nzion = read_sf(system.file(\"vector/zion.gpkg\", package = \"spDataLarge\"))\nzion = st_transform(zion, crs(srtm))\n\ncrs(srtm) == crs(zion)\n\n[1] TRUE\n\nsrtm.crop &lt;- crop(x=srtm, y=zion, snap=\"near\")"
  },
  {
    "objectID": "slides/13-slides.html#using-mask",
    "href": "slides/13-slides.html#using-mask",
    "title": "Operations on Raster Data I",
    "section": "Using mask()",
    "text": "Using mask()\n\n\nOften want to get rid of all values outside of vector\nCan set mask=TRUE in crop() (y must be SpatVector)\nOr use mask()\n\n\n\n\n\nsrtm.crop.msk &lt;- crop(x=srtm, y=vect(zion), snap=\"near\", mask=TRUE)\nplot(srtm.crop.msk)\n\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion))\nplot(srtm.msk)"
  },
  {
    "objectID": "slides/13-slides.html#using-mask-1",
    "href": "slides/13-slides.html#using-mask-1",
    "title": "Operations on Raster Data I",
    "section": "Using mask()",
    "text": "Using mask()\n\n\nAllows more control over what the mask does\nCan set maskvalues and updatevalues to change the resulting raster\nCan also use inverse to mask out the vector\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), updatevalue=-1000)\nplot(srtm.msk)\n\n\n\n\n\n\nsrtm.msk &lt;- mask(srtm.crop, vect(zion), inverse=TRUE, updatevalue=0)\nplot(srtm.msk)"
  },
  {
    "objectID": "slides/13-slides.html#extending-boundaries",
    "href": "slides/13-slides.html#extending-boundaries",
    "title": "Operations on Raster Data I",
    "section": "Extending boundaries",
    "text": "Extending boundaries\n\n\nVector slightly larger than raster\nEspecially when using buffered datasets\nCan use extend\nNot exact; depends on snap()\n\n\n\n\n\nzion.buff &lt;-  zion %&gt;% \n  st_buffer(., 10000)\nsrtm.ext &lt;- extend(srtm, vect(zion.buff))\next(srtm.ext)\n\nSpatExtent : -113.343749879444, -112.74791654615, 37.0479167631968, 37.5979167631601 (xmin, xmax, ymin, ymax)\n\next(vect(zion.buff))\n\nSpatExtent : -113.343652923976, -112.747986193365, 37.0477357596604, 37.5977812137969 (xmin, xmax, ymin, ymax)"
  },
  {
    "objectID": "slides/15-slides.html#objectives",
    "href": "slides/15-slides.html#objectives",
    "title": "Combining Tabular and Spatial Data",
    "section": "Objectives",
    "text": "Objectives\nBy the end of today, you should be able to:\n\nDefine spatial analysis\nDescribe the steps in planning a spatial analysis\nUnderstand the structure of relational databases\nUse attributes and topology to subset data\nGenerate new features using geographic data\nJoin data based on attributes and location"
  },
  {
    "objectID": "slides/15-slides.html#what-is-spatial-analysis-1",
    "href": "slides/15-slides.html#what-is-spatial-analysis-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "What is spatial analysis?",
    "text": "What is spatial analysis?\n\n“The process of examining the locations, attributes, and relationships of features in spatial data through overlay and other analytical techniques in order to address a question or gain useful knowledge. Spatial analysis extracts or creates new information from spatial data”.\n\n— ESRI Dictionary"
  },
  {
    "objectID": "slides/15-slides.html#what-is-spatial-analysis-2",
    "href": "slides/15-slides.html#what-is-spatial-analysis-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "What is spatial analysis?",
    "text": "What is spatial analysis?\n\n\n\nThe process of turning maps into information\nAny- or everything we do with GIS\nThe use of computational and statistical algorithms to understand the relations between things that co-occur in space.\n\n\n\n\n\nJohn Snow’s cholera outbreak map"
  },
  {
    "objectID": "slides/15-slides.html#common-goals-for-spatial-analysis",
    "href": "slides/15-slides.html#common-goals-for-spatial-analysis",
    "title": "Combining Tabular and Spatial Data",
    "section": "Common goals for spatial analysis",
    "text": "Common goals for spatial analysis\n\n\n\n\n\ncourtesy of NatureServe\n\n\n\n\nDescribe and visualize locations or events\nQuantify patterns\nCharacterize ‘suitability’\nDetermine (statistical) relations"
  },
  {
    "objectID": "slides/15-slides.html#common-pitfalls-of-spatial-analysis",
    "href": "slides/15-slides.html#common-pitfalls-of-spatial-analysis",
    "title": "Combining Tabular and Spatial Data",
    "section": "Common pitfalls of spatial analysis",
    "text": "Common pitfalls of spatial analysis\n\nLocational Fallacy: Error due to the spatial characterization chosen for elements of study\nAtomic Fallacy: Applying conclusions from individuals to entire spatial units\nEcological Fallacy: Applying conclusions from aggregated information to individuals\n\n\n\nSpatial analysis is an inherently complex endeavor and one that is advancing rapidly. So-called “best practices” for addressing many of these issues are still being developed and debated. This doesn’t mean you shouldn’t do spatial analysis, but you should keep these things in mind as you design, implement, and interpret your analyses"
  },
  {
    "objectID": "slides/15-slides.html#workflows-for-spatial-analysis-1",
    "href": "slides/15-slides.html#workflows-for-spatial-analysis-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Workflows for spatial analysis",
    "text": "Workflows for spatial analysis\n\n\n\nAcquisition (not really a focus, but see Resources)\nGeoprocessing\nAnalysis\nVisualization\n\n\n\n\n\ncourtesy of University of Illinois"
  },
  {
    "objectID": "slides/15-slides.html#geoprocessing",
    "href": "slides/15-slides.html#geoprocessing",
    "title": "Combining Tabular and Spatial Data",
    "section": "Geoprocessing",
    "text": "Geoprocessing\nManipulation of data for subsequent use\n\nAlignment\nData cleaning and transformation\nCombination of multiple datasets\nSelection and subsetting"
  },
  {
    "objectID": "slides/15-slides.html#databases-and-attributes-1",
    "href": "slides/15-slides.html#databases-and-attributes-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Databases and Attributes",
    "text": "Databases and Attributes\n\n\n\n\n\ncourtesy of Giscommons\n\n\n\n\n\nAttributes: Information that further describes a spatial feature\nAttributes → predictors for analysis\nLast week focus on thematic relations between datasets\n\nShared ‘keys’ help define linkages between objects\n\nSometimes we are interested in attributes that describe location (overlaps, contains, distance)\nSometimes we want to join based on location rather than thematic connections\n\nMust have the same CRS"
  },
  {
    "objectID": "slides/15-slides.html#databases-and-attributes-2",
    "href": "slides/15-slides.html#databases-and-attributes-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Databases and attributes",
    "text": "Databases and attributes\n\n\n\n\n\ncourtesy of Giscommons\n\n\n\n\n\nPrevious focus has been largely on location\nGeographic data often also includes non-spatial data\nAttributes: Non-spatial information that further describes a spatial feature\nTypically stored in tables where each row represents a spatial feature\n\nWide vs. long format"
  },
  {
    "objectID": "slides/15-slides.html#common-attribute-operations",
    "href": "slides/15-slides.html#common-attribute-operations",
    "title": "Combining Tabular and Spatial Data",
    "section": "Common attribute operations",
    "text": "Common attribute operations\n\nsf designed to work with tidyverse\nAllows use of dplyr data manipulation verbs (e.g. filter, select, slice)\nCan use scales package for units\nAlso allows %&gt;% to chain together multiple steps\ngeometries are “sticky”"
  },
  {
    "objectID": "slides/15-slides.html#subsetting-by-field",
    "href": "slides/15-slides.html#subsetting-by-field",
    "title": "Combining Tabular and Spatial Data",
    "section": "Subsetting by Field",
    "text": "Subsetting by Field\n\nFields contain individual attributes\nSelecting fields\n\n\n\n\ncolnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\nhead(world)[,1:3] %&gt;% \n  st_drop_geometry()\n\n# A tibble: 6 × 3\n  iso_a2 name_long      continent    \n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;        \n1 FJ     Fiji           Oceania      \n2 TZ     Tanzania       Africa       \n3 EH     Western Sahara Africa       \n4 CA     Canada         North America\n5 US     United States  North America\n6 KZ     Kazakhstan     Asia         \n\n\n\n\nworld %&gt;%\n  dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.) \n\n# A tibble: 6 × 2\n  name_long      continent    \n  &lt;chr&gt;          &lt;chr&gt;        \n1 Fiji           Oceania      \n2 Tanzania       Africa       \n3 Western Sahara Africa       \n4 Canada         North America\n5 United States  North America\n6 Kazakhstan     Asia"
  },
  {
    "objectID": "slides/15-slides.html#subsetting-by-features",
    "href": "slides/15-slides.html#subsetting-by-features",
    "title": "Combining Tabular and Spatial Data",
    "section": "Subsetting by Features",
    "text": "Subsetting by Features\n\nFeatures refer to the individual observations in the dataset\nSelecting features\n\n\n\n\nhead(world)[1:3, 1:3] %&gt;% \n  st_drop_geometry()\n\n# A tibble: 3 × 3\n  iso_a2 name_long      continent\n* &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;    \n1 FJ     Fiji           Oceania  \n2 TZ     Tanzania       Africa   \n3 EH     Western Sahara Africa   \n\n\n\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n    dplyr::select(name_long, continent) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n# A tibble: 6 × 2\n  name_long   continent\n  &lt;chr&gt;       &lt;chr&gt;    \n1 Kazakhstan  Asia     \n2 Uzbekistan  Asia     \n3 Indonesia   Asia     \n4 Timor-Leste Asia     \n5 Israel      Asia     \n6 Lebanon     Asia"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting",
    "href": "slides/15-slides.html#topological-subsetting",
    "title": "Combining Tabular and Spatial Data",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\n\nTopological relations describe the spatial relationships between objects\nWe can use the overlap (or not) of vector data to subset the data based on topology\nNeed valid geometries\nEasiest way is to use [ notation, but also most restrictive\n\n\n\n\n\ncanterbury = nz  %&gt;% filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting-1",
    "href": "slides/15-slides.html#topological-subsetting-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\n\n\n\nLots of verbs in sf for doing this (e.g., st_intersects, st_contains, st_touches)\nsee ?geos_binary_pred for a full list\nCreates an implicit attribute (the records in x that are “in” y)\n\n\n\nUsing sparse=TRUE\n\nco = filter(nz, grepl(\"Canter|Otag\", Name))\nst_intersects(nz_height, co, \n              sparse = TRUE)[1:3] \n\n[[1]]\ninteger(0)\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 2\n\nlengths(st_intersects(nz_height, \n                      co, sparse = TRUE))[1:3] &gt; 0\n\n[1] FALSE  TRUE  TRUE"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting-2",
    "href": "slides/15-slides.html#topological-subsetting-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\nThe sparse option controls how the results are returned\nWe can then find out if one or more elements satisfies the criteria\n\nUsing sparse=FALSE\n\nst_intersects(nz_height, co, sparse = FALSE)[1:3,] \n\n      [,1]  [,2]\n[1,] FALSE FALSE\n[2,] FALSE  TRUE\n[3,] FALSE  TRUE\n\napply(st_intersects(nz_height, co, sparse = FALSE), 1,any)[1:3]\n\n[1] FALSE  TRUE  TRUE"
  },
  {
    "objectID": "slides/15-slides.html#topological-subsetting-3",
    "href": "slides/15-slides.html#topological-subsetting-3",
    "title": "Combining Tabular and Spatial Data",
    "section": "Topological Subsetting",
    "text": "Topological Subsetting\n\n\n\ncanterbury_height3 = nz_height %&gt;%\n  filter(st_intersects(x = ., y = canterbury, sparse = FALSE))"
  },
  {
    "objectID": "slides/15-slides.html#revisiting-the-tidyverse",
    "href": "slides/15-slides.html#revisiting-the-tidyverse",
    "title": "Combining Tabular and Spatial Data",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\nCreating new fields\n\n\nworld %&gt;%\n  filter(continent == \"Asia\") %&gt;% \n    dplyr::select(name_long, continent, pop, gdpPercap ,area_km2) %&gt;%\n  mutate(., dens = pop/area_km2,\n         totGDP = gdpPercap * pop) %&gt;%\n  st_drop_geometry() %&gt;% \n  head(.)\n\n# A tibble: 6 × 7\n  name_long   continent       pop gdpPercap area_km2   dens  totGDP\n  &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Kazakhstan  Asia       17288285    23587. 2729811.   6.33 4.08e11\n2 Uzbekistan  Asia       30757700     5371.  461410.  66.7  1.65e11\n3 Indonesia   Asia      255131116    10003. 1819251. 140.   2.55e12\n4 Timor-Leste Asia        1212814     6263.   14715.  82.4  7.60e 9\n5 Israel      Asia        8215700    31702.   22991. 357.   2.60e11\n6 Lebanon     Asia        5603279    13831.   10099. 555.   7.75e10"
  },
  {
    "objectID": "slides/15-slides.html#revisiting-the-tidyverse-1",
    "href": "slides/15-slides.html#revisiting-the-tidyverse-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\nCreating new fields"
  },
  {
    "objectID": "slides/15-slides.html#revisiting-the-tidyverse-2",
    "href": "slides/15-slides.html#revisiting-the-tidyverse-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Revisiting the tidyverse",
    "text": "Revisiting the tidyverse\n\nAggregating data\n\n\n\n\nworld %&gt;%\n  st_drop_geometry(.) %&gt;% \n  group_by(continent) %&gt;%\n  summarize(pop = sum(pop, na.rm = TRUE))\n\n# A tibble: 8 × 2\n  continent                      pop\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 Africa                  1154946633\n2 Antarctica                       0\n3 Asia                    4311408059\n4 Europe                   669036256\n5 North America            565028684\n6 Oceania                   37757833\n7 Seven seas (open ocean)          0\n8 South America            412060811"
  },
  {
    "objectID": "slides/15-slides.html#attributes-based-on-geometry-and-location-measures",
    "href": "slides/15-slides.html#attributes-based-on-geometry-and-location-measures",
    "title": "Combining Tabular and Spatial Data",
    "section": "Attributes based on geometry and location (measures)",
    "text": "Attributes based on geometry and location (measures)\n\nAttributes like area and length can be useful for a number of analyses\n\nEstimates of ‘effort’ in sampling designs\nOffsets for modeling rates (e.g., Poisson regression)\n\nNeed to assign the result of the function to a column in data frame (e.g., $, mutate, and summarize)\nOften useful to test before assigning"
  },
  {
    "objectID": "slides/15-slides.html#estimating-area",
    "href": "slides/15-slides.html#estimating-area",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating area",
    "text": "Estimating area\n\n\n\n\nsf bases area (and length) calculations on the map units of the CRS\nthe units library allows conversion into a variety of units\n\n\n\n\nnz.sf &lt;- nz %&gt;% \n  mutate(area = st_area(nz))\nhead(nz.sf$area, 3)\n\nUnits: [m^2]\n[1] 12890576439  4911565037 24588819863\n\n\n\nnz.sf$areakm &lt;- units::set_units(st_area(nz), km^2)\nhead(nz.sf$areakm, 3)\n\nUnits: [km^2]\n[1] 12890.576  4911.565 24588.820"
  },
  {
    "objectID": "slides/15-slides.html#estimating-density-in-polygons",
    "href": "slides/15-slides.html#estimating-density-in-polygons",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating Density in Polygons",
    "text": "Estimating Density in Polygons\n\n\n\n\n\n\n\n\n\n\nCreating new features based on the frequency of occurrence\nClarifying graphics\nUnderlies quadrat sampling for point patterns\nTwo steps: count and area"
  },
  {
    "objectID": "slides/15-slides.html#estimating-density-in-polygons-1",
    "href": "slides/15-slides.html#estimating-density-in-polygons-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating Density in Polygons",
    "text": "Estimating Density in Polygons\n\n\n\n\n\n\n\n\n\nnz.df &lt;- nz %&gt;% \nmutate(counts = lengths(st_intersects(., random_nz)),\n       area = st_area(nz),\n       density = counts/area)\nhead(st_drop_geometry(nz.df[,7:10]))\n\n  counts              area              density\n1     17 12890576439 [m^2] 1.318793e-09 [1/m^2]\n2      6  4911565037 [m^2] 1.221607e-09 [1/m^2]\n3     37 24588819863 [m^2] 1.504749e-09 [1/m^2]\n4     22 12271015945 [m^2] 1.792843e-09 [1/m^2]\n5     12  8364554416 [m^2] 1.434625e-09 [1/m^2]\n6     19 14242517871 [m^2] 1.334034e-09 [1/m^2]"
  },
  {
    "objectID": "slides/15-slides.html#estimating-density-in-polygons-2",
    "href": "slides/15-slides.html#estimating-density-in-polygons-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating Density in Polygons",
    "text": "Estimating Density in Polygons"
  },
  {
    "objectID": "slides/15-slides.html#estimating-distance",
    "href": "slides/15-slides.html#estimating-distance",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating Distance",
    "text": "Estimating Distance\n\nAs a covariate\nFor use in covariance matrices\nAs a means of assigning connections in networks"
  },
  {
    "objectID": "slides/15-slides.html#estimating-single-point-distance",
    "href": "slides/15-slides.html#estimating-single-point-distance",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating Single Point Distance",
    "text": "Estimating Single Point Distance\n\n\n\nst_distance returns distances between all features in x and all features in y\nOne-to-One relationship requires choosing a single point for y"
  },
  {
    "objectID": "slides/15-slides.html#estimating-single-point-distance-1",
    "href": "slides/15-slides.html#estimating-single-point-distance-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating Single Point Distance",
    "text": "Estimating Single Point Distance\n\nSubsetting y into a single feature\n\n\n\n\ncanterbury = nz %&gt;% filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\nco = filter(nz, grepl(\"Canter|Otag\", Name))\nst_distance(nz_height[1:3, ], co)\n\nUnits: [m]\n          [,1]     [,2]\n[1,] 123537.16 15497.72\n[2,]  94282.77     0.00\n[3,]  93018.56     0.00"
  },
  {
    "objectID": "slides/15-slides.html#estimating-single-point-distance-2",
    "href": "slides/15-slides.html#estimating-single-point-distance-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Estimating Single Point Distance",
    "text": "Estimating Single Point Distance\n\nUsing nearest neighbor distances\n\n\n\n\nua &lt;- urban_areas(cb = FALSE, progress_bar = FALSE) %&gt;% \n  filter(., UATYP10 == \"U\") %&gt;% \n  filter(., str_detect(NAME10, \"ID\")) %&gt;% \n  st_transform(., crs=2163)\n\n#get index of nearest ID city\nnearest &lt;-  st_nearest_feature(ua)\n#estimate distance\n(dist = st_distance(ua, ua[nearest,], by_element=TRUE))\n\nUnits: [m]\n[1]  61386.444  61386.444   1646.182   1646.182 136908.183 136908.183"
  },
  {
    "objectID": "slides/15-slides.html#joining-aspatial-data-1",
    "href": "slides/15-slides.html#joining-aspatial-data-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Joining (a)spatial data",
    "text": "Joining (a)spatial data\n\n\n\nRequires a “key” field\nMultiple outcomes possible\nThink about your final data form"
  },
  {
    "objectID": "slides/15-slides.html#left-join",
    "href": "slides/15-slides.html#left-join",
    "title": "Combining Tabular and Spatial Data",
    "section": "Left Join",
    "text": "Left Join\n\nUseful for adding other attributes not in your spatial data\nReturns all of the records in x attributed with y\nPay attention to the number of rows!"
  },
  {
    "objectID": "slides/15-slides.html#left-join-1",
    "href": "slides/15-slides.html#left-join-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Left Join",
    "text": "Left Join"
  },
  {
    "objectID": "slides/15-slides.html#left-join-2",
    "href": "slides/15-slides.html#left-join-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Left Join",
    "text": "Left Join\n\n\n\nhead(coffee_data)\n\n# A tibble: 6 × 3\n  name_long                coffee_production_2016 coffee_production_2017\n  &lt;chr&gt;                                     &lt;int&gt;                  &lt;int&gt;\n1 Angola                                       NA                     NA\n2 Bolivia                                       3                      4\n3 Brazil                                     3277                   2786\n4 Burundi                                      37                     38\n5 Cameroon                                      8                      6\n6 Central African Republic                     NA                     NA\n\n\n\n\nworld_coffee = left_join(world, coffee_data)\nnrow(world_coffee)\n\n[1] 177"
  },
  {
    "objectID": "slides/15-slides.html#left-join-3",
    "href": "slides/15-slides.html#left-join-3",
    "title": "Combining Tabular and Spatial Data",
    "section": "Left Join",
    "text": "Left Join"
  },
  {
    "objectID": "slides/15-slides.html#inner-join",
    "href": "slides/15-slides.html#inner-join",
    "title": "Combining Tabular and Spatial Data",
    "section": "Inner Join",
    "text": "Inner Join\n\nUseful for subsetting to “complete” records\nReturns all of the records in x with matching y\nPay attention to the number of rows!"
  },
  {
    "objectID": "slides/15-slides.html#inner-join-1",
    "href": "slides/15-slides.html#inner-join-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Inner Join",
    "text": "Inner Join"
  },
  {
    "objectID": "slides/15-slides.html#inner-join-2",
    "href": "slides/15-slides.html#inner-join-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Inner Join",
    "text": "Inner Join\n\n\n\nworld_coffee_inner = inner_join(world, coffee_data)\nnrow(world_coffee_inner)\n\n[1] 45\n\n\n\n\nsetdiff(coffee_data$name_long, world$name_long)\n\n[1] \"Congo, Dem. Rep. of\" \"Others\""
  },
  {
    "objectID": "slides/15-slides.html#inner-join-3",
    "href": "slides/15-slides.html#inner-join-3",
    "title": "Combining Tabular and Spatial Data",
    "section": "Inner Join",
    "text": "Inner Join"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-1",
    "href": "slides/15-slides.html#spatial-joins-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\nsf package provides st_join for vectors\nAllows joins based on the predicates (st_intersects, st_touches, st_within_distance, etc.)\nDefault is a left join"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-2",
    "href": "slides/15-slides.html#spatial-joins-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\n\n\nset.seed(2018)\n(bb = st_bbox(world)) # the world's bounds\n\n      xmin       ymin       xmax       ymax \n-180.00000  -89.90000  179.99999   83.64513 \n\n#&gt;   xmin   ymin   xmax   ymax \n#&gt; -180.0  -89.9  180.0   83.6\nrandom_df = data.frame(\n  x = runif(n = 10, min = bb[1], max = bb[3]),\n  y = runif(n = 10, min = bb[2], max = bb[4])\n)\nrandom_points = random_df |&gt; \n  st_as_sf(coords = c(\"x\", \"y\")) |&gt; # set coordinates\n  st_set_crs(\"EPSG:4326\") # set geographic CRS\n\nrandom_joined = st_join(random_points, world[\"name_long\"])"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-3",
    "href": "slides/15-slides.html#spatial-joins-3",
    "title": "Combining Tabular and Spatial Data",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\nSometimes we may want to be less restrictive\nJust because objects don’t touch doesn’t mean they don’t relate to each other\nCan use predicates in st_join\nRemember that default is left_join (so the number of records can grow if multiple matches)"
  },
  {
    "objectID": "slides/15-slides.html#spatial-joins-4",
    "href": "slides/15-slides.html#spatial-joins-4",
    "title": "Combining Tabular and Spatial Data",
    "section": "Spatial Joins",
    "text": "Spatial Joins\n\n\n\nany(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE))\n\n[1] FALSE\n\nz = st_join(cycle_hire, cycle_hire_osm, st_is_within_distance, dist = 20)\nnrow(cycle_hire)\n\n[1] 742\n\nnrow(z)\n\n[1] 762"
  },
  {
    "objectID": "slides/15-slides.html#extending-joins-1",
    "href": "slides/15-slides.html#extending-joins-1",
    "title": "Combining Tabular and Spatial Data",
    "section": "Extending Joins",
    "text": "Extending Joins\n\n\nSometimes we are interested in analyzing locations that contain the overlap between two vectors\n\nHow much of home range a occurs on soil type b\nHow much of each Census tract is contained with a service provision area?\n\nst_intersection, st_union, and st_difference return new geometries that we can use as records in our spatial database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintersect_pct &lt;- st_intersection(nc, tr_buff) %&gt;% \n   mutate(intersect_area = st_area(.)) %&gt;%   # create new column with shape area\n   dplyr::select(NAME, intersect_area) %&gt;%   # only select columns needed to merge\n   st_drop_geometry()\n\nnc &lt;- mutate(nc, county_area = st_area(nc))\n\n# Merge by county name\nnc &lt;- merge(nc, intersect_pct, by = \"NAME\", all.x = TRUE)\n\n# Calculate coverage\nnc &lt;- nc %&gt;% \n   mutate(coverage = as.numeric(intersect_area/county_area))"
  },
  {
    "objectID": "slides/15-slides.html#extending-joins-2",
    "href": "slides/15-slides.html#extending-joins-2",
    "title": "Combining Tabular and Spatial Data",
    "section": "Extending Joins",
    "text": "Extending Joins"
  },
  {
    "objectID": "slides/17-slides.html#description-vs.-process",
    "href": "slides/17-slides.html#description-vs.-process",
    "title": "Interpolation and Autocorrelation",
    "section": "Description vs. process?",
    "text": "Description vs. process?\n\n\n\nVizualization and the detection of patterns\nThe challenge of geographic data\nImplications for analysis\n\n\n\n\n\nInequality in the United States: Quintiles of Gini Index by County: 2006–2010. The greater the Gini index, the more unequal a county’s income distribution is."
  },
  {
    "objectID": "slides/17-slides.html#patterns-as-realizations-of-spatial-processes",
    "href": "slides/17-slides.html#patterns-as-realizations-of-spatial-processes",
    "title": "Interpolation and Autocorrelation",
    "section": "Patterns as realizations of spatial processes",
    "text": "Patterns as realizations of spatial processes\n\nA spatial process is a description of how a spatial pattern might be generated\nGenerative models\nAn observed pattern as a possible realization of an hypothesized process"
  },
  {
    "objectID": "slides/17-slides.html#deterministic-vs.-stochastic-processes",
    "href": "slides/17-slides.html#deterministic-vs.-stochastic-processes",
    "title": "Interpolation and Autocorrelation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\n\n\nDeterministic processes: always produces the same outcome\n\n\\[\nz = 2x + 3y\n\\]\n\nResults in a spatially continuous field\n\n\nx &lt;- rast(nrows = 10, ncols=10, xmin = 0, xmax=10, ymin = 0, ymax=10)\nvalues(x) &lt;- 1\nz &lt;- x\nvalues(z) &lt;- 2 * crds(x)[,1] + 3*crds(x)[,2]"
  },
  {
    "objectID": "slides/17-slides.html#deterministic-vs.-stochastic-processes-1",
    "href": "slides/17-slides.html#deterministic-vs.-stochastic-processes-1",
    "title": "Interpolation and Autocorrelation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\n\n\n\nStochastic processes: variation makes each realization difficult to predict\n\n\\[\nz = 2x + 3y + d\n\\]\n\nThe process is random, not the result (!!)\nMeasurement error makes deterministic processes appear stochastic\n\n\n\n\nx &lt;- rast(nrows = 10, ncols=10, xmin = 0, xmax=10, ymin = 0, ymax=10)\nvalues(x) &lt;- 1\nfun &lt;- function(z){\na &lt;- z\nd &lt;- runif(ncell(z), -50,50)\nvalues(a) &lt;- 2 * crds(x)[,1] + 3*crds(x)[,2] + d\nreturn(a)\n}\n\nb &lt;- replicate(n=6, fun(z=x), simplify=FALSE)\nd &lt;- do.call(c, b)"
  },
  {
    "objectID": "slides/17-slides.html#deterministic-vs.-stochastic-processes-2",
    "href": "slides/17-slides.html#deterministic-vs.-stochastic-processes-2",
    "title": "Interpolation and Autocorrelation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes"
  },
  {
    "objectID": "slides/17-slides.html#expected-values-and-hypothesis-testing",
    "href": "slides/17-slides.html#expected-values-and-hypothesis-testing",
    "title": "Interpolation and Autocorrelation",
    "section": "Expected values and hypothesis testing",
    "text": "Expected values and hypothesis testing\n\n\n\n\nConsidering each outcome as the realization of a process allows us to generate expected values\nThe simplest spatial process is Completely Spatial Random (CSR) process\nFirst Order effects: any event has an equal probability of occurring in a location\nSecond Order effects: the location of one event is independent of the other events\n\n\n\n\n\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/17-slides.html#generating-expectations-for-csr",
    "href": "slides/17-slides.html#generating-expectations-for-csr",
    "title": "Interpolation and Autocorrelation",
    "section": "Generating expectations for CSR",
    "text": "Generating expectations for CSR\n\n\n\n\n\n\n\n\n\n\nWe can use quadrat counts to estimate the expected number of events in a given area\nThe probability of each possible count is given by:\n\n\\[\nP(n,k) = {n \\choose x}p^k(1-p)^{n-k}\n\\]\n\nGiven total coverage of quadrats, then \\(p=\\frac{\\frac{a}{x}}{a}\\) and\n\n\\[\n\\begin{equation}\nP(k,n,x) = {n \\choose k}\\bigg(\\frac{1}{x}\\bigg)^k\\bigg(\\frac{x-1}{x}\\bigg)^{n-k}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/17-slides.html#spatial-autocorrelation",
    "href": "slides/17-slides.html#spatial-autocorrelation",
    "title": "Interpolation and Autocorrelation",
    "section": "Spatial autocorrelation",
    "text": "Spatial autocorrelation\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/17-slides.html#one-measure-of-autocorrelation",
    "href": "slides/17-slides.html#one-measure-of-autocorrelation",
    "title": "Interpolation and Autocorrelation",
    "section": "(One) Measure of autocorrelation",
    "text": "(One) Measure of autocorrelation\n\n\n\nMoran’s I"
  },
  {
    "objectID": "slides/17-slides.html#morans-i-an-example",
    "href": "slides/17-slides.html#morans-i-an-example",
    "title": "Interpolation and Autocorrelation",
    "section": "Moran’s I: An example",
    "text": "Moran’s I: An example\n\n\n\n\nUse spdep package\nEstimate neighbors\nGenerate weighted average\n\n\nset.seed(2354)\n# Load the shapefile\ns &lt;- readRDS(url(\"https://github.com/mgimond/Data/raw/gh-pages/Exercises/fl_hr80.rds\"))\n\n# Define the neighbors (use queen case)\nnb &lt;- poly2nb(s, queen=TRUE)\n\n# Compute the neighboring average homicide rates\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n#estimate Moran's I\nmoran.test(s$HR80,lw, alternative=\"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  s$HR80  \nweights: lw    \n\nMoran I statistic standard deviate = 1.8891, p-value = 0.02944\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.136277593      -0.015151515       0.006425761"
  },
  {
    "objectID": "slides/17-slides.html#morans-i-an-example-1",
    "href": "slides/17-slides.html#morans-i-an-example-1",
    "title": "Interpolation and Autocorrelation",
    "section": "Moran’s I: An example",
    "text": "Moran’s I: An example\n\n\n\nM1 &lt;- moran.mc(s$HR80, lw, nsim=9999, alternative=\"greater\")\n\n\n\n# Display the resulting statistics\nM1\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  s$HR80 \nweights: lw  \nnumber of simulations + 1: 10000 \n\nstatistic = 0.13628, observed rank = 9575, p-value = 0.0425\nalternative hypothesis: greater"
  },
  {
    "objectID": "slides/17-slides.html#the-challenge-of-areal-data",
    "href": "slides/17-slides.html#the-challenge-of-areal-data",
    "title": "Interpolation and Autocorrelation",
    "section": "The challenge of areal data",
    "text": "The challenge of areal data\n\nSpatial autocorrelation threatens second order randomness\nAreal data means an infinite number of potential distances\nNeighbor matrices, \\(\\boldsymbol W\\), allow different characterizations"
  },
  {
    "objectID": "slides/17-slides.html#interpolation-1",
    "href": "slides/17-slides.html#interpolation-1",
    "title": "Interpolation and Autocorrelation",
    "section": "Interpolation",
    "text": "Interpolation\n\nGoal: estimate the value of \\(z\\) at new points in \\(\\mathbf{x_i}\\)\nMost useful for continuous values\nNearest-neighbor, Inverse Distance Weighting, Kriging"
  },
  {
    "objectID": "slides/17-slides.html#nearest-neighbor",
    "href": "slides/17-slides.html#nearest-neighbor",
    "title": "Interpolation and Autocorrelation",
    "section": "Nearest neighbor",
    "text": "Nearest neighbor\n\nfind \\(i\\) such that \\(| \\mathbf{x_i} - \\mathbf{x}|\\) is minimized\nThe estimate of \\(z\\) is \\(z_i\\)\n\n\n\n\n\ndata(meuse)\nr &lt;- rast(system.file(\"ex/meuse.tif\", package=\"terra\"))\nsfmeuse &lt;- st_as_sf(meuse, coords = c(\"x\", \"y\"), crs=crs(r))\nnodes &lt;- st_make_grid(sfmeuse,\n                      cellsize = 25,\n                      what = \"centers\")\n\ndist &lt;- distance(vect(nodes), vect(sfmeuse))\nnearest &lt;- apply(dist, 1, function(x) which(x == min(x)))\nzinc_nn &lt;- sfmeuse$zinc[nearest]\npreds &lt;- st_as_sf(nodes)\npreds$zn &lt;- zinc_nn\npreds &lt;- as(preds, \"Spatial\")\ngridded(preds) &lt;- TRUE\npreds.rast &lt;- rast(preds)\nr.resamp &lt;- resample(r, preds.rast)\npreds.rast &lt;- mask(preds.rast, r.resamp)"
  },
  {
    "objectID": "slides/17-slides.html#inverse-distance-weighting",
    "href": "slides/17-slides.html#inverse-distance-weighting",
    "title": "Interpolation and Autocorrelation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting\n\nWeight closer observations more heavily\n\n\\[\n\\begin{equation}\n\\hat{z}(\\mathbf{x}) = \\frac{\\sum_{i=1}w_iz_i}{\\sum_{i=1}w_i}\n\\end{equation}\n\\] where\n\\[\n\\begin{equation}\nw_i = | \\mathbf{x} - \\mathbf{x}_i |^{-\\alpha}\n\\end{equation}\n\\] and \\(\\alpha &gt; 0\\) (\\(\\alpha = 1\\) is inverse; \\(\\alpha = 2\\) is inverse square)"
  },
  {
    "objectID": "slides/17-slides.html#inverse-distance-weighting-1",
    "href": "slides/17-slides.html#inverse-distance-weighting-1",
    "title": "Interpolation and Autocorrelation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting\n\nterra::interpolate provides flexible interpolation methods\nUse the gstat package to develop the formula\n\n\nmgsf05 &lt;- gstat(id = \"zinc\", formula = zinc~1, data=sfmeuse,  nmax=7, set=list(idp = 0.5))\nmgsf2 &lt;- gstat(id = \"zinc\", formula = zinc~1, data=sfmeuse,  nmax=7, set=list(idp = 2))\ninterpolate_gstat &lt;- function(model, x, crs, ...) {\n    v &lt;- st_as_sf(x, coords=c(\"x\", \"y\"), crs=crs)\n    p &lt;- predict(model, v, ...)\n    as.data.frame(p)[,1:2]\n}\nzsf05 &lt;- interpolate(r, mgsf05, debug.level=0, fun=interpolate_gstat, crs=crs(r), index=1)\nzsf2 &lt;- interpolate(r, mgsf2, debug.level=0, fun=interpolate_gstat, crs=crs(r), index=1)"
  },
  {
    "objectID": "slides/17-slides.html#inverse-distance-weighting-2",
    "href": "slides/17-slides.html#inverse-distance-weighting-2",
    "title": "Interpolation and Autocorrelation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting"
  },
  {
    "objectID": "slides/17-slides.html#inverse-distance-weighting-3",
    "href": "slides/17-slides.html#inverse-distance-weighting-3",
    "title": "Interpolation and Autocorrelation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting"
  },
  {
    "objectID": "slides/17-slides.html#kriging",
    "href": "slides/17-slides.html#kriging",
    "title": "Interpolation and Autocorrelation",
    "section": "Kriging",
    "text": "Kriging\n\n\nPrevious methods predict \\(z\\) as a (weighted) function of distance\nTreat the observations as perfect (no error)\nIf we imagine that \\(z\\) is the outcome of some spatial process such that:\n\n\\[\n\\begin{equation}\nz(\\mathbf{x}) = \\mu(\\mathbf{x}) + \\epsilon(\\mathbf{x})\n\\end{equation}\n\\]\nthen any observed value of \\(z\\) is some function of the process (\\(\\mu(\\mathbf{x})\\)) and some error (\\(\\epsilon(\\mathbf{x})\\))\n\nKriging exploits autocorrelation in \\(\\epsilon(\\mathbf{x})\\) to identify the trend and interpolate accordingly"
  },
  {
    "objectID": "slides/17-slides.html#autocorrelation",
    "href": "slides/17-slides.html#autocorrelation",
    "title": "Interpolation and Autocorrelation",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nCorrelation the tendency for two variables to be related\nAutocorrelation the tendency for observations that are closer (in space or time) to be correlated\nPositive autocorrelation neighboring observations have \\(\\epsilon\\) with the same sign\nNegative autocorrelation neighboring observations have \\(\\epsilon\\) with a different sign (rare in geography)"
  },
  {
    "objectID": "slides/17-slides.html#ordinary-kriging",
    "href": "slides/17-slides.html#ordinary-kriging",
    "title": "Interpolation and Autocorrelation",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging\n\nAssumes that the deterministic part of the process (\\(\\mu(\\mathbf{x})\\)) is an unknown constant (\\(\\mu\\))\n\n\\[\n\\begin{equation}\nz(\\mathbf{x}) = \\mu + \\epsilon(\\mathbf{x})\n\\end{equation}\n\\] * Specified in call to variogram and gstat as y~1 (or some other constant)\n\nv &lt;- variogram(log(zinc)~1, ~x+y, data=meuse)\nmv &lt;- fit.variogram(v, vgm(1, \"Sph\", 300, 1))\ngOK &lt;- gstat(NULL, \"log.zinc\", log(zinc)~1, meuse, locations=~x+y, model=mv)\nOK &lt;- interpolate(r, gOK, debug.level=0)"
  },
  {
    "objectID": "slides/17-slides.html#ordinary-kriging-1",
    "href": "slides/17-slides.html#ordinary-kriging-1",
    "title": "Interpolation and Autocorrelation",
    "section": "Ordinary Kriging",
    "text": "Ordinary Kriging"
  },
  {
    "objectID": "slides/17-slides.html#universal-kriging",
    "href": "slides/17-slides.html#universal-kriging",
    "title": "Interpolation and Autocorrelation",
    "section": "Universal Kriging",
    "text": "Universal Kriging\n\nAssumes that the deterministic part of the process (\\(\\mu(\\mathbf{x})\\)) is now a function of the location \\(\\mathbf{x}\\)\nCould be the location or some other attribute\nNow y is a function of some aspect of x\n\n\nvu &lt;- variogram(log(zinc)~elev, ~x+y, data=meuse)\nmu &lt;- fit.variogram(vu, vgm(1, \"Sph\", 300, 1))\ngUK &lt;- gstat(NULL, \"log.zinc\", log(zinc)~elev, meuse, locations=~x+y, model=mu)\nnames(r) &lt;- \"elev\"\nUK &lt;- interpolate(r, gUK, debug.level=0)"
  },
  {
    "objectID": "slides/17-slides.html#universal-kriging-1",
    "href": "slides/17-slides.html#universal-kriging-1",
    "title": "Interpolation and Autocorrelation",
    "section": "Universal Kriging",
    "text": "Universal Kriging"
  },
  {
    "objectID": "slides/17-slides.html#universal-kriging-2",
    "href": "slides/17-slides.html#universal-kriging-2",
    "title": "Interpolation and Autocorrelation",
    "section": "Universal Kriging",
    "text": "Universal Kriging\n\nvu &lt;- variogram(log(zinc)~x + x^2 + y + y^2, ~x+y, data=meuse)\nmu &lt;- fit.variogram(vu, vgm(1, \"Sph\", 300, 1))\ngUK &lt;- gstat(NULL, \"log.zinc\", log(zinc)~x + x^2 + y + y^2, meuse, locations=~x+y, model=mu)\nnames(r) &lt;- \"elev\"\nUK &lt;- interpolate(r, gUK, debug.level=0)"
  },
  {
    "objectID": "slides/17-slides.html#universal-kriging-3",
    "href": "slides/17-slides.html#universal-kriging-3",
    "title": "Interpolation and Autocorrelation",
    "section": "Universal Kriging",
    "text": "Universal Kriging"
  },
  {
    "objectID": "slides/17-slides.html#co-kriging",
    "href": "slides/17-slides.html#co-kriging",
    "title": "Interpolation and Autocorrelation",
    "section": "Co-Kriging",
    "text": "Co-Kriging\n\nrelies on autocorrelation in \\(\\epsilon_1(\\mathbf{x})\\) for \\(z_1\\) AND cross correlation with other variables (\\(z_{2...j}\\))\nExtending the ordinary kriging model gives:\n\n\\[\n\\begin{equation}\nz_1(\\mathbf{x}) = \\mu_1 + \\epsilon_1(\\mathbf{x})\\\\\nz_2(\\mathbf{x}) = \\mu_2 + \\epsilon_2(\\mathbf{x})\n\\end{equation}\n\\] * Note that there is autocorrelation within both \\(z_1\\) and \\(z_2\\) (because of the \\(\\epsilon\\)) and cross-correlation (because of the location, \\(\\mathbf{x}\\))\n\nNot required that all variables are measured at exactly the same points"
  },
  {
    "objectID": "slides/17-slides.html#co-kriging-1",
    "href": "slides/17-slides.html#co-kriging-1",
    "title": "Interpolation and Autocorrelation",
    "section": "Co-Kriging",
    "text": "Co-Kriging\n\nProcess is just a linked series of gstat calls\n\n\ngCoK &lt;- gstat(NULL, 'log.zinc', log(zinc)~1, meuse, locations=~x+y)\ngCoK &lt;- gstat(gCoK, 'elev', elev~1, meuse, locations=~x+y)\ngCoK &lt;- gstat(gCoK, 'cadmium', cadmium~1, meuse, locations=~x+y)\ncoV &lt;- variogram(gCoK)\ncoV.fit &lt;- fit.lmc(coV, gCoK, vgm(model='Sph', range=1000))\n\ncoK &lt;- interpolate(r, coV.fit, debug.level=0)"
  },
  {
    "objectID": "slides/17-slides.html#co-kriging-2",
    "href": "slides/17-slides.html#co-kriging-2",
    "title": "Interpolation and Autocorrelation",
    "section": "Co-Kriging",
    "text": "Co-Kriging"
  },
  {
    "objectID": "slides/17-slides.html#co-kriging-3",
    "href": "slides/17-slides.html#co-kriging-3",
    "title": "Interpolation and Autocorrelation",
    "section": "Co-Kriging",
    "text": "Co-Kriging"
  },
  {
    "objectID": "slides/17-slides.html#a-note-about-semivariograms",
    "href": "slides/17-slides.html#a-note-about-semivariograms",
    "title": "Interpolation and Autocorrelation",
    "section": "A Note about Semivariograms",
    "text": "A Note about Semivariograms\n\nnugget - the proportion of semivariance that occurs at small distances\nsill - the maximum semivariance between pairs of observations\nrange - the distance at which the sill occurs\nexperimental vs. fitted variograms"
  },
  {
    "objectID": "slides/17-slides.html#a-note-about-semivariograms-1",
    "href": "slides/17-slides.html#a-note-about-semivariograms-1",
    "title": "Interpolation and Autocorrelation",
    "section": "A Note about Semivariograms",
    "text": "A Note about Semivariograms"
  },
  {
    "objectID": "slides/17-slides.html#fitted-semivariograms",
    "href": "slides/17-slides.html#fitted-semivariograms",
    "title": "Interpolation and Autocorrelation",
    "section": "Fitted Semivariograms",
    "text": "Fitted Semivariograms\n\nRely on functional forms to model semivariance"
  },
  {
    "objectID": "slides/19-slides.html#patterns-as-realizations-of-spatial-processes",
    "href": "slides/19-slides.html#patterns-as-realizations-of-spatial-processes",
    "title": "Interpolation",
    "section": "Patterns as realizations of spatial processes",
    "text": "Patterns as realizations of spatial processes\n\nA spatial process is a description of how a spatial pattern might be generated\nGenerative models\nAn observed pattern as a possible realization of an hypothesized process"
  },
  {
    "objectID": "slides/19-slides.html#deterministic-vs.-stochastic-processes",
    "href": "slides/19-slides.html#deterministic-vs.-stochastic-processes",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\nDeterministic processes: always produce the same outcome\n\n\\[\nz = 2x + 3y\n\\]\n\nResults in a spatially continuous field"
  },
  {
    "objectID": "slides/19-slides.html#deterministic-vs.-stochastic-processes-1",
    "href": "slides/19-slides.html#deterministic-vs.-stochastic-processes-1",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\nx &lt;- rast(nrows = 10, ncols=10, xmin = 0, xmax=10, ymin = 0, ymax=10)\nvalues(x) &lt;- 1\nz &lt;- x\nvalues(z) &lt;- 2 * crds(x)[,1] + 3*crds(x)[,2]"
  },
  {
    "objectID": "slides/19-slides.html#deterministic-vs.-stochastic-processes-2",
    "href": "slides/19-slides.html#deterministic-vs.-stochastic-processes-2",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes\n\n\n\n\nStochastic processes: variation makes each realization difficult to predict\n\n\\[\nz = 2x + 3y + d\n\\]\n\nThe process is random, not the result (!!)\nMeasurement error makes deterministic processes appear stochastic\n\n\n\n\nx &lt;- rast(nrows = 10, ncols=10, xmin = 0, xmax=10, ymin = 0, ymax=10)\nvalues(x) &lt;- 1\nfun &lt;- function(z){\na &lt;- z\nd &lt;- runif(ncell(z), -50,50)\nvalues(a) &lt;- 2 * crds(x)[,1] + 3*crds(x)[,2] + d\nreturn(a)\n}\n\nb &lt;- replicate(n=6, fun(z=x), simplify=FALSE)\nd &lt;- do.call(c, b)"
  },
  {
    "objectID": "slides/19-slides.html#deterministic-vs.-stochastic-processes-3",
    "href": "slides/19-slides.html#deterministic-vs.-stochastic-processes-3",
    "title": "Interpolation",
    "section": "Deterministic vs. stochastic processes",
    "text": "Deterministic vs. stochastic processes"
  },
  {
    "objectID": "slides/19-slides.html#expected-values-and-hypothesis-testing",
    "href": "slides/19-slides.html#expected-values-and-hypothesis-testing",
    "title": "Interpolation",
    "section": "Expected values and hypothesis testing",
    "text": "Expected values and hypothesis testing\n\n\n\n\nConsidering each outcome as the realization of a process allows us to generate expected values\nThe simplest spatial process is Completely Spatial Random (CSR) process\nFirst Order effects: any event has an equal probability of occurring in a location\nSecond Order effects: the location of one event is independent of the other events\n\n\n\n\n\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/19-slides.html#generating-expactations-for-csr",
    "href": "slides/19-slides.html#generating-expactations-for-csr",
    "title": "Interpolation",
    "section": "Generating expactations for CSR",
    "text": "Generating expactations for CSR\n\n\n\n\n\n\n\n\n\n\nWe can use quadrat counts to estimate the expected number of events in a given area\nThe probability of each possible count is given by:\n\n\\[\nP(n,k) = {n \\choose x}p^k(1-p)^{n-k}\n\\]\n\nGiven total coverage of quadrats, then \\(p=\\frac{\\frac{a}{x}}{a}\\) and\n\n\\[\n\\begin{equation}\nP(k,n,x) = {n \\choose k}\\bigg(\\frac{1}{x}\\bigg)^k\\bigg(\\frac{x-1}{x}\\bigg)^{n-k}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/19-slides.html#revisiting-ripleys-k",
    "href": "slides/19-slides.html#revisiting-ripleys-k",
    "title": "Interpolation",
    "section": "Revisiting Ripley’s \\(K\\)",
    "text": "Revisiting Ripley’s \\(K\\)\n\n\nNearest neighbor methods throw away a lot of information\nIf points have independent, fixed marginal densities, then they exhibit complete, spatial randomness (CSR)\nThe K function is an alternative, based on a series of circles with increasing radius\n\n\\[\n\\begin{equation}\nK(d) = \\lambda^{-1}E(N_d)\n\\end{equation}\n\\]\n\nWe can test for clustering by comparing to the expectation:\n\n\\[\n\\begin{equation}\nK_{CSR}(d) = \\pi d^2\n\\end{equation}\n\\]\n\nif \\(k(d) &gt; K_{CSR}(d)\\) then there is clustering at the scale defined by \\(d\\)"
  },
  {
    "objectID": "slides/19-slides.html#ripleys-k-function",
    "href": "slides/19-slides.html#ripleys-k-function",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nWhen working with a sample the distribution of \\(K\\) is unknown\nEstimate with\n\n\\[\n\\begin{equation}\n\\hat{K}(d) = \\hat{\\lambda}^{-1}\\sum_{i=1}^n\\sum_{j=1}^n\\frac{I(d_{ij} &lt;d)}{n(n-1)}\n\\end{equation}\n\\]\nwhere:\n\\[\n\\begin{equation}\n\\hat{\\lambda} = \\frac{n}{|A|}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/19-slides.html#ripleys-k-function-1",
    "href": "slides/19-slides.html#ripleys-k-function-1",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nUsing the spatstat package"
  },
  {
    "objectID": "slides/19-slides.html#ripleys-k-function-2",
    "href": "slides/19-slides.html#ripleys-k-function-2",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\nkf &lt;- Kest(bramblecanes, correction-\"border\")\nplot(kf)"
  },
  {
    "objectID": "slides/19-slides.html#ripleys-k-function-3",
    "href": "slides/19-slides.html#ripleys-k-function-3",
    "title": "Interpolation",
    "section": "Ripley’s \\(K\\) Function",
    "text": "Ripley’s \\(K\\) Function\n\naccounting for variation in \\(d\\)\n\n\nkf.env &lt;- envelope(bramblecanes, correction=\"border\", envelope = FALSE, verbose = FALSE)\nplot(kf.env)"
  },
  {
    "objectID": "slides/19-slides.html#other-functions",
    "href": "slides/19-slides.html#other-functions",
    "title": "Interpolation",
    "section": "Other functions",
    "text": "Other functions\n\n\n\n\\(L\\) function: square root transformation of \\(K\\)\n\\(G\\) function: the cummulative frequency distribution of the nearest neighbor distances\n\\(F\\) function: similar to \\(G\\) but based on randomly located points"
  },
  {
    "objectID": "slides/19-slides.html#spatial-autocorrelation",
    "href": "slides/19-slides.html#spatial-autocorrelation",
    "title": "Interpolation",
    "section": "Spatial autocorrelation",
    "text": "Spatial autocorrelation\n\nFrom Manuel Gimond"
  },
  {
    "objectID": "slides/19-slides.html#one-measure-of-autocorrelation",
    "href": "slides/19-slides.html#one-measure-of-autocorrelation",
    "title": "Interpolation",
    "section": "(One) Measure of autocorrelation",
    "text": "(One) Measure of autocorrelation\n\n\n\nMoran’s I"
  },
  {
    "objectID": "slides/19-slides.html#morans-i-an-example",
    "href": "slides/19-slides.html#morans-i-an-example",
    "title": "Interpolation",
    "section": "Moran’s I: An example",
    "text": "Moran’s I: An example\n\n\n\n\nUse spdep package\nEstimate neighbors\nGenerate weighted average\n\n\nset.seed(2354)\n# Load the shapefile\ns &lt;- readRDS(url(\"https://github.com/mgimond/Data/raw/gh-pages/Exercises/fl_hr80.rds\"))\n\n# Define the neighbors (use queen case)\nnb &lt;- poly2nb(s, queen=TRUE)\n\n# Compute the neighboring average homicide rates\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n#estimate Moran's I\nmoran.test(s$HR80,lw, alternative=\"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  s$HR80  \nweights: lw    \n\nMoran I statistic standard deviate = 1.8891, p-value = 0.02944\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.136277593      -0.015151515       0.006425761"
  },
  {
    "objectID": "slides/19-slides.html#morans-i-an-example-1",
    "href": "slides/19-slides.html#morans-i-an-example-1",
    "title": "Interpolation",
    "section": "Moran’s I: An example",
    "text": "Moran’s I: An example\n\n\n\nM1 &lt;- moran.mc(s$HR80, lw, nsim=9999, alternative=\"greater\")\n\n\n\n# Display the resulting statistics\nM1\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  s$HR80 \nweights: lw  \nnumber of simulations + 1: 10000 \n\nstatistic = 0.13628, observed rank = 9575, p-value = 0.0425\nalternative hypothesis: greater"
  },
  {
    "objectID": "slides/19-slides.html#the-challenge-of-areal-data",
    "href": "slides/19-slides.html#the-challenge-of-areal-data",
    "title": "Interpolation",
    "section": "The challenge of areal data",
    "text": "The challenge of areal data\n\nSpatial autocorrelation threatens second order randomness\nAreal data means an infinite number of potential distances\nNeighbor matrices, \\(\\boldsymbol W\\), allow different characterizations"
  },
  {
    "objectID": "slides/19-slides.html#interpolation-1",
    "href": "slides/19-slides.html#interpolation-1",
    "title": "Interpolation",
    "section": "Interpolation",
    "text": "Interpolation\n\nGoal: estimate the value of \\(z\\) at new points in \\(\\mathbf{x_i}\\)\nMost useful for continuous values\nNearest-neighbor, Inverse Distance Weighting, Kriging"
  },
  {
    "objectID": "slides/19-slides.html#nearest-neighbor",
    "href": "slides/19-slides.html#nearest-neighbor",
    "title": "Interpolation",
    "section": "Nearest neighbor",
    "text": "Nearest neighbor\n\nfind \\(i\\) such that \\(| \\mathbf{x_i} - \\mathbf{x}|\\) is minimized\nThe estimate of \\(z\\) is \\(z_i\\)\n\n\n\n\n\naq &lt;- read_csv(\"data/ad_viz_plotval_data.csv\") %&gt;% \n  st_as_sf(., coords = c(\"SITE_LONGITUDE\", \"SITE_LATITUDE\"), crs = \"EPSG:4326\") %&gt;% \n  st_transform(., crs = \"EPSG:8826\") %&gt;% \n  mutate(date = as_date(parse_datetime(Date, \"%m/%d/%Y\"))) %&gt;% \n  filter(., date &gt;= 2023-07-01) %&gt;% \n  filter(., date &gt; \"2023-07-01\" & date &lt; \"2023-07-31\")\naq.sum &lt;- aq %&gt;% \n  group_by(., `Site Name`) %&gt;% \n  summarise(., meanpm25 = mean(DAILY_AQI_VALUE))\n\nnodes &lt;- st_make_grid(aq.sum,\n                      what = \"centers\")\n\ndist &lt;- distance(vect(nodes), vect(aq.sum))\nnearest &lt;- apply(dist, 1, function(x) which(x == min(x)))\naq.nn &lt;- aq.sum$meanpm25[nearest]\npreds &lt;- st_as_sf(nodes)\npreds$aq &lt;- aq.nn\n\npreds &lt;- as(preds, \"Spatial\")\nsp::gridded(preds) &lt;- TRUE\npreds.rast &lt;- rast(preds)"
  },
  {
    "objectID": "slides/19-slides.html#inverse-distance-weighting",
    "href": "slides/19-slides.html#inverse-distance-weighting",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting\n\nWeight closer observations more heavily\n\n\\[\n\\begin{equation}\n\\hat{z}(\\mathbf{x}) = \\frac{\\sum_{i=1}w_iz_i}{\\sum_{i=1}w_i}\n\\end{equation}\n\\] where\n\\[\n\\begin{equation}\nw_i = | \\mathbf{x} - \\mathbf{x}_i |^{-\\alpha}\n\\end{equation}\n\\] and \\(\\alpha &gt; 0\\) (\\(\\alpha = 1\\) is inverse; \\(\\alpha = 2\\) is inverse square)"
  },
  {
    "objectID": "slides/19-slides.html#inverse-distance-weighting-1",
    "href": "slides/19-slides.html#inverse-distance-weighting-1",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting\n\nterra::interpolate provides flexible interpolation methods\nUse the gstat package to develop the formula\n\n\nmgsf05 &lt;- gstat(id = \"meanpm25\", formula = meanpm25~1, data=aq.sum,  nmax=7, set=list(idp = 0.5))\nmgsf2 &lt;- gstat(id = \"meanpm25\", formula = meanpm25~1, data=aq.sum,  nmax=7, set=list(idp = 2))\ninterpolate_gstat &lt;- function(model, x, crs, ...) {\n    v &lt;- st_as_sf(x, coords=c(\"x\", \"y\"), crs=crs)\n    p &lt;- predict(model, v, ...)\n    as.data.frame(p)[,1:2]\n}\nzsf05 &lt;- interpolate(preds.rast, mgsf05, debug.level=0, fun=interpolate_gstat, crs=crs(preds.rast), index=1)\nzsf2 &lt;- interpolate(preds.rast, mgsf2, debug.level=0, fun=interpolate_gstat, crs=crs(preds.rast), index=1)"
  },
  {
    "objectID": "slides/19-slides.html#inverse-distance-weighting-2",
    "href": "slides/19-slides.html#inverse-distance-weighting-2",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting"
  },
  {
    "objectID": "slides/19-slides.html#inverse-distance-weighting-3",
    "href": "slides/19-slides.html#inverse-distance-weighting-3",
    "title": "Interpolation",
    "section": "Inverse-Distance Weighting",
    "text": "Inverse-Distance Weighting"
  },
  {
    "objectID": "slides/21-slides.html#best-model-for-what",
    "href": "slides/21-slides.html#best-model-for-what",
    "title": "Multivariate Analysis III",
    "section": "Best Model for What?",
    "text": "Best Model for What?\n\n\n\n\n\nfrom Tradennick et al. 2021\n\n\n\n\n\nExploration: describe patterns in the data and generate hypotheses\nInference: evaluate the strength of evidence for some statement about the process\nPrediction: forecast outcomes at unsampled locations based on covariates"
  },
  {
    "objectID": "slides/21-slides.html#the-importance-of-model-fit",
    "href": "slides/21-slides.html#the-importance-of-model-fit",
    "title": "Multivariate Analysis III",
    "section": "The Importance of Model Fit",
    "text": "The Importance of Model Fit\n\nThe general regression context:\n\n\\[\n\\begin{equation}\n\\hat{y} = \\mathbf{X}\\hat{\\beta}\n\\end{equation}\n\\]\n\nInference is focused on robust estimates of \\(\\hat{\\beta}\\) given the data we have\nPrediction is focused on accurate forecasts of \\(\\hat{y}\\) at locations where we have yet to collect the data"
  },
  {
    "objectID": "slides/21-slides.html#inference-and-presenceabsence-data",
    "href": "slides/21-slides.html#inference-and-presenceabsence-data",
    "title": "Multivariate Analysis III",
    "section": "Inference and Presence/Absence Data",
    "text": "Inference and Presence/Absence Data\n\n\\(\\hat{\\beta}\\) is conditional on variables in the model and those not in the model\n\n\nnsamp &lt;- 1000\ndf &lt;- data.frame(x1 = rnorm(nsamp,0,1),\n                 x2 = rnorm(nsamp,0,1),\n                 x3 = rnorm(nsamp,0,1))\n\nlinpred &lt;- 1 + 2*df$x1 -0.18*df$x2 -3.5*df$x3\ny &lt;- rbinom(nsamp, 1, plogis(linpred))\ndf &lt;- cbind(df, y)\n\nmod1 &lt;- glm(y~x1 +x2, data=df, family=\"binomial\")\nmod2 &lt;- glm(y~x1 +x2 + x3, data=df, family=\"binomial\")"
  },
  {
    "objectID": "slides/21-slides.html#inference-presenceabsence-data",
    "href": "slides/21-slides.html#inference-presenceabsence-data",
    "title": "Multivariate Analysis III",
    "section": "Inference & Presence/Absence Data",
    "text": "Inference & Presence/Absence Data\n\n\n\ncoef(mod1)\ncoef(mod2)\n\n\n\nprd1 &lt;- predict(mod1, df, \"response\")\ndif1 &lt;- plogis(linpred) - prd1\nprd2 &lt;- predict(mod2, df, \"response\")\ndif2 &lt;- plogis(linpred) - prd2\n\n\n\n\nInferring coefficient effects requires that your model fit the data well"
  },
  {
    "objectID": "slides/21-slides.html#using-test-statistics",
    "href": "slides/21-slides.html#using-test-statistics",
    "title": "Multivariate Analysis III",
    "section": "Using Test Statistics",
    "text": "Using Test Statistics\n\n\n\n\\(R^2\\) for linear regression:\n\n\\[\n\\begin{equation}\nR^2 = 1- \\frac{SS_{res}}{SS_{tot}}\\\\\nSS_{res} = \\sum_{i}(y_i- f_i)^2\\\\\nSS_{tot} = \\sum_{i}(y_i-\\bar{y})^2\n\\end{equation}\n\\]\n\n\nPerfect prediction (\\(f_i = y_i\\)); \\(SS_{res} = 0\\); and \\(R^2 = 1\\)\nNull prediction (Intercept only) (\\(f_i = \\bar{y}\\)); \\(SS_{res} = SS_{tot}\\); and \\(R^2 = 0\\)\nNo direct way of implementing for logistic regression"
  },
  {
    "objectID": "slides/21-slides.html#pseudo--r2",
    "href": "slides/21-slides.html#pseudo--r2",
    "title": "Multivariate Analysis III",
    "section": "Pseudo- \\(R^2\\)",
    "text": "Pseudo- \\(R^2\\)\n\n\n\\[\n\\begin{equation}\nR^2_L = \\frac{D_{null} - D_{fitted}}{D_{null}}\n\\end{equation}\n\\]\n\n\nCohen’s Likelihood Ratio\nDeviance (\\(D\\)), the difference between the model and some hypothetical perfect model (lower is better)\nChallenge: Not monotonically related to \\(p\\)\nChallenge: How high is too high?"
  },
  {
    "objectID": "slides/21-slides.html#cohens-likelihood-ratio",
    "href": "slides/21-slides.html#cohens-likelihood-ratio",
    "title": "Multivariate Analysis III",
    "section": "Cohen’s Likelihood Ratio",
    "text": "Cohen’s Likelihood Ratio\n\nlogistic.rich &lt;- glm(y ~ MeanAnnTemp + PrecipWetQuarter + PrecipDryQuarter, \n                     family=binomial(link=\"logit\"),\n                     data=pts.df[,2:8])\n\nwith(logistic.rich, \n     null.deviance - deviance)/with(logistic.rich,\n                                    null.deviance)"
  },
  {
    "objectID": "slides/21-slides.html#pseudo--r2-1",
    "href": "slides/21-slides.html#pseudo--r2-1",
    "title": "Multivariate Analysis III",
    "section": "Pseudo- \\(R^2\\)",
    "text": "Pseudo- \\(R^2\\)\n\n\n\\[\n\\begin{equation}\n\\begin{aligned}\nR^2_{CS} &= 1 - \\left( \\frac{L_0}{L_M} \\right)^{(2/n)}\\\\\n&= 1 - \\exp^{2(ln(L_0)-ln(L_M))/n}\n\\end{aligned}\n\\end{equation}\n\\]\n\n\n\nCox and Snell \\(R^2\\)\nLikelihood (\\(L\\)), the probability of observing the sample given an assumed distribution\nChallenge: Maximum value is less than 1 and changes with \\(n\\)\nCorrection by Nagelkerke so that maximum is 1"
  },
  {
    "objectID": "slides/21-slides.html#cox-and-snell-r2",
    "href": "slides/21-slides.html#cox-and-snell-r2",
    "title": "Multivariate Analysis III",
    "section": "Cox and Snell \\(R^2\\)",
    "text": "Cox and Snell \\(R^2\\)\n\nlogistic.null &lt;- glm(y ~ 1, \n                     family=binomial(link=\"logit\"),\n                     data=pts.df[,2:8])\n\n1 - exp(2*(logLik(logistic.null)[1] - logLik(logistic.rich)[1])/nobs(logistic.rich))"
  },
  {
    "objectID": "slides/21-slides.html#using-test-statistics-1",
    "href": "slides/21-slides.html#using-test-statistics-1",
    "title": "Multivariate Analysis III",
    "section": "Using Test Statistics",
    "text": "Using Test Statistics\n\nBased on the data used in the model (i.e., not prediction)\nLikelihood Ratio behaves most similarly to \\(R^2\\)\nCox and Snell (and Nagelkerke) increases with more presences\nOngoing debate over which is “best”\nDon’t defer to a single statistic"
  },
  {
    "objectID": "slides/21-slides.html#predictive-performance-and-fit",
    "href": "slides/21-slides.html#predictive-performance-and-fit",
    "title": "Multivariate Analysis III",
    "section": "Predictive Performance and Fit",
    "text": "Predictive Performance and Fit\n\nPredictive performance can be an estimate of fit\nComparisons are often relative (better \\(\\neq\\) good)\nTheoretical and subsampling methods"
  },
  {
    "objectID": "slides/21-slides.html#theoretical-assessment-of-predictive-performance",
    "href": "slides/21-slides.html#theoretical-assessment-of-predictive-performance",
    "title": "Multivariate Analysis III",
    "section": "Theoretical Assessment of Predictive Performance",
    "text": "Theoretical Assessment of Predictive Performance\n\n\n\n\n\nHirotugu Akaike of AIC\n\n\n\n\n\nInformation Criterion Methods\nMinimize the amount of information lost by using model to approximate true process\nTrade-off between fit and overfitting\nCan’t know the true process (so comparisons are relative) \\[\n\\begin{equation}\nAIC = -2ln(\\hat{L}) +2k\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/21-slides.html#aic-comparison",
    "href": "slides/21-slides.html#aic-comparison",
    "title": "Multivariate Analysis III",
    "section": "AIC Comparison",
    "text": "AIC Comparison\n\nlogistic.null &lt;- glm(y ~ 1, \n                     family=binomial(link=\"logit\"),\n                     data=pts.df[,2:8])\n\nlogistic.rich &lt;- glm(y ~ MeanAnnTemp + PrecipWetQuarter + PrecipDryQuarter, \n                     family=binomial(link=\"logit\"),\n                     data=pts.df[,2:8])\n\nAIC(logistic.null, logistic.rich)"
  },
  {
    "objectID": "slides/21-slides.html#sub-sampling-methods",
    "href": "slides/21-slides.html#sub-sampling-methods",
    "title": "Multivariate Analysis III",
    "section": "Sub-sampling Methods",
    "text": "Sub-sampling Methods\n\n\n\nSplit data into training and testing\nTesting set needs to be large enough for results to be statistically meaningful\nTest set should be representative of the data as a whole\nValidation data used to tune parameters (not always)"
  },
  {
    "objectID": "slides/21-slides.html#subsampling-your-data-with-caret",
    "href": "slides/21-slides.html#subsampling-your-data-with-caret",
    "title": "Multivariate Analysis III",
    "section": "Subsampling your data with caret",
    "text": "Subsampling your data with caret\n\npts.df$y &lt;- as.factor(ifelse(pts.df$y == 1, \"Yes\", \"No\"))\nlibrary(caret)\nTrain &lt;- createDataPartition(pts.df$y, p=0.6, list=FALSE)\n\ntraining &lt;- pts.df[ Train, ]\ntesting &lt;- pts.df[ -Train, ]"
  },
  {
    "objectID": "slides/21-slides.html#misclassification",
    "href": "slides/21-slides.html#misclassification",
    "title": "Multivariate Analysis III",
    "section": "Misclassification",
    "text": "Misclassification\n\n\n\nConfusion matrices compare actual values to predictions\n\n\n\nTrue Positive (TN) - This is correctly classified as the class if interest / target.\nTrue Negative (TN) - This is correctly classified as not a class of interest / target.\nFalse Positive (FP) - This is wrongly classified as the class of interest / target.\nFalse Negative (FN) - This is wrongly classified as not a class of interest / target."
  },
  {
    "objectID": "slides/21-slides.html#confusion-matrices-in-r",
    "href": "slides/21-slides.html#confusion-matrices-in-r",
    "title": "Multivariate Analysis III",
    "section": "Confusion Matrices in R",
    "text": "Confusion Matrices in R\n\n\n\ntrain.log &lt;- glm(y ~ ., \n                 family=\"binomial\", \n                 data=training[,2:8])\n\npredicted.log &lt;- predict(train.log, \n                         newdata=testing[,2:8], \n                         type=\"response\")\n\npred &lt;- as.factor(\n  ifelse(predicted.log &gt; 0.5, \n                         \"Yes\",\n                         \"No\"))\n\n\n\n\nconfusionMatrix(testing$y, pred)"
  },
  {
    "objectID": "slides/21-slides.html#confusion-matrices",
    "href": "slides/21-slides.html#confusion-matrices",
    "title": "Multivariate Analysis III",
    "section": "Confusion Matrices",
    "text": "Confusion Matrices\n\n\n\\[\n\\begin{equation}\n\\begin{aligned}\nAccuracy &= \\frac{TP + TN}{TP + TN + FP + FN}\\\\\nSensitivity &= \\frac{TP}{TP + FN}\\\\\nSpecificity &= \\frac{TN}{FP + TN}\\\\\nPrecision &= \\frac{TP}{TP + FP}\\\\\nRecall &= \\frac{TP}{TP + FN}\n\\end{aligned}\n\\end{equation}\n\\]\n\n\nDepends upon threshold!!"
  },
  {
    "objectID": "slides/21-slides.html#confusion-matrices-in-r-1",
    "href": "slides/21-slides.html#confusion-matrices-in-r-1",
    "title": "Multivariate Analysis III",
    "section": "Confusion Matrices in R",
    "text": "Confusion Matrices in R\n\n\n\nlibrary(tree)\ntree.model &lt;- tree(y ~ . , training[,2:8])\npredict.tree &lt;- predict(tree.model, newdata=testing[,2:8], type=\"class\")\n\n\n\n\nconfusionMatrix(testing$y, predict.tree)"
  },
  {
    "objectID": "slides/21-slides.html#confusion-matrices-in-r-2",
    "href": "slides/21-slides.html#confusion-matrices-in-r-2",
    "title": "Multivariate Analysis III",
    "section": "Confusion Matrices in R",
    "text": "Confusion Matrices in R\n\n\n\nlibrary(randomForest)\nclass.model &lt;- y ~ .\nrf &lt;- randomForest(class.model, data=training[,2:8])\npredict.rf &lt;- predict(rf, newdata=testing[,2:8], type=\"class\")\n\n\n\n\nconfusionMatrix(testing$y, predict.rf)"
  },
  {
    "objectID": "slides/21-slides.html#threshold-free-methods",
    "href": "slides/21-slides.html#threshold-free-methods",
    "title": "Multivariate Analysis III",
    "section": "Threshold-Free Methods",
    "text": "Threshold-Free Methods\n\n\n\n\n\nReceiver Operating Characteristic Curves\nIllustrates discrimination of binary classifier as the threshold is varied\nArea Under the Curve (AUC) provides an estimate of classification ability"
  },
  {
    "objectID": "slides/21-slides.html#criticisms-of-rocauc",
    "href": "slides/21-slides.html#criticisms-of-rocauc",
    "title": "Multivariate Analysis III",
    "section": "Criticisms of ROC/AUC",
    "text": "Criticisms of ROC/AUC\n\nTreats false positives and false negatives equally\nUndervalues models that predict across smaller geographies\nFocus on discrimination and not calibration\nNew methods for presence-only data"
  },
  {
    "objectID": "slides/21-slides.html#roc-in-r",
    "href": "slides/21-slides.html#roc-in-r",
    "title": "Multivariate Analysis III",
    "section": "ROC in R",
    "text": "ROC in R"
  }
]