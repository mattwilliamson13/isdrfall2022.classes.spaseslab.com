{
  "hash": "798febf7efea4c0be86e15e96d12f479",
  "result": {
    "markdown": "---\ntitle: \"Multivariate Analysis II\"\nsubtitle: \"HES 505 Fall 2022: Session 20\"\nauthor: \"Matt Williamson\"\nexecute: \n  eval: false\nformat: \n  revealjs:\n    theme: mytheme.scss\n    slide-number: true\n    show-slide-number: print\n    self-contained: true  \n---\n\n\n\n\n# Objectives {background=\"#0033A0\"}\n\nBy the end of today you should be able to:\n\n- Articulate the differences between statisitical learning classifiers and logistic regression\n\n- Describe several classification trees and their relationship to Random Forests\n\n- Describe MaxEnt models for presence-only data\n\n# Revisiting Classification {background=\"#0033A0\"}\n\n## Favorability in General\n\n$$\n\\begin{equation}\nF(\\mathbf{s}) = f(w_1X_1(\\mathbf{s}), w_2X_2(\\mathbf{s}), w_3X_3(\\mathbf{s}), ..., w_mX_m(\\mathbf{s}))\n\\end{equation}\n$$\n\n* Logistic regression treats $f(x)$ as a (generalized) linear function\n\n* Allows for multiple qualitative classes\n\n* Ensures that estimates of $F(\\mathbf{s})$ are [0,1] \n\n## Beyond Linearity\n\n* Logistic (and other generalized linear models) are relatively interpretable\n\n* Probability theory allows robust inference of effects\n\n* Predictive power can be low\n\n* Relaxing the linearity assumption can help\n\n## Classification Trees\n\n* Use decision rules to segment the predictor space\n\n* Series of consecutive decision rules form a 'tree'\n\n* Terminal nodes (leaves) are the outcome; internal nodes (branches) the splits\n\n\n## Classification Trees\n\n* Divide the predictor space ($R$) into $J$ non-overlapping regions\n\n* Every observation in $R_j$ gets the same prediction\n\n* _Recursive binary splitting_\n\n* Pruning and over-fitting\n\n## An Example\n\nInputs from the `dismo` package\n\n\n::: {.cell}\n\n:::\n\n\n## An Example\n\nThe sample data\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(pres.abs)\n```\n:::\n\n:::\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n:::\n\n:::\n:::\n\n## An Example\nBuilding our dataframe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npts.df <- terra::extract(pred.stack, vect(pres.abs), df=TRUE)\nhead(pts.df)\n```\n:::\n\n\n## An Example\nBuilding our dataframe\n\n::: {.cell}\n\n```{.r .cell-code}\npts.df[,2:7] <- scale(pts.df[,2:7])\nsummary(pts.df)\n```\n:::\n\n\n## An example\n* Fitting the classification tree\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\npts.df <- cbind(pts.df, pres.abs$y)\ncolnames(pts.df)[8] <- \"y\"\npts.df$y <- as.factor(ifelse(pts.df$y == 1, \"Yes\", \"No\"))\ntree.model <- tree(y ~ . , pts.df)\n```\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree.model)\ntext(tree.model, pretty=0)\n```\n:::\n\n:::\n:::\n\n## An example\n* Fitting the classification tree\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(tree.model)\n```\n:::\n\n\n\n## Benefits and drawbacks\n\n::: columns\n::: {.column width=\"50%\"}\n**Benefits**\n\n* Easy to explain\n\n* Links to human decision-making\n\n* Graphical displays\n\n* Easy handling of qualitative predictors\n:::\n::: {.column width=\"50%\"}\n**Costs**\n\n* Lower predictive accuracy than other methods\n\n* Not necessarily robust\n:::\n:::\n\n## Random Forests\n\n::: columns\n::: {.column width=\"60%\"}\n::: {style=\"font-size: 0.7em\"}\n* Grow 100(000s) of trees using bootstrapping\n\n* Random sample of predictors considered at each split\n\n* Avoids correlation amongst multiple predictions\n\n* Average of trees improves overall outcome (usually)\n\n* Lots of extensions\n:::\n:::\n::: {.column width=\"40%\"}\n![](img/slide_20/randomforest.png)\n:::\n:::\n\n## An example\n* Fitting the Random Forest\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nclass.model <- y ~ .\nrf2 <- randomForest(class.model, data=pts.df)\n```\n:::\n\n:::\n::: {.column width=\"60%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(rf2)\n```\n:::\n\n:::\n:::\n\n## MaxEnt\n\n::: columns\n::: {.column width=\"40%\"}\n\n![From [Lentz et al. 2008](https://www.journals.uchicago.edu/doi/full/10.1086/528754)](img/slide_20/maxentresult.png)\n\n:::\n::: {.column width=\"60%\"}\n\n* Opportunistic collection of presences only\n\n* Hypothesized predictors of occurrence are measured (or extracted) at each presence\n\n* Background points (or pseudoabsences) generated for comparison\n:::\n:::\n\n## Maximum Entropy models\n\n::: columns \n::: {.column width=\"60%\"}\n::: {style=\"font-size: 0.7em\"}\n* MaxEnt (after the original software)\n\n* Need _plausible_ background points across the remainder of the study area\n\n* Iterative fitting to maximize the distance between predictions generated by a spatially uniform model \n\n* Tuning parameters to account for differences in sampling effort, placement of background points, etc\n\n* Development of the model beyond the scope of this course, but see [Elith et al. 2010](https://web.stanford.edu/~hastie/Papers/maxent_explained.pdf)\n:::\n:::\n::: {.column width=\"40%\"}\n![From [Elith et al. 2010](https://web.stanford.edu/~hastie/Papers/maxent_explained.pdf)](img/slide_20/maxentschem.png)\n:::\n:::\n\n\n## Challenges with MaxEnt\n\n* Not measuring _probability_, but relative likelihood of occurrence\n\n* Sampling bias affects estimation (but can be mitigated using tuning parameters)\n\n* Theoretical issues with background points and the intercept\n\n* Recent developments relate MaxEnt (with cloglog links) to Inhomogenous Point Process models\n\n## Extensions\n\n* Polynomial, splines, piece-wise regression\n\n* Neural nets, Support Vector Machines, many many more\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}